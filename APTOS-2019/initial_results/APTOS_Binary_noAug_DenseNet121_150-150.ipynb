{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 1689949331002293519, name: \"/device:XLA_CPU:0\"\n device_type: \"XLA_CPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 6178698848686461209\n physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n device_type: \"XLA_GPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 17167513397322380397\n physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 15884438733\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 1713834749183421958\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\nfrom sklearn.metrics import log_loss\nimport sys\nimport time\nimport math\nimport os\nimport pandas as pd\nfrom keras.callbacks import EarlyStopping\nimport numpy as np\nfrom glob import glob\nimport cv2\nimport skimage\nfrom skimage.transform import resize\nfrom keras.utils.np_utils import to_categorical\nimport keras\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.models import load_model\n# import keras.callbacks as kcall\nfrom keras.optimizers import Adam, RMSprop,SGD\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\nfrom keras.applications.vgg19 import VGG19\nfrom keras.regularizers import l2, l1\n\nimport matplotlib.pyplot as plt\nfrom keras.layers import Input, concatenate\nfrom keras import optimizers, metrics, models\nfrom keras.layers import Input, Flatten, Dense\n\n%matplotlib inline","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\n\nprint(\"Keras Version\", keras.__version__)\nprint(\"tensorflow Version\", tf.__version__)\n# print(\"dim_ordering:\", K.image_dim_ordering())","execution_count":4,"outputs":[{"output_type":"stream","text":"Keras Version 2.2.4\ntensorflow Version 1.13.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_height, img_width = 150, 150\ninput_shape = (img_height, img_width, 3)\nepochs = 1000","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/aptos-binary-noaug/aptos_binary/APTOS_Binary/\"))","execution_count":6,"outputs":[{"output_type":"stream","text":"['test', 'train']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/aptos-binary-noaug/aptos_binary/APTOS_Binary/train/'\ntest_dir = '../input/aptos-binary-noaug/aptos_binary/APTOS_Binary/test/'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(x):\n    # 'RGB'->'BGR'\n    x = x[:, :, ::-1]\n    # Zero-center by imagenet mean pixel\n    x[:, :, 0] -= 103.939\n    x[:, :, 1] -= 116.779\n    x[:, :, 2] -= 123.68\n    return x","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = np.random.seed(1142)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n    preprocessing_function = preprocess_input,\n#     zoom_range=0.2,\n#     shear_range=0.2,\n   validation_split= 0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'training',\n    class_mode='categorical')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'validation',\n    class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    class_mode='categorical')","execution_count":9,"outputs":[{"output_type":"stream","text":"Found 2656 images belonging to 2 classes.\nFound 663 images belonging to 2 classes.\nFound 343 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(train_generator.filenames)\nnb_validation_samples = len(validation_generator.filenames)\nnb_test_samples = len(test_generator.filenames)\n\npredict_size_train = int(math.ceil(nb_train_samples / batch_size))\npredict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\npredict_size_test = int(math.ceil(nb_test_samples / batch_size))\n\nnum_classes = len(train_generator.class_indices)\n\nprint(\"nb_train_samples:\", nb_train_samples)\nprint(\"nb_validation_samples:\", nb_validation_samples)\nprint(\"nb_test_samples:\", nb_test_samples)\n\nprint(\"\\npredict_size_train:\", predict_size_train)\nprint(\"predict_size_validation:\", predict_size_validation)\nprint(\"predict_size_test:\", predict_size_test)\n\nprint(\"\\n num_classes:\", num_classes)","execution_count":10,"outputs":[{"output_type":"stream","text":"nb_train_samples: 2656\nnb_validation_samples: 663\nnb_test_samples: 343\n\npredict_size_train: 83\npredict_size_validation: 21\npredict_size_test: 11\n\n num_classes: 2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"extracted_features\")\nextracted_features_dir = \"extracted_features/\"\nmodel_name = \"MobileNet_descriptors\"","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg19_weights =\"../input/full-keras-pretrained-no-top/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_weights =\"../input/full-keras-pretrained-no-top//inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nvgg16_weights =\"../input/full-keras-pretrained-no-top/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet201_weights =\"../input/full-keras-pretrained-no-top/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet121_weights =\"../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nresenet50_weights =\"../input/full-keras-pretrained-no-top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_resnet_v2_weights =\"../input/full-keras-pretrained-no-top/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nnasnet_weights =\"../input/full-keras-pretrained-no-top/nasnet_large_no_top.h5\"\nnasnet_mobile_weights =\"../input/full-keras-pretrained-no-top/nasnet_mobile_no_top.h5\"\nmobilenet_weights =\"../input/full-keras-pretrained-no-top/mobilenet_1_0_224_tf_no_top.h5\"\nxception_weights = \"../input/full-keras-pretrained-no-top/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.applications import DenseNet201\nfrom keras.applications import DenseNet121\nfrom keras.applications import ResNet50\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications import NASNetLarge, NASNetMobile\nfrom keras.applications import MobileNet","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor = Input(shape = input_shape)  \n\nbase_model1=DenseNet121(weights=denseNet121_weights, include_top=False, pooling = \"avg\", input_tensor = input_tensor )\n# base_model2=Xception(input_shape= input_shape,weights=xception_weights, include_top=False, input_tensor=input_tensor)\n\n# x1 = base_model1.output\n# x1 = GlobalAveragePooling2D()(x1)\n\n# x2 = base_model2.output\n# x2 = GlobalAveragePooling2D()(x2)\n\n# merge = concatenate([x1, x2])\n# predictions = Dense(num_classes, activation='softmax')(merge)\n\n# model = Model(inputs=input_tensor,outputs=predictions)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model1.summary()","execution_count":15,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n__________________________________________________________________________________________________\nzero_padding2d_1 (ZeroPadding2D (None, 156, 156, 3)  0           input_1[0][0]                    \n__________________________________________________________________________________________________\nconv1/conv (Conv2D)             (None, 75, 75, 64)   9408        zero_padding2d_1[0][0]           \n__________________________________________________________________________________________________\nconv1/bn (BatchNormalization)   (None, 75, 75, 64)   256         conv1/conv[0][0]                 \n__________________________________________________________________________________________________\nconv1/relu (Activation)         (None, 75, 75, 64)   0           conv1/bn[0][0]                   \n__________________________________________________________________________________________________\nzero_padding2d_2 (ZeroPadding2D (None, 77, 77, 64)   0           conv1/relu[0][0]                 \n__________________________________________________________________________________________________\npool1 (MaxPooling2D)            (None, 38, 38, 64)   0           zero_padding2d_2[0][0]           \n__________________________________________________________________________________________________\nconv2_block1_0_bn (BatchNormali (None, 38, 38, 64)   256         pool1[0][0]                      \n__________________________________________________________________________________________________\nconv2_block1_0_relu (Activation (None, 38, 38, 64)   0           conv2_block1_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block1_1_conv (Conv2D)    (None, 38, 38, 128)  8192        conv2_block1_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_1_relu (Activation (None, 38, 38, 128)  0           conv2_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block1_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block1_concat (Concatenat (None, 38, 38, 96)   0           pool1[0][0]                      \n                                                                 conv2_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_0_bn (BatchNormali (None, 38, 38, 96)   384         conv2_block1_concat[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_0_relu (Activation (None, 38, 38, 96)   0           conv2_block2_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block2_1_conv (Conv2D)    (None, 38, 38, 128)  12288       conv2_block2_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_1_relu (Activation (None, 38, 38, 128)  0           conv2_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block2_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block2_concat (Concatenat (None, 38, 38, 128)  0           conv2_block1_concat[0][0]        \n                                                                 conv2_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_0_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block2_concat[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_0_relu (Activation (None, 38, 38, 128)  0           conv2_block3_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block3_1_conv (Conv2D)    (None, 38, 38, 128)  16384       conv2_block3_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_1_relu (Activation (None, 38, 38, 128)  0           conv2_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block3_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block3_concat (Concatenat (None, 38, 38, 160)  0           conv2_block2_concat[0][0]        \n                                                                 conv2_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block4_0_bn (BatchNormali (None, 38, 38, 160)  640         conv2_block3_concat[0][0]        \n__________________________________________________________________________________________________\nconv2_block4_0_relu (Activation (None, 38, 38, 160)  0           conv2_block4_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block4_1_conv (Conv2D)    (None, 38, 38, 128)  20480       conv2_block4_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block4_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block4_1_relu (Activation (None, 38, 38, 128)  0           conv2_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block4_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block4_concat (Concatenat (None, 38, 38, 192)  0           conv2_block3_concat[0][0]        \n                                                                 conv2_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block5_0_bn (BatchNormali (None, 38, 38, 192)  768         conv2_block4_concat[0][0]        \n__________________________________________________________________________________________________\nconv2_block5_0_relu (Activation (None, 38, 38, 192)  0           conv2_block5_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block5_1_conv (Conv2D)    (None, 38, 38, 128)  24576       conv2_block5_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block5_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block5_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block5_1_relu (Activation (None, 38, 38, 128)  0           conv2_block5_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block5_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block5_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block5_concat (Concatenat (None, 38, 38, 224)  0           conv2_block4_concat[0][0]        \n                                                                 conv2_block5_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block6_0_bn (BatchNormali (None, 38, 38, 224)  896         conv2_block5_concat[0][0]        \n__________________________________________________________________________________________________\nconv2_block6_0_relu (Activation (None, 38, 38, 224)  0           conv2_block6_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block6_1_conv (Conv2D)    (None, 38, 38, 128)  28672       conv2_block6_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block6_1_bn (BatchNormali (None, 38, 38, 128)  512         conv2_block6_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv2_block6_1_relu (Activation (None, 38, 38, 128)  0           conv2_block6_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv2_block6_2_conv (Conv2D)    (None, 38, 38, 32)   36864       conv2_block6_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv2_block6_concat (Concatenat (None, 38, 38, 256)  0           conv2_block5_concat[0][0]        \n                                                                 conv2_block6_2_conv[0][0]        \n__________________________________________________________________________________________________\npool2_bn (BatchNormalization)   (None, 38, 38, 256)  1024        conv2_block6_concat[0][0]        \n__________________________________________________________________________________________________\npool2_relu (Activation)         (None, 38, 38, 256)  0           pool2_bn[0][0]                   \n__________________________________________________________________________________________________\npool2_conv (Conv2D)             (None, 38, 38, 128)  32768       pool2_relu[0][0]                 \n__________________________________________________________________________________________________\npool2_pool (AveragePooling2D)   (None, 19, 19, 128)  0           pool2_conv[0][0]                 \n__________________________________________________________________________________________________\nconv3_block1_0_bn (BatchNormali (None, 19, 19, 128)  512         pool2_pool[0][0]                 \n__________________________________________________________________________________________________\nconv3_block1_0_relu (Activation (None, 19, 19, 128)  0           conv3_block1_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block1_1_conv (Conv2D)    (None, 19, 19, 128)  16384       conv3_block1_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_1_relu (Activation (None, 19, 19, 128)  0           conv3_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block1_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block1_concat (Concatenat (None, 19, 19, 160)  0           pool2_pool[0][0]                 \n                                                                 conv3_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_0_bn (BatchNormali (None, 19, 19, 160)  640         conv3_block1_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_0_relu (Activation (None, 19, 19, 160)  0           conv3_block2_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block2_1_conv (Conv2D)    (None, 19, 19, 128)  20480       conv3_block2_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_1_relu (Activation (None, 19, 19, 128)  0           conv3_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block2_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block2_concat (Concatenat (None, 19, 19, 192)  0           conv3_block1_concat[0][0]        \n                                                                 conv3_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_0_bn (BatchNormali (None, 19, 19, 192)  768         conv3_block2_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_0_relu (Activation (None, 19, 19, 192)  0           conv3_block3_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block3_1_conv (Conv2D)    (None, 19, 19, 128)  24576       conv3_block3_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_1_relu (Activation (None, 19, 19, 128)  0           conv3_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block3_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block3_concat (Concatenat (None, 19, 19, 224)  0           conv3_block2_concat[0][0]        \n                                                                 conv3_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_0_bn (BatchNormali (None, 19, 19, 224)  896         conv3_block3_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_0_relu (Activation (None, 19, 19, 224)  0           conv3_block4_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block4_1_conv (Conv2D)    (None, 19, 19, 128)  28672       conv3_block4_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_1_relu (Activation (None, 19, 19, 128)  0           conv3_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block4_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block4_concat (Concatenat (None, 19, 19, 256)  0           conv3_block3_concat[0][0]        \n                                                                 conv3_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block5_0_bn (BatchNormali (None, 19, 19, 256)  1024        conv3_block4_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block5_0_relu (Activation (None, 19, 19, 256)  0           conv3_block5_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block5_1_conv (Conv2D)    (None, 19, 19, 128)  32768       conv3_block5_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block5_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block5_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block5_1_relu (Activation (None, 19, 19, 128)  0           conv3_block5_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block5_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block5_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block5_concat (Concatenat (None, 19, 19, 288)  0           conv3_block4_concat[0][0]        \n                                                                 conv3_block5_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block6_0_bn (BatchNormali (None, 19, 19, 288)  1152        conv3_block5_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block6_0_relu (Activation (None, 19, 19, 288)  0           conv3_block6_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block6_1_conv (Conv2D)    (None, 19, 19, 128)  36864       conv3_block6_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block6_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block6_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block6_1_relu (Activation (None, 19, 19, 128)  0           conv3_block6_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block6_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block6_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block6_concat (Concatenat (None, 19, 19, 320)  0           conv3_block5_concat[0][0]        \n                                                                 conv3_block6_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block7_0_bn (BatchNormali (None, 19, 19, 320)  1280        conv3_block6_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block7_0_relu (Activation (None, 19, 19, 320)  0           conv3_block7_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block7_1_conv (Conv2D)    (None, 19, 19, 128)  40960       conv3_block7_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block7_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block7_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block7_1_relu (Activation (None, 19, 19, 128)  0           conv3_block7_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block7_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block7_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block7_concat (Concatenat (None, 19, 19, 352)  0           conv3_block6_concat[0][0]        \n                                                                 conv3_block7_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block8_0_bn (BatchNormali (None, 19, 19, 352)  1408        conv3_block7_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block8_0_relu (Activation (None, 19, 19, 352)  0           conv3_block8_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block8_1_conv (Conv2D)    (None, 19, 19, 128)  45056       conv3_block8_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block8_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block8_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block8_1_relu (Activation (None, 19, 19, 128)  0           conv3_block8_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block8_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block8_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block8_concat (Concatenat (None, 19, 19, 384)  0           conv3_block7_concat[0][0]        \n                                                                 conv3_block8_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block9_0_bn (BatchNormali (None, 19, 19, 384)  1536        conv3_block8_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block9_0_relu (Activation (None, 19, 19, 384)  0           conv3_block9_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block9_1_conv (Conv2D)    (None, 19, 19, 128)  49152       conv3_block9_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block9_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block9_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block9_1_relu (Activation (None, 19, 19, 128)  0           conv3_block9_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv3_block9_2_conv (Conv2D)    (None, 19, 19, 32)   36864       conv3_block9_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv3_block9_concat (Concatenat (None, 19, 19, 416)  0           conv3_block8_concat[0][0]        \n                                                                 conv3_block9_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv3_block10_0_bn (BatchNormal (None, 19, 19, 416)  1664        conv3_block9_concat[0][0]        \n__________________________________________________________________________________________________\nconv3_block10_0_relu (Activatio (None, 19, 19, 416)  0           conv3_block10_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block10_1_conv (Conv2D)   (None, 19, 19, 128)  53248       conv3_block10_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block10_1_bn (BatchNormal (None, 19, 19, 128)  512         conv3_block10_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv3_block10_1_relu (Activatio (None, 19, 19, 128)  0           conv3_block10_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block10_2_conv (Conv2D)   (None, 19, 19, 32)   36864       conv3_block10_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block10_concat (Concatena (None, 19, 19, 448)  0           conv3_block9_concat[0][0]        \n                                                                 conv3_block10_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv3_block11_0_bn (BatchNormal (None, 19, 19, 448)  1792        conv3_block10_concat[0][0]       \n__________________________________________________________________________________________________\nconv3_block11_0_relu (Activatio (None, 19, 19, 448)  0           conv3_block11_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block11_1_conv (Conv2D)   (None, 19, 19, 128)  57344       conv3_block11_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block11_1_bn (BatchNormal (None, 19, 19, 128)  512         conv3_block11_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv3_block11_1_relu (Activatio (None, 19, 19, 128)  0           conv3_block11_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block11_2_conv (Conv2D)   (None, 19, 19, 32)   36864       conv3_block11_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block11_concat (Concatena (None, 19, 19, 480)  0           conv3_block10_concat[0][0]       \n                                                                 conv3_block11_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv3_block12_0_bn (BatchNormal (None, 19, 19, 480)  1920        conv3_block11_concat[0][0]       \n__________________________________________________________________________________________________\nconv3_block12_0_relu (Activatio (None, 19, 19, 480)  0           conv3_block12_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block12_1_conv (Conv2D)   (None, 19, 19, 128)  61440       conv3_block12_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block12_1_bn (BatchNormal (None, 19, 19, 128)  512         conv3_block12_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv3_block12_1_relu (Activatio (None, 19, 19, 128)  0           conv3_block12_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv3_block12_2_conv (Conv2D)   (None, 19, 19, 32)   36864       conv3_block12_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv3_block12_concat (Concatena (None, 19, 19, 512)  0           conv3_block11_concat[0][0]       \n                                                                 conv3_block12_2_conv[0][0]       \n__________________________________________________________________________________________________\npool3_bn (BatchNormalization)   (None, 19, 19, 512)  2048        conv3_block12_concat[0][0]       \n__________________________________________________________________________________________________\npool3_relu (Activation)         (None, 19, 19, 512)  0           pool3_bn[0][0]                   \n__________________________________________________________________________________________________\npool3_conv (Conv2D)             (None, 19, 19, 256)  131072      pool3_relu[0][0]                 \n__________________________________________________________________________________________________\npool3_pool (AveragePooling2D)   (None, 9, 9, 256)    0           pool3_conv[0][0]                 \n__________________________________________________________________________________________________\nconv4_block1_0_bn (BatchNormali (None, 9, 9, 256)    1024        pool3_pool[0][0]                 \n__________________________________________________________________________________________________\nconv4_block1_0_relu (Activation (None, 9, 9, 256)    0           conv4_block1_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block1_1_conv (Conv2D)    (None, 9, 9, 128)    32768       conv4_block1_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_1_relu (Activation (None, 9, 9, 128)    0           conv4_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block1_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block1_concat (Concatenat (None, 9, 9, 288)    0           pool3_pool[0][0]                 \n                                                                 conv4_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_0_bn (BatchNormali (None, 9, 9, 288)    1152        conv4_block1_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_0_relu (Activation (None, 9, 9, 288)    0           conv4_block2_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block2_1_conv (Conv2D)    (None, 9, 9, 128)    36864       conv4_block2_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_1_relu (Activation (None, 9, 9, 128)    0           conv4_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block2_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block2_concat (Concatenat (None, 9, 9, 320)    0           conv4_block1_concat[0][0]        \n                                                                 conv4_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_0_bn (BatchNormali (None, 9, 9, 320)    1280        conv4_block2_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_0_relu (Activation (None, 9, 9, 320)    0           conv4_block3_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block3_1_conv (Conv2D)    (None, 9, 9, 128)    40960       conv4_block3_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_1_relu (Activation (None, 9, 9, 128)    0           conv4_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block3_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block3_concat (Concatenat (None, 9, 9, 352)    0           conv4_block2_concat[0][0]        \n                                                                 conv4_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_0_bn (BatchNormali (None, 9, 9, 352)    1408        conv4_block3_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_0_relu (Activation (None, 9, 9, 352)    0           conv4_block4_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block4_1_conv (Conv2D)    (None, 9, 9, 128)    45056       conv4_block4_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_1_relu (Activation (None, 9, 9, 128)    0           conv4_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block4_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block4_concat (Concatenat (None, 9, 9, 384)    0           conv4_block3_concat[0][0]        \n                                                                 conv4_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_0_bn (BatchNormali (None, 9, 9, 384)    1536        conv4_block4_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_0_relu (Activation (None, 9, 9, 384)    0           conv4_block5_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block5_1_conv (Conv2D)    (None, 9, 9, 128)    49152       conv4_block5_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block5_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_1_relu (Activation (None, 9, 9, 128)    0           conv4_block5_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block5_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block5_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block5_concat (Concatenat (None, 9, 9, 416)    0           conv4_block4_concat[0][0]        \n                                                                 conv4_block5_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_0_bn (BatchNormali (None, 9, 9, 416)    1664        conv4_block5_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_0_relu (Activation (None, 9, 9, 416)    0           conv4_block6_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block6_1_conv (Conv2D)    (None, 9, 9, 128)    53248       conv4_block6_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block6_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_1_relu (Activation (None, 9, 9, 128)    0           conv4_block6_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block6_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block6_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block6_concat (Concatenat (None, 9, 9, 448)    0           conv4_block5_concat[0][0]        \n                                                                 conv4_block6_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block7_0_bn (BatchNormali (None, 9, 9, 448)    1792        conv4_block6_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block7_0_relu (Activation (None, 9, 9, 448)    0           conv4_block7_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block7_1_conv (Conv2D)    (None, 9, 9, 128)    57344       conv4_block7_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block7_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block7_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block7_1_relu (Activation (None, 9, 9, 128)    0           conv4_block7_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block7_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block7_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block7_concat (Concatenat (None, 9, 9, 480)    0           conv4_block6_concat[0][0]        \n                                                                 conv4_block7_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block8_0_bn (BatchNormali (None, 9, 9, 480)    1920        conv4_block7_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block8_0_relu (Activation (None, 9, 9, 480)    0           conv4_block8_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block8_1_conv (Conv2D)    (None, 9, 9, 128)    61440       conv4_block8_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block8_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block8_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block8_1_relu (Activation (None, 9, 9, 128)    0           conv4_block8_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block8_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block8_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block8_concat (Concatenat (None, 9, 9, 512)    0           conv4_block7_concat[0][0]        \n                                                                 conv4_block8_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block9_0_bn (BatchNormali (None, 9, 9, 512)    2048        conv4_block8_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block9_0_relu (Activation (None, 9, 9, 512)    0           conv4_block9_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block9_1_conv (Conv2D)    (None, 9, 9, 128)    65536       conv4_block9_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block9_1_bn (BatchNormali (None, 9, 9, 128)    512         conv4_block9_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block9_1_relu (Activation (None, 9, 9, 128)    0           conv4_block9_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv4_block9_2_conv (Conv2D)    (None, 9, 9, 32)     36864       conv4_block9_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv4_block9_concat (Concatenat (None, 9, 9, 544)    0           conv4_block8_concat[0][0]        \n                                                                 conv4_block9_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv4_block10_0_bn (BatchNormal (None, 9, 9, 544)    2176        conv4_block9_concat[0][0]        \n__________________________________________________________________________________________________\nconv4_block10_0_relu (Activatio (None, 9, 9, 544)    0           conv4_block10_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block10_1_conv (Conv2D)   (None, 9, 9, 128)    69632       conv4_block10_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block10_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block10_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block10_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block10_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block10_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block10_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block10_concat (Concatena (None, 9, 9, 576)    0           conv4_block9_concat[0][0]        \n                                                                 conv4_block10_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block11_0_bn (BatchNormal (None, 9, 9, 576)    2304        conv4_block10_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block11_0_relu (Activatio (None, 9, 9, 576)    0           conv4_block11_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block11_1_conv (Conv2D)   (None, 9, 9, 128)    73728       conv4_block11_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block11_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block11_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block11_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block11_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block11_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block11_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block11_concat (Concatena (None, 9, 9, 608)    0           conv4_block10_concat[0][0]       \n                                                                 conv4_block11_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block12_0_bn (BatchNormal (None, 9, 9, 608)    2432        conv4_block11_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block12_0_relu (Activatio (None, 9, 9, 608)    0           conv4_block12_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block12_1_conv (Conv2D)   (None, 9, 9, 128)    77824       conv4_block12_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block12_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block12_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block12_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block12_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block12_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block12_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block12_concat (Concatena (None, 9, 9, 640)    0           conv4_block11_concat[0][0]       \n                                                                 conv4_block12_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block13_0_bn (BatchNormal (None, 9, 9, 640)    2560        conv4_block12_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block13_0_relu (Activatio (None, 9, 9, 640)    0           conv4_block13_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block13_1_conv (Conv2D)   (None, 9, 9, 128)    81920       conv4_block13_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block13_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block13_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block13_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block13_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block13_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block13_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block13_concat (Concatena (None, 9, 9, 672)    0           conv4_block12_concat[0][0]       \n                                                                 conv4_block13_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block14_0_bn (BatchNormal (None, 9, 9, 672)    2688        conv4_block13_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block14_0_relu (Activatio (None, 9, 9, 672)    0           conv4_block14_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block14_1_conv (Conv2D)   (None, 9, 9, 128)    86016       conv4_block14_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block14_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block14_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block14_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block14_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block14_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block14_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block14_concat (Concatena (None, 9, 9, 704)    0           conv4_block13_concat[0][0]       \n                                                                 conv4_block14_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block15_0_bn (BatchNormal (None, 9, 9, 704)    2816        conv4_block14_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block15_0_relu (Activatio (None, 9, 9, 704)    0           conv4_block15_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block15_1_conv (Conv2D)   (None, 9, 9, 128)    90112       conv4_block15_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block15_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block15_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block15_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block15_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block15_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block15_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block15_concat (Concatena (None, 9, 9, 736)    0           conv4_block14_concat[0][0]       \n                                                                 conv4_block15_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block16_0_bn (BatchNormal (None, 9, 9, 736)    2944        conv4_block15_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block16_0_relu (Activatio (None, 9, 9, 736)    0           conv4_block16_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block16_1_conv (Conv2D)   (None, 9, 9, 128)    94208       conv4_block16_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block16_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block16_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block16_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block16_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block16_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block16_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block16_concat (Concatena (None, 9, 9, 768)    0           conv4_block15_concat[0][0]       \n                                                                 conv4_block16_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block17_0_bn (BatchNormal (None, 9, 9, 768)    3072        conv4_block16_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block17_0_relu (Activatio (None, 9, 9, 768)    0           conv4_block17_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block17_1_conv (Conv2D)   (None, 9, 9, 128)    98304       conv4_block17_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block17_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block17_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block17_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block17_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block17_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block17_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block17_concat (Concatena (None, 9, 9, 800)    0           conv4_block16_concat[0][0]       \n                                                                 conv4_block17_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block18_0_bn (BatchNormal (None, 9, 9, 800)    3200        conv4_block17_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block18_0_relu (Activatio (None, 9, 9, 800)    0           conv4_block18_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block18_1_conv (Conv2D)   (None, 9, 9, 128)    102400      conv4_block18_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block18_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block18_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block18_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block18_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block18_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block18_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block18_concat (Concatena (None, 9, 9, 832)    0           conv4_block17_concat[0][0]       \n                                                                 conv4_block18_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block19_0_bn (BatchNormal (None, 9, 9, 832)    3328        conv4_block18_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block19_0_relu (Activatio (None, 9, 9, 832)    0           conv4_block19_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block19_1_conv (Conv2D)   (None, 9, 9, 128)    106496      conv4_block19_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block19_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block19_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block19_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block19_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block19_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block19_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block19_concat (Concatena (None, 9, 9, 864)    0           conv4_block18_concat[0][0]       \n                                                                 conv4_block19_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block20_0_bn (BatchNormal (None, 9, 9, 864)    3456        conv4_block19_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block20_0_relu (Activatio (None, 9, 9, 864)    0           conv4_block20_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block20_1_conv (Conv2D)   (None, 9, 9, 128)    110592      conv4_block20_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block20_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block20_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block20_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block20_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block20_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block20_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block20_concat (Concatena (None, 9, 9, 896)    0           conv4_block19_concat[0][0]       \n                                                                 conv4_block20_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block21_0_bn (BatchNormal (None, 9, 9, 896)    3584        conv4_block20_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block21_0_relu (Activatio (None, 9, 9, 896)    0           conv4_block21_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block21_1_conv (Conv2D)   (None, 9, 9, 128)    114688      conv4_block21_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block21_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block21_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block21_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block21_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block21_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block21_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block21_concat (Concatena (None, 9, 9, 928)    0           conv4_block20_concat[0][0]       \n                                                                 conv4_block21_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block22_0_bn (BatchNormal (None, 9, 9, 928)    3712        conv4_block21_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block22_0_relu (Activatio (None, 9, 9, 928)    0           conv4_block22_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block22_1_conv (Conv2D)   (None, 9, 9, 128)    118784      conv4_block22_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block22_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block22_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block22_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block22_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block22_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block22_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block22_concat (Concatena (None, 9, 9, 960)    0           conv4_block21_concat[0][0]       \n                                                                 conv4_block22_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block23_0_bn (BatchNormal (None, 9, 9, 960)    3840        conv4_block22_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block23_0_relu (Activatio (None, 9, 9, 960)    0           conv4_block23_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block23_1_conv (Conv2D)   (None, 9, 9, 128)    122880      conv4_block23_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block23_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block23_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block23_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block23_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block23_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block23_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block23_concat (Concatena (None, 9, 9, 992)    0           conv4_block22_concat[0][0]       \n                                                                 conv4_block23_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block24_0_bn (BatchNormal (None, 9, 9, 992)    3968        conv4_block23_concat[0][0]       \n__________________________________________________________________________________________________\nconv4_block24_0_relu (Activatio (None, 9, 9, 992)    0           conv4_block24_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block24_1_conv (Conv2D)   (None, 9, 9, 128)    126976      conv4_block24_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block24_1_bn (BatchNormal (None, 9, 9, 128)    512         conv4_block24_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv4_block24_1_relu (Activatio (None, 9, 9, 128)    0           conv4_block24_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv4_block24_2_conv (Conv2D)   (None, 9, 9, 32)     36864       conv4_block24_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv4_block24_concat (Concatena (None, 9, 9, 1024)   0           conv4_block23_concat[0][0]       \n                                                                 conv4_block24_2_conv[0][0]       \n__________________________________________________________________________________________________\npool4_bn (BatchNormalization)   (None, 9, 9, 1024)   4096        conv4_block24_concat[0][0]       \n__________________________________________________________________________________________________\npool4_relu (Activation)         (None, 9, 9, 1024)   0           pool4_bn[0][0]                   \n__________________________________________________________________________________________________\npool4_conv (Conv2D)             (None, 9, 9, 512)    524288      pool4_relu[0][0]                 \n__________________________________________________________________________________________________\npool4_pool (AveragePooling2D)   (None, 4, 4, 512)    0           pool4_conv[0][0]                 \n__________________________________________________________________________________________________\nconv5_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        pool4_pool[0][0]                 \n__________________________________________________________________________________________________\nconv5_block1_0_relu (Activation (None, 4, 4, 512)    0           conv5_block1_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block1_1_conv (Conv2D)    (None, 4, 4, 128)    65536       conv5_block1_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block1_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_1_relu (Activation (None, 4, 4, 128)    0           conv5_block1_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block1_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block1_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block1_concat (Concatenat (None, 4, 4, 544)    0           pool4_pool[0][0]                 \n                                                                 conv5_block1_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_0_bn (BatchNormali (None, 4, 4, 544)    2176        conv5_block1_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_0_relu (Activation (None, 4, 4, 544)    0           conv5_block2_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block2_1_conv (Conv2D)    (None, 4, 4, 128)    69632       conv5_block2_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block2_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_1_relu (Activation (None, 4, 4, 128)    0           conv5_block2_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block2_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block2_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block2_concat (Concatenat (None, 4, 4, 576)    0           conv5_block1_concat[0][0]        \n                                                                 conv5_block2_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_0_bn (BatchNormali (None, 4, 4, 576)    2304        conv5_block2_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_0_relu (Activation (None, 4, 4, 576)    0           conv5_block3_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block3_1_conv (Conv2D)    (None, 4, 4, 128)    73728       conv5_block3_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block3_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_1_relu (Activation (None, 4, 4, 128)    0           conv5_block3_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block3_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block3_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block3_concat (Concatenat (None, 4, 4, 608)    0           conv5_block2_concat[0][0]        \n                                                                 conv5_block3_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block4_0_bn (BatchNormali (None, 4, 4, 608)    2432        conv5_block3_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block4_0_relu (Activation (None, 4, 4, 608)    0           conv5_block4_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block4_1_conv (Conv2D)    (None, 4, 4, 128)    77824       conv5_block4_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block4_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block4_1_relu (Activation (None, 4, 4, 128)    0           conv5_block4_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block4_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block4_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block4_concat (Concatenat (None, 4, 4, 640)    0           conv5_block3_concat[0][0]        \n                                                                 conv5_block4_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block5_0_bn (BatchNormali (None, 4, 4, 640)    2560        conv5_block4_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block5_0_relu (Activation (None, 4, 4, 640)    0           conv5_block5_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block5_1_conv (Conv2D)    (None, 4, 4, 128)    81920       conv5_block5_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block5_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block5_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block5_1_relu (Activation (None, 4, 4, 128)    0           conv5_block5_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block5_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block5_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block5_concat (Concatenat (None, 4, 4, 672)    0           conv5_block4_concat[0][0]        \n                                                                 conv5_block5_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block6_0_bn (BatchNormali (None, 4, 4, 672)    2688        conv5_block5_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block6_0_relu (Activation (None, 4, 4, 672)    0           conv5_block6_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block6_1_conv (Conv2D)    (None, 4, 4, 128)    86016       conv5_block6_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block6_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block6_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block6_1_relu (Activation (None, 4, 4, 128)    0           conv5_block6_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block6_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block6_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block6_concat (Concatenat (None, 4, 4, 704)    0           conv5_block5_concat[0][0]        \n                                                                 conv5_block6_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block7_0_bn (BatchNormali (None, 4, 4, 704)    2816        conv5_block6_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block7_0_relu (Activation (None, 4, 4, 704)    0           conv5_block7_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block7_1_conv (Conv2D)    (None, 4, 4, 128)    90112       conv5_block7_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block7_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block7_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block7_1_relu (Activation (None, 4, 4, 128)    0           conv5_block7_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block7_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block7_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block7_concat (Concatenat (None, 4, 4, 736)    0           conv5_block6_concat[0][0]        \n                                                                 conv5_block7_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block8_0_bn (BatchNormali (None, 4, 4, 736)    2944        conv5_block7_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block8_0_relu (Activation (None, 4, 4, 736)    0           conv5_block8_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block8_1_conv (Conv2D)    (None, 4, 4, 128)    94208       conv5_block8_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block8_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block8_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block8_1_relu (Activation (None, 4, 4, 128)    0           conv5_block8_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block8_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block8_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block8_concat (Concatenat (None, 4, 4, 768)    0           conv5_block7_concat[0][0]        \n                                                                 conv5_block8_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block9_0_bn (BatchNormali (None, 4, 4, 768)    3072        conv5_block8_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block9_0_relu (Activation (None, 4, 4, 768)    0           conv5_block9_0_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block9_1_conv (Conv2D)    (None, 4, 4, 128)    98304       conv5_block9_0_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block9_1_bn (BatchNormali (None, 4, 4, 128)    512         conv5_block9_1_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block9_1_relu (Activation (None, 4, 4, 128)    0           conv5_block9_1_bn[0][0]          \n__________________________________________________________________________________________________\nconv5_block9_2_conv (Conv2D)    (None, 4, 4, 32)     36864       conv5_block9_1_relu[0][0]        \n__________________________________________________________________________________________________\nconv5_block9_concat (Concatenat (None, 4, 4, 800)    0           conv5_block8_concat[0][0]        \n                                                                 conv5_block9_2_conv[0][0]        \n__________________________________________________________________________________________________\nconv5_block10_0_bn (BatchNormal (None, 4, 4, 800)    3200        conv5_block9_concat[0][0]        \n__________________________________________________________________________________________________\nconv5_block10_0_relu (Activatio (None, 4, 4, 800)    0           conv5_block10_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block10_1_conv (Conv2D)   (None, 4, 4, 128)    102400      conv5_block10_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block10_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block10_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block10_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block10_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block10_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block10_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block10_concat (Concatena (None, 4, 4, 832)    0           conv5_block9_concat[0][0]        \n                                                                 conv5_block10_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block11_0_bn (BatchNormal (None, 4, 4, 832)    3328        conv5_block10_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block11_0_relu (Activatio (None, 4, 4, 832)    0           conv5_block11_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block11_1_conv (Conv2D)   (None, 4, 4, 128)    106496      conv5_block11_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block11_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block11_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block11_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block11_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block11_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block11_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block11_concat (Concatena (None, 4, 4, 864)    0           conv5_block10_concat[0][0]       \n                                                                 conv5_block11_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block12_0_bn (BatchNormal (None, 4, 4, 864)    3456        conv5_block11_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block12_0_relu (Activatio (None, 4, 4, 864)    0           conv5_block12_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block12_1_conv (Conv2D)   (None, 4, 4, 128)    110592      conv5_block12_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block12_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block12_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block12_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block12_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block12_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block12_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block12_concat (Concatena (None, 4, 4, 896)    0           conv5_block11_concat[0][0]       \n                                                                 conv5_block12_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block13_0_bn (BatchNormal (None, 4, 4, 896)    3584        conv5_block12_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block13_0_relu (Activatio (None, 4, 4, 896)    0           conv5_block13_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block13_1_conv (Conv2D)   (None, 4, 4, 128)    114688      conv5_block13_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block13_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block13_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block13_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block13_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block13_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block13_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block13_concat (Concatena (None, 4, 4, 928)    0           conv5_block12_concat[0][0]       \n                                                                 conv5_block13_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block14_0_bn (BatchNormal (None, 4, 4, 928)    3712        conv5_block13_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block14_0_relu (Activatio (None, 4, 4, 928)    0           conv5_block14_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block14_1_conv (Conv2D)   (None, 4, 4, 128)    118784      conv5_block14_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block14_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block14_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block14_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block14_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block14_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block14_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block14_concat (Concatena (None, 4, 4, 960)    0           conv5_block13_concat[0][0]       \n                                                                 conv5_block14_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block15_0_bn (BatchNormal (None, 4, 4, 960)    3840        conv5_block14_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block15_0_relu (Activatio (None, 4, 4, 960)    0           conv5_block15_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block15_1_conv (Conv2D)   (None, 4, 4, 128)    122880      conv5_block15_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block15_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block15_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block15_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block15_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block15_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block15_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block15_concat (Concatena (None, 4, 4, 992)    0           conv5_block14_concat[0][0]       \n                                                                 conv5_block15_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block16_0_bn (BatchNormal (None, 4, 4, 992)    3968        conv5_block15_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block16_0_relu (Activatio (None, 4, 4, 992)    0           conv5_block16_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block16_1_conv (Conv2D)   (None, 4, 4, 128)    126976      conv5_block16_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block16_1_bn (BatchNormal (None, 4, 4, 128)    512         conv5_block16_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block16_1_relu (Activatio (None, 4, 4, 128)    0           conv5_block16_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block16_2_conv (Conv2D)   (None, 4, 4, 32)     36864       conv5_block16_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block16_concat (Concatena (None, 4, 4, 1024)   0           conv5_block15_concat[0][0]       \n                                                                 conv5_block16_2_conv[0][0]       \n__________________________________________________________________________________________________\nbn (BatchNormalization)         (None, 4, 4, 1024)   4096        conv5_block16_concat[0][0]       \n__________________________________________________________________________________________________\nrelu (Activation)               (None, 4, 4, 1024)   0           bn[0][0]                         \n__________________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]                       \n==================================================================================================\nTotal params: 7,037,504\nTrainable params: 6,953,856\nNon-trainable params: 83,648\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i, layer in enumerate(model.layers):\n#     print(i, layer.name)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_final_model = base_model1","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train, max_q_size=1, pickle_safe=False)\nnp.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\nnp.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\nnp.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import tensorflow as tf\n# from keras.backend.tensorflow_backend import get_session\n# from keras.backend.tensorflow_backend import clear_session\n# from keras.backend.tensorflow_backend import set_session\n\n# def reset_keras_tf_session():\n#     \"\"\"\n#     this function clears the gpu memory and set the \n#     tf session to not use the whole gpu\n#     \"\"\"\n#     sess = get_session()\n#     clear_session()\n#     sess.close()\n#     sess = get_session()\n\n# #     config = tf.ConfigProto()\n# #     config.gpu_options.allow_growth = True\n# #     set_session(tf.Session(config=config))\n\n# reset_keras_tf_session()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\nvalidation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\ntest_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n\ntrain_labels = train_generator.classes\ntrain_labels = to_categorical(train_labels, num_classes=num_classes)\n\nvalidation_labels = validation_generator.classes\nvalidation_labels = to_categorical(validation_labels, num_classes=num_classes)\n\ntest_labels = test_generator.classes\ntest_labels = to_categorical(test_labels, num_classes=num_classes)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adam_opt=Adam(lr=0.0001, beta_1=0.8, beta_2=0.9)\ndropout_rate = 0.5\n\nmodel = Sequential()\nmodel.add(Dense(4096, activation=\"elu\", kernel_regularizer=l2(1e-05), bias_regularizer=l2(0.1), activity_regularizer=l1(1e-05)))\nmodel.add(Dropout(dropout_rate))\n\nmodel.add(Dense(256, activation=\"elu\", kernel_regularizer=l2(1e-05), bias_regularizer=l2(0.1), activity_regularizer=l1(1e-05)))\nmodel.add(Dropout(dropout_rate))\n\nmodel.add(Dense(num_classes, activation=\"softmax\"))\n\nmodel.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_data, train_labels,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    validation_data=(validation_data, validation_labels),\n                    verbose= 2)\n","execution_count":23,"outputs":[{"output_type":"stream","text":"Train on 2656 samples, validate on 663 samples\nEpoch 1/1000\n - 4s - loss: 1.0140 - acc: 0.8532 - val_loss: 0.6729 - val_acc: 0.9548\nEpoch 2/1000\n - 0s - loss: 0.6927 - acc: 0.9322 - val_loss: 0.5437 - val_acc: 0.9668\nEpoch 3/1000\n - 1s - loss: 0.6093 - acc: 0.9424 - val_loss: 0.5086 - val_acc: 0.9683\nEpoch 4/1000\n - 1s - loss: 0.5440 - acc: 0.9582 - val_loss: 0.4905 - val_acc: 0.9668\nEpoch 5/1000\n - 1s - loss: 0.5184 - acc: 0.9552 - val_loss: 0.4744 - val_acc: 0.9713\nEpoch 6/1000\n - 1s - loss: 0.5197 - acc: 0.9578 - val_loss: 0.5232 - val_acc: 0.9548\nEpoch 7/1000\n - 1s - loss: 0.5060 - acc: 0.9616 - val_loss: 0.4598 - val_acc: 0.9713\nEpoch 8/1000\n - 0s - loss: 0.4884 - acc: 0.9608 - val_loss: 0.4901 - val_acc: 0.9608\nEpoch 9/1000\n - 1s - loss: 0.4773 - acc: 0.9650 - val_loss: 0.4512 - val_acc: 0.9638\nEpoch 10/1000\n - 1s - loss: 0.4736 - acc: 0.9593 - val_loss: 0.4799 - val_acc: 0.9608\nEpoch 11/1000\n - 0s - loss: 0.4603 - acc: 0.9620 - val_loss: 0.4552 - val_acc: 0.9638\nEpoch 12/1000\n - 1s - loss: 0.4524 - acc: 0.9672 - val_loss: 0.4511 - val_acc: 0.9593\nEpoch 13/1000\n - 1s - loss: 0.4486 - acc: 0.9665 - val_loss: 0.4592 - val_acc: 0.9638\nEpoch 14/1000\n - 1s - loss: 0.4592 - acc: 0.9654 - val_loss: 0.4477 - val_acc: 0.9638\nEpoch 15/1000\n - 0s - loss: 0.4289 - acc: 0.9688 - val_loss: 0.4452 - val_acc: 0.9653\nEpoch 16/1000\n - 1s - loss: 0.4368 - acc: 0.9665 - val_loss: 0.4463 - val_acc: 0.9623\nEpoch 17/1000\n - 0s - loss: 0.4104 - acc: 0.9721 - val_loss: 0.4377 - val_acc: 0.9623\nEpoch 18/1000\n - 0s - loss: 0.4196 - acc: 0.9661 - val_loss: 0.4323 - val_acc: 0.9668\nEpoch 19/1000\n - 1s - loss: 0.4103 - acc: 0.9725 - val_loss: 0.4252 - val_acc: 0.9683\nEpoch 20/1000\n - 1s - loss: 0.3967 - acc: 0.9748 - val_loss: 0.4427 - val_acc: 0.9668\nEpoch 21/1000\n - 1s - loss: 0.3984 - acc: 0.9733 - val_loss: 0.4321 - val_acc: 0.9593\nEpoch 22/1000\n - 1s - loss: 0.3958 - acc: 0.9752 - val_loss: 0.4192 - val_acc: 0.9623\nEpoch 23/1000\n - 1s - loss: 0.3886 - acc: 0.9729 - val_loss: 0.4420 - val_acc: 0.9532\nEpoch 24/1000\n - 1s - loss: 0.3791 - acc: 0.9748 - val_loss: 0.4465 - val_acc: 0.9548\nEpoch 25/1000\n - 1s - loss: 0.3867 - acc: 0.9744 - val_loss: 0.4240 - val_acc: 0.9608\nEpoch 26/1000\n - 1s - loss: 0.3774 - acc: 0.9736 - val_loss: 0.4493 - val_acc: 0.9593\nEpoch 27/1000\n - 1s - loss: 0.3700 - acc: 0.9789 - val_loss: 0.4428 - val_acc: 0.9593\nEpoch 28/1000\n - 1s - loss: 0.3654 - acc: 0.9774 - val_loss: 0.4233 - val_acc: 0.9623\nEpoch 29/1000\n - 1s - loss: 0.3586 - acc: 0.9808 - val_loss: 0.4310 - val_acc: 0.9593\nEpoch 30/1000\n - 0s - loss: 0.3588 - acc: 0.9800 - val_loss: 0.4400 - val_acc: 0.9593\nEpoch 31/1000\n - 1s - loss: 0.3691 - acc: 0.9733 - val_loss: 0.4713 - val_acc: 0.9427\nEpoch 32/1000\n - 1s - loss: 0.3563 - acc: 0.9853 - val_loss: 0.4123 - val_acc: 0.9578\nEpoch 33/1000\n - 0s - loss: 0.3497 - acc: 0.9782 - val_loss: 0.4114 - val_acc: 0.9638\nEpoch 34/1000\n - 1s - loss: 0.3474 - acc: 0.9785 - val_loss: 0.4148 - val_acc: 0.9532\nEpoch 35/1000\n - 1s - loss: 0.3419 - acc: 0.9831 - val_loss: 0.4599 - val_acc: 0.9563\nEpoch 36/1000\n - 1s - loss: 0.3383 - acc: 0.9800 - val_loss: 0.4027 - val_acc: 0.9698\nEpoch 37/1000\n - 1s - loss: 0.3289 - acc: 0.9861 - val_loss: 0.4044 - val_acc: 0.9563\nEpoch 38/1000\n - 1s - loss: 0.3244 - acc: 0.9831 - val_loss: 0.4691 - val_acc: 0.9532\nEpoch 39/1000\n - 1s - loss: 0.3291 - acc: 0.9842 - val_loss: 0.4244 - val_acc: 0.9548\nEpoch 40/1000\n - 1s - loss: 0.3212 - acc: 0.9853 - val_loss: 0.4142 - val_acc: 0.9608\nEpoch 41/1000\n - 1s - loss: 0.3184 - acc: 0.9849 - val_loss: 0.4041 - val_acc: 0.9578\nEpoch 42/1000\n - 0s - loss: 0.3336 - acc: 0.9767 - val_loss: 0.4114 - val_acc: 0.9578\nEpoch 43/1000\n - 1s - loss: 0.3140 - acc: 0.9861 - val_loss: 0.4223 - val_acc: 0.9623\nEpoch 44/1000\n - 1s - loss: 0.3205 - acc: 0.9816 - val_loss: 0.4029 - val_acc: 0.9548\nEpoch 45/1000\n - 0s - loss: 0.3281 - acc: 0.9804 - val_loss: 0.4449 - val_acc: 0.9502\nEpoch 46/1000\n - 1s - loss: 0.3162 - acc: 0.9827 - val_loss: 0.4210 - val_acc: 0.9638\nEpoch 47/1000\n - 0s - loss: 0.3139 - acc: 0.9846 - val_loss: 0.4187 - val_acc: 0.9608\nEpoch 48/1000\n - 0s - loss: 0.3096 - acc: 0.9846 - val_loss: 0.3998 - val_acc: 0.9593\nEpoch 49/1000\n - 1s - loss: 0.3086 - acc: 0.9846 - val_loss: 0.3985 - val_acc: 0.9517\nEpoch 50/1000\n - 1s - loss: 0.3090 - acc: 0.9831 - val_loss: 0.4292 - val_acc: 0.9593\nEpoch 51/1000\n - 1s - loss: 0.2970 - acc: 0.9876 - val_loss: 0.4098 - val_acc: 0.9548\nEpoch 52/1000\n - 1s - loss: 0.3017 - acc: 0.9846 - val_loss: 0.4273 - val_acc: 0.9593\nEpoch 53/1000\n - 0s - loss: 0.2925 - acc: 0.9876 - val_loss: 0.3885 - val_acc: 0.9578\nEpoch 54/1000\n - 1s - loss: 0.3025 - acc: 0.9831 - val_loss: 0.4397 - val_acc: 0.9578\nEpoch 55/1000\n - 1s - loss: 0.3020 - acc: 0.9816 - val_loss: 0.4321 - val_acc: 0.9548\nEpoch 56/1000\n - 1s - loss: 0.3220 - acc: 0.9778 - val_loss: 0.3973 - val_acc: 0.9578\nEpoch 57/1000\n - 1s - loss: 0.2889 - acc: 0.9872 - val_loss: 0.3997 - val_acc: 0.9517\nEpoch 58/1000\n - 1s - loss: 0.2979 - acc: 0.9834 - val_loss: 0.5858 - val_acc: 0.9020\nEpoch 59/1000\n - 0s - loss: 0.3075 - acc: 0.9797 - val_loss: 0.4006 - val_acc: 0.9578\nEpoch 60/1000\n - 0s - loss: 0.2879 - acc: 0.9868 - val_loss: 0.4184 - val_acc: 0.9487\nEpoch 61/1000\n - 0s - loss: 0.2985 - acc: 0.9834 - val_loss: 0.3922 - val_acc: 0.9593\nEpoch 62/1000\n - 0s - loss: 0.2809 - acc: 0.9880 - val_loss: 0.4100 - val_acc: 0.9517\nEpoch 63/1000\n - 1s - loss: 0.2844 - acc: 0.9876 - val_loss: 0.3899 - val_acc: 0.9578\nEpoch 64/1000\n - 1s - loss: 0.2940 - acc: 0.9846 - val_loss: 0.5648 - val_acc: 0.9367\nEpoch 65/1000\n - 0s - loss: 0.2804 - acc: 0.9857 - val_loss: 0.4319 - val_acc: 0.9563\nEpoch 66/1000\n - 1s - loss: 0.2831 - acc: 0.9857 - val_loss: 0.3972 - val_acc: 0.9532\nEpoch 67/1000\n - 1s - loss: 0.2680 - acc: 0.9895 - val_loss: 0.4011 - val_acc: 0.9563\nEpoch 68/1000\n - 1s - loss: 0.2801 - acc: 0.9864 - val_loss: 0.4001 - val_acc: 0.9548\nEpoch 69/1000\n - 1s - loss: 0.2752 - acc: 0.9880 - val_loss: 0.3974 - val_acc: 0.9548\nEpoch 70/1000\n - 0s - loss: 0.2654 - acc: 0.9902 - val_loss: 0.3972 - val_acc: 0.9517\nEpoch 71/1000\n - 1s - loss: 0.2676 - acc: 0.9891 - val_loss: 0.5509 - val_acc: 0.9382\nEpoch 72/1000\n - 1s - loss: 0.2651 - acc: 0.9891 - val_loss: 0.3850 - val_acc: 0.9578\nEpoch 73/1000\n - 1s - loss: 0.2685 - acc: 0.9887 - val_loss: 0.4042 - val_acc: 0.9548\nEpoch 74/1000\n - 1s - loss: 0.2635 - acc: 0.9891 - val_loss: 0.3856 - val_acc: 0.9517\nEpoch 75/1000\n - 1s - loss: 0.2666 - acc: 0.9872 - val_loss: 0.4409 - val_acc: 0.9517\nEpoch 76/1000\n - 1s - loss: 0.2549 - acc: 0.9902 - val_loss: 0.4195 - val_acc: 0.9563\nEpoch 77/1000\n - 1s - loss: 0.2579 - acc: 0.9913 - val_loss: 0.3841 - val_acc: 0.9563\nEpoch 78/1000\n - 1s - loss: 0.2493 - acc: 0.9910 - val_loss: 0.4155 - val_acc: 0.9578\nEpoch 79/1000\n - 1s - loss: 0.2632 - acc: 0.9861 - val_loss: 0.3962 - val_acc: 0.9548\nEpoch 80/1000\n - 1s - loss: 0.2541 - acc: 0.9895 - val_loss: 0.3849 - val_acc: 0.9593\nEpoch 81/1000\n - 1s - loss: 0.2559 - acc: 0.9898 - val_loss: 0.3757 - val_acc: 0.9608\nEpoch 82/1000\n - 1s - loss: 0.2671 - acc: 0.9864 - val_loss: 0.3796 - val_acc: 0.9517\nEpoch 83/1000\n - 1s - loss: 0.2697 - acc: 0.9819 - val_loss: 0.3639 - val_acc: 0.9623\nEpoch 84/1000\n - 0s - loss: 0.2540 - acc: 0.9895 - val_loss: 0.3744 - val_acc: 0.9593\nEpoch 85/1000\n - 0s - loss: 0.2522 - acc: 0.9887 - val_loss: 0.3888 - val_acc: 0.9548\nEpoch 86/1000\n - 1s - loss: 0.2511 - acc: 0.9887 - val_loss: 0.4039 - val_acc: 0.9563\nEpoch 87/1000\n - 0s - loss: 0.2518 - acc: 0.9898 - val_loss: 0.3789 - val_acc: 0.9593\nEpoch 88/1000\n - 0s - loss: 0.2578 - acc: 0.9853 - val_loss: 0.3851 - val_acc: 0.9638\nEpoch 89/1000\n - 0s - loss: 0.2508 - acc: 0.9891 - val_loss: 0.3978 - val_acc: 0.9502\nEpoch 90/1000\n - 0s - loss: 0.2350 - acc: 0.9928 - val_loss: 0.3778 - val_acc: 0.9532\nEpoch 91/1000\n - 1s - loss: 0.2525 - acc: 0.9887 - val_loss: 0.4019 - val_acc: 0.9623\nEpoch 92/1000\n - 1s - loss: 0.2512 - acc: 0.9864 - val_loss: 0.3889 - val_acc: 0.9593\nEpoch 93/1000\n - 0s - loss: 0.2415 - acc: 0.9913 - val_loss: 0.3975 - val_acc: 0.9502\nEpoch 94/1000\n - 1s - loss: 0.2398 - acc: 0.9913 - val_loss: 0.3821 - val_acc: 0.9487\nEpoch 95/1000\n - 1s - loss: 0.2299 - acc: 0.9932 - val_loss: 0.3757 - val_acc: 0.9487\n","name":"stdout"},{"output_type":"stream","text":"Epoch 96/1000\n - 1s - loss: 0.2447 - acc: 0.9864 - val_loss: 0.3959 - val_acc: 0.9608\nEpoch 97/1000\n - 0s - loss: 0.2340 - acc: 0.9910 - val_loss: 0.4013 - val_acc: 0.9593\nEpoch 98/1000\n - 1s - loss: 0.2405 - acc: 0.9895 - val_loss: 0.3738 - val_acc: 0.9593\nEpoch 99/1000\n - 0s - loss: 0.2313 - acc: 0.9913 - val_loss: 0.3665 - val_acc: 0.9517\nEpoch 100/1000\n - 0s - loss: 0.2267 - acc: 0.9944 - val_loss: 0.4301 - val_acc: 0.9548\nEpoch 101/1000\n - 1s - loss: 0.2259 - acc: 0.9906 - val_loss: 0.4163 - val_acc: 0.9548\nEpoch 102/1000\n - 0s - loss: 0.2425 - acc: 0.9868 - val_loss: 0.4659 - val_acc: 0.9517\nEpoch 103/1000\n - 0s - loss: 0.2203 - acc: 0.9936 - val_loss: 0.3824 - val_acc: 0.9472\nEpoch 104/1000\n - 0s - loss: 0.2359 - acc: 0.9891 - val_loss: 0.3792 - val_acc: 0.9487\nEpoch 105/1000\n - 0s - loss: 0.2409 - acc: 0.9849 - val_loss: 0.3714 - val_acc: 0.9502\nEpoch 106/1000\n - 1s - loss: 0.2226 - acc: 0.9925 - val_loss: 0.3774 - val_acc: 0.9593\nEpoch 107/1000\n - 0s - loss: 0.2313 - acc: 0.9902 - val_loss: 0.3600 - val_acc: 0.9548\nEpoch 108/1000\n - 1s - loss: 0.2267 - acc: 0.9913 - val_loss: 0.3756 - val_acc: 0.9548\nEpoch 109/1000\n - 1s - loss: 0.2201 - acc: 0.9921 - val_loss: 0.3834 - val_acc: 0.9593\nEpoch 110/1000\n - 0s - loss: 0.2145 - acc: 0.9928 - val_loss: 0.3937 - val_acc: 0.9532\nEpoch 111/1000\n - 1s - loss: 0.2155 - acc: 0.9925 - val_loss: 0.3768 - val_acc: 0.9548\nEpoch 112/1000\n - 1s - loss: 0.2134 - acc: 0.9928 - val_loss: 0.3791 - val_acc: 0.9578\nEpoch 113/1000\n - 1s - loss: 0.2340 - acc: 0.9861 - val_loss: 0.3700 - val_acc: 0.9548\nEpoch 114/1000\n - 0s - loss: 0.2284 - acc: 0.9898 - val_loss: 0.4421 - val_acc: 0.9548\nEpoch 115/1000\n - 1s - loss: 0.2347 - acc: 0.9857 - val_loss: 0.3761 - val_acc: 0.9563\nEpoch 116/1000\n - 1s - loss: 0.2213 - acc: 0.9917 - val_loss: 0.3747 - val_acc: 0.9548\nEpoch 117/1000\n - 1s - loss: 0.2259 - acc: 0.9887 - val_loss: 0.3825 - val_acc: 0.9487\nEpoch 118/1000\n - 1s - loss: 0.2175 - acc: 0.9921 - val_loss: 0.3793 - val_acc: 0.9563\nEpoch 119/1000\n - 1s - loss: 0.2106 - acc: 0.9936 - val_loss: 0.3745 - val_acc: 0.9472\nEpoch 120/1000\n - 0s - loss: 0.2096 - acc: 0.9913 - val_loss: 0.3832 - val_acc: 0.9563\nEpoch 121/1000\n - 0s - loss: 0.2167 - acc: 0.9910 - val_loss: 0.3590 - val_acc: 0.9578\nEpoch 122/1000\n - 0s - loss: 0.2071 - acc: 0.9928 - val_loss: 0.3930 - val_acc: 0.9532\nEpoch 123/1000\n - 1s - loss: 0.2129 - acc: 0.9932 - val_loss: 0.3799 - val_acc: 0.9623\nEpoch 124/1000\n - 0s - loss: 0.2115 - acc: 0.9913 - val_loss: 0.3952 - val_acc: 0.9608\nEpoch 125/1000\n - 0s - loss: 0.1990 - acc: 0.9955 - val_loss: 0.3547 - val_acc: 0.9472\nEpoch 126/1000\n - 0s - loss: 0.2078 - acc: 0.9921 - val_loss: 0.3606 - val_acc: 0.9517\nEpoch 127/1000\n - 0s - loss: 0.2178 - acc: 0.9902 - val_loss: 0.4232 - val_acc: 0.9397\nEpoch 128/1000\n - 1s - loss: 0.2237 - acc: 0.9864 - val_loss: 0.4516 - val_acc: 0.9502\nEpoch 129/1000\n - 1s - loss: 0.2184 - acc: 0.9895 - val_loss: 0.3658 - val_acc: 0.9532\nEpoch 130/1000\n - 1s - loss: 0.1989 - acc: 0.9947 - val_loss: 0.3611 - val_acc: 0.9563\nEpoch 131/1000\n - 1s - loss: 0.2091 - acc: 0.9913 - val_loss: 0.3561 - val_acc: 0.9548\nEpoch 132/1000\n - 1s - loss: 0.2035 - acc: 0.9913 - val_loss: 0.3670 - val_acc: 0.9502\nEpoch 133/1000\n - 1s - loss: 0.1996 - acc: 0.9936 - val_loss: 0.3482 - val_acc: 0.9563\nEpoch 134/1000\n - 1s - loss: 0.2129 - acc: 0.9895 - val_loss: 0.3492 - val_acc: 0.9578\nEpoch 135/1000\n - 1s - loss: 0.1993 - acc: 0.9936 - val_loss: 0.4036 - val_acc: 0.9563\nEpoch 136/1000\n - 1s - loss: 0.2146 - acc: 0.9887 - val_loss: 0.3890 - val_acc: 0.9563\nEpoch 137/1000\n - 0s - loss: 0.1928 - acc: 0.9944 - val_loss: 0.3581 - val_acc: 0.9563\nEpoch 138/1000\n - 1s - loss: 0.2038 - acc: 0.9917 - val_loss: 0.4191 - val_acc: 0.9578\nEpoch 139/1000\n - 0s - loss: 0.2084 - acc: 0.9891 - val_loss: 0.3576 - val_acc: 0.9593\nEpoch 140/1000\n - 0s - loss: 0.2000 - acc: 0.9917 - val_loss: 0.3653 - val_acc: 0.9578\nEpoch 141/1000\n - 0s - loss: 0.2071 - acc: 0.9925 - val_loss: 0.4272 - val_acc: 0.9563\nEpoch 142/1000\n - 1s - loss: 0.2036 - acc: 0.9913 - val_loss: 0.3744 - val_acc: 0.9623\nEpoch 143/1000\n - 0s - loss: 0.2069 - acc: 0.9880 - val_loss: 0.3516 - val_acc: 0.9578\nEpoch 144/1000\n - 1s - loss: 0.2019 - acc: 0.9917 - val_loss: 0.3633 - val_acc: 0.9442\nEpoch 145/1000\n - 1s - loss: 0.2148 - acc: 0.9876 - val_loss: 0.4399 - val_acc: 0.9532\nEpoch 146/1000\n - 1s - loss: 0.2006 - acc: 0.9906 - val_loss: 0.3729 - val_acc: 0.9502\nEpoch 147/1000\n - 0s - loss: 0.2011 - acc: 0.9902 - val_loss: 0.3581 - val_acc: 0.9472\nEpoch 148/1000\n - 1s - loss: 0.1991 - acc: 0.9898 - val_loss: 0.3704 - val_acc: 0.9517\nEpoch 149/1000\n - 1s - loss: 0.1995 - acc: 0.9913 - val_loss: 0.3684 - val_acc: 0.9578\nEpoch 150/1000\n - 1s - loss: 0.1933 - acc: 0.9940 - val_loss: 0.3793 - val_acc: 0.9502\nEpoch 151/1000\n - 1s - loss: 0.1837 - acc: 0.9959 - val_loss: 0.4250 - val_acc: 0.9563\nEpoch 152/1000\n - 1s - loss: 0.1934 - acc: 0.9932 - val_loss: 0.3926 - val_acc: 0.9563\nEpoch 153/1000\n - 1s - loss: 0.1877 - acc: 0.9936 - val_loss: 0.3554 - val_acc: 0.9532\nEpoch 154/1000\n - 1s - loss: 0.1794 - acc: 0.9962 - val_loss: 0.3840 - val_acc: 0.9532\nEpoch 155/1000\n - 1s - loss: 0.1928 - acc: 0.9917 - val_loss: 0.3335 - val_acc: 0.9548\nEpoch 156/1000\n - 1s - loss: 0.2062 - acc: 0.9861 - val_loss: 0.3402 - val_acc: 0.9593\nEpoch 157/1000\n - 1s - loss: 0.1862 - acc: 0.9925 - val_loss: 0.3604 - val_acc: 0.9472\nEpoch 158/1000\n - 1s - loss: 0.1904 - acc: 0.9932 - val_loss: 0.4859 - val_acc: 0.9427\nEpoch 159/1000\n - 1s - loss: 0.1888 - acc: 0.9925 - val_loss: 0.3473 - val_acc: 0.9563\nEpoch 160/1000\n - 1s - loss: 0.1903 - acc: 0.9932 - val_loss: 0.3874 - val_acc: 0.9532\nEpoch 161/1000\n - 0s - loss: 0.1925 - acc: 0.9895 - val_loss: 0.3571 - val_acc: 0.9608\nEpoch 162/1000\n - 1s - loss: 0.1909 - acc: 0.9925 - val_loss: 0.3474 - val_acc: 0.9563\nEpoch 163/1000\n - 1s - loss: 0.1927 - acc: 0.9910 - val_loss: 0.3437 - val_acc: 0.9653\nEpoch 164/1000\n - 1s - loss: 0.1864 - acc: 0.9917 - val_loss: 0.3662 - val_acc: 0.9638\nEpoch 165/1000\n - 0s - loss: 0.1867 - acc: 0.9921 - val_loss: 0.3624 - val_acc: 0.9502\nEpoch 166/1000\n - 0s - loss: 0.1776 - acc: 0.9940 - val_loss: 0.3509 - val_acc: 0.9532\nEpoch 167/1000\n - 1s - loss: 0.1865 - acc: 0.9928 - val_loss: 0.4135 - val_acc: 0.9517\nEpoch 168/1000\n - 1s - loss: 0.1811 - acc: 0.9947 - val_loss: 0.3620 - val_acc: 0.9427\nEpoch 169/1000\n - 0s - loss: 0.1887 - acc: 0.9906 - val_loss: 0.3581 - val_acc: 0.9532\nEpoch 170/1000\n - 0s - loss: 0.2038 - acc: 0.9887 - val_loss: 0.3335 - val_acc: 0.9548\nEpoch 171/1000\n - 0s - loss: 0.1785 - acc: 0.9936 - val_loss: 0.3360 - val_acc: 0.9548\nEpoch 172/1000\n - 1s - loss: 0.1779 - acc: 0.9925 - val_loss: 0.3640 - val_acc: 0.9442\nEpoch 173/1000\n - 0s - loss: 0.1812 - acc: 0.9944 - val_loss: 0.3518 - val_acc: 0.9548\nEpoch 174/1000\n - 1s - loss: 0.1845 - acc: 0.9921 - val_loss: 0.3776 - val_acc: 0.9593\nEpoch 175/1000\n - 1s - loss: 0.1777 - acc: 0.9947 - val_loss: 0.3741 - val_acc: 0.9472\nEpoch 176/1000\n - 1s - loss: 0.1787 - acc: 0.9932 - val_loss: 0.3591 - val_acc: 0.9548\nEpoch 177/1000\n - 1s - loss: 0.1755 - acc: 0.9947 - val_loss: 0.3533 - val_acc: 0.9548\nEpoch 178/1000\n - 1s - loss: 0.1879 - acc: 0.9913 - val_loss: 0.3689 - val_acc: 0.9593\nEpoch 179/1000\n - 1s - loss: 0.1720 - acc: 0.9955 - val_loss: 0.3590 - val_acc: 0.9502\nEpoch 180/1000\n - 1s - loss: 0.1746 - acc: 0.9944 - val_loss: 0.3371 - val_acc: 0.9563\nEpoch 181/1000\n - 1s - loss: 0.1699 - acc: 0.9947 - val_loss: 0.3477 - val_acc: 0.9548\nEpoch 182/1000\n - 1s - loss: 0.1787 - acc: 0.9928 - val_loss: 0.3469 - val_acc: 0.9472\nEpoch 183/1000\n - 1s - loss: 0.1713 - acc: 0.9944 - val_loss: 0.3661 - val_acc: 0.9532\nEpoch 184/1000\n - 1s - loss: 0.1773 - acc: 0.9936 - val_loss: 0.3957 - val_acc: 0.9563\nEpoch 185/1000\n - 0s - loss: 0.1805 - acc: 0.9940 - val_loss: 0.3466 - val_acc: 0.9578\nEpoch 186/1000\n - 1s - loss: 0.1692 - acc: 0.9959 - val_loss: 0.3462 - val_acc: 0.9548\nEpoch 187/1000\n - 1s - loss: 0.1728 - acc: 0.9940 - val_loss: 0.3631 - val_acc: 0.9532\nEpoch 188/1000\n - 1s - loss: 0.1673 - acc: 0.9951 - val_loss: 0.3744 - val_acc: 0.9548\nEpoch 189/1000\n - 0s - loss: 0.1738 - acc: 0.9940 - val_loss: 0.3355 - val_acc: 0.9578\nEpoch 190/1000\n - 1s - loss: 0.1657 - acc: 0.9951 - val_loss: 0.3714 - val_acc: 0.9517\n","name":"stdout"},{"output_type":"stream","text":"Epoch 191/1000\n - 1s - loss: 0.1617 - acc: 0.9959 - val_loss: 0.3665 - val_acc: 0.9442\nEpoch 192/1000\n - 1s - loss: 0.1642 - acc: 0.9955 - val_loss: 0.3746 - val_acc: 0.9563\nEpoch 193/1000\n - 0s - loss: 0.1768 - acc: 0.9921 - val_loss: 0.3470 - val_acc: 0.9548\nEpoch 194/1000\n - 1s - loss: 0.1755 - acc: 0.9925 - val_loss: 0.4791 - val_acc: 0.9472\nEpoch 195/1000\n - 1s - loss: 0.1693 - acc: 0.9944 - val_loss: 0.3524 - val_acc: 0.9548\nEpoch 196/1000\n - 0s - loss: 0.1714 - acc: 0.9928 - val_loss: 0.3592 - val_acc: 0.9442\nEpoch 197/1000\n - 1s - loss: 0.1642 - acc: 0.9944 - val_loss: 0.3375 - val_acc: 0.9578\nEpoch 198/1000\n - 1s - loss: 0.1976 - acc: 0.9872 - val_loss: 0.3395 - val_acc: 0.9623\nEpoch 199/1000\n - 0s - loss: 0.1740 - acc: 0.9925 - val_loss: 0.3509 - val_acc: 0.9608\nEpoch 200/1000\n - 1s - loss: 0.1645 - acc: 0.9940 - val_loss: 0.3422 - val_acc: 0.9472\nEpoch 201/1000\n - 0s - loss: 0.1651 - acc: 0.9944 - val_loss: 0.3518 - val_acc: 0.9593\nEpoch 202/1000\n - 1s - loss: 0.1703 - acc: 0.9928 - val_loss: 0.3375 - val_acc: 0.9517\nEpoch 203/1000\n - 1s - loss: 0.1643 - acc: 0.9940 - val_loss: 0.3562 - val_acc: 0.9563\nEpoch 204/1000\n - 0s - loss: 0.1685 - acc: 0.9928 - val_loss: 0.3114 - val_acc: 0.9638\nEpoch 205/1000\n - 1s - loss: 0.1687 - acc: 0.9940 - val_loss: 0.3449 - val_acc: 0.9593\nEpoch 206/1000\n - 1s - loss: 0.1575 - acc: 0.9951 - val_loss: 0.3304 - val_acc: 0.9578\nEpoch 207/1000\n - 1s - loss: 0.1740 - acc: 0.9917 - val_loss: 0.3316 - val_acc: 0.9517\nEpoch 208/1000\n - 1s - loss: 0.1586 - acc: 0.9955 - val_loss: 0.3334 - val_acc: 0.9578\nEpoch 209/1000\n - 1s - loss: 0.1620 - acc: 0.9944 - val_loss: 0.3468 - val_acc: 0.9608\nEpoch 210/1000\n - 1s - loss: 0.1652 - acc: 0.9951 - val_loss: 0.3284 - val_acc: 0.9563\nEpoch 211/1000\n - 0s - loss: 0.1649 - acc: 0.9932 - val_loss: 0.3684 - val_acc: 0.9487\nEpoch 212/1000\n - 1s - loss: 0.1525 - acc: 0.9966 - val_loss: 0.3209 - val_acc: 0.9548\nEpoch 213/1000\n - 0s - loss: 0.1630 - acc: 0.9932 - val_loss: 0.3407 - val_acc: 0.9593\nEpoch 214/1000\n - 0s - loss: 0.1726 - acc: 0.9910 - val_loss: 0.4023 - val_acc: 0.9563\nEpoch 215/1000\n - 1s - loss: 0.1585 - acc: 0.9944 - val_loss: 0.3195 - val_acc: 0.9623\nEpoch 216/1000\n - 0s - loss: 0.1570 - acc: 0.9940 - val_loss: 0.3404 - val_acc: 0.9548\nEpoch 217/1000\n - 0s - loss: 0.1769 - acc: 0.9906 - val_loss: 0.3263 - val_acc: 0.9548\nEpoch 218/1000\n - 1s - loss: 0.1497 - acc: 0.9974 - val_loss: 0.3322 - val_acc: 0.9563\nEpoch 219/1000\n - 0s - loss: 0.1632 - acc: 0.9936 - val_loss: 0.3404 - val_acc: 0.9487\nEpoch 220/1000\n - 1s - loss: 0.1637 - acc: 0.9917 - val_loss: 0.3444 - val_acc: 0.9623\nEpoch 221/1000\n - 0s - loss: 0.1623 - acc: 0.9955 - val_loss: 0.3363 - val_acc: 0.9517\nEpoch 222/1000\n - 0s - loss: 0.1581 - acc: 0.9955 - val_loss: 0.3470 - val_acc: 0.9487\nEpoch 223/1000\n - 0s - loss: 0.1759 - acc: 0.9898 - val_loss: 0.3511 - val_acc: 0.9517\nEpoch 224/1000\n - 0s - loss: 0.1632 - acc: 0.9932 - val_loss: 0.3276 - val_acc: 0.9578\nEpoch 225/1000\n - 0s - loss: 0.1527 - acc: 0.9951 - val_loss: 0.3347 - val_acc: 0.9608\nEpoch 226/1000\n - 0s - loss: 0.1523 - acc: 0.9959 - val_loss: 0.3388 - val_acc: 0.9548\nEpoch 227/1000\n - 0s - loss: 0.1634 - acc: 0.9910 - val_loss: 0.3398 - val_acc: 0.9487\nEpoch 228/1000\n - 1s - loss: 0.1601 - acc: 0.9932 - val_loss: 0.3280 - val_acc: 0.9608\nEpoch 229/1000\n - 0s - loss: 0.1529 - acc: 0.9951 - val_loss: 0.3404 - val_acc: 0.9563\nEpoch 230/1000\n - 1s - loss: 0.1519 - acc: 0.9947 - val_loss: 0.3402 - val_acc: 0.9563\nEpoch 231/1000\n - 0s - loss: 0.1553 - acc: 0.9947 - val_loss: 0.3236 - val_acc: 0.9532\nEpoch 232/1000\n - 0s - loss: 0.1549 - acc: 0.9936 - val_loss: 0.3216 - val_acc: 0.9563\nEpoch 233/1000\n - 1s - loss: 0.1566 - acc: 0.9944 - val_loss: 0.4218 - val_acc: 0.9502\nEpoch 234/1000\n - 0s - loss: 0.1581 - acc: 0.9932 - val_loss: 0.3389 - val_acc: 0.9502\nEpoch 235/1000\n - 1s - loss: 0.1770 - acc: 0.9868 - val_loss: 0.3778 - val_acc: 0.9563\nEpoch 236/1000\n - 0s - loss: 0.1635 - acc: 0.9906 - val_loss: 0.3958 - val_acc: 0.9578\nEpoch 237/1000\n - 0s - loss: 0.1633 - acc: 0.9925 - val_loss: 0.3635 - val_acc: 0.9593\nEpoch 238/1000\n - 0s - loss: 0.1559 - acc: 0.9936 - val_loss: 0.3408 - val_acc: 0.9608\nEpoch 239/1000\n - 0s - loss: 0.1483 - acc: 0.9947 - val_loss: 0.3583 - val_acc: 0.9563\nEpoch 240/1000\n - 0s - loss: 0.1476 - acc: 0.9962 - val_loss: 0.3368 - val_acc: 0.9593\nEpoch 241/1000\n - 0s - loss: 0.1573 - acc: 0.9925 - val_loss: 0.4326 - val_acc: 0.9487\nEpoch 242/1000\n - 0s - loss: 0.1495 - acc: 0.9959 - val_loss: 0.3169 - val_acc: 0.9593\nEpoch 243/1000\n - 0s - loss: 0.1448 - acc: 0.9951 - val_loss: 0.3091 - val_acc: 0.9608\nEpoch 244/1000\n - 0s - loss: 0.1598 - acc: 0.9925 - val_loss: 0.3320 - val_acc: 0.9472\nEpoch 245/1000\n - 0s - loss: 0.1579 - acc: 0.9932 - val_loss: 0.3709 - val_acc: 0.9578\nEpoch 246/1000\n - 1s - loss: 0.1700 - acc: 0.9906 - val_loss: 0.3381 - val_acc: 0.9578\nEpoch 247/1000\n - 0s - loss: 0.1451 - acc: 0.9966 - val_loss: 0.3251 - val_acc: 0.9593\nEpoch 248/1000\n - 0s - loss: 0.1549 - acc: 0.9925 - val_loss: 0.3616 - val_acc: 0.9593\nEpoch 249/1000\n - 0s - loss: 0.1517 - acc: 0.9947 - val_loss: 0.3914 - val_acc: 0.9593\nEpoch 250/1000\n - 0s - loss: 0.1429 - acc: 0.9962 - val_loss: 0.3344 - val_acc: 0.9548\nEpoch 251/1000\n - 0s - loss: 0.1396 - acc: 0.9970 - val_loss: 0.3122 - val_acc: 0.9623\nEpoch 252/1000\n - 1s - loss: 0.1347 - acc: 0.9974 - val_loss: 0.3215 - val_acc: 0.9608\nEpoch 253/1000\n - 0s - loss: 0.1480 - acc: 0.9936 - val_loss: 0.3376 - val_acc: 0.9563\nEpoch 254/1000\n - 0s - loss: 0.1515 - acc: 0.9947 - val_loss: 0.3318 - val_acc: 0.9593\nEpoch 255/1000\n - 0s - loss: 0.1540 - acc: 0.9940 - val_loss: 0.3529 - val_acc: 0.9593\nEpoch 256/1000\n - 0s - loss: 0.1462 - acc: 0.9959 - val_loss: 0.4031 - val_acc: 0.9563\nEpoch 257/1000\n - 0s - loss: 0.1659 - acc: 0.9921 - val_loss: 0.3429 - val_acc: 0.9548\nEpoch 258/1000\n - 0s - loss: 0.1429 - acc: 0.9962 - val_loss: 0.3281 - val_acc: 0.9548\nEpoch 259/1000\n - 0s - loss: 0.1498 - acc: 0.9947 - val_loss: 0.3312 - val_acc: 0.9578\nEpoch 260/1000\n - 1s - loss: 0.1567 - acc: 0.9913 - val_loss: 0.3637 - val_acc: 0.9593\nEpoch 261/1000\n - 1s - loss: 0.1390 - acc: 0.9955 - val_loss: 0.3431 - val_acc: 0.9563\nEpoch 262/1000\n - 1s - loss: 0.1491 - acc: 0.9944 - val_loss: 0.3602 - val_acc: 0.9563\nEpoch 263/1000\n - 0s - loss: 0.1474 - acc: 0.9944 - val_loss: 0.3525 - val_acc: 0.9442\nEpoch 264/1000\n - 1s - loss: 0.1452 - acc: 0.9947 - val_loss: 0.3161 - val_acc: 0.9593\nEpoch 265/1000\n - 0s - loss: 0.1488 - acc: 0.9928 - val_loss: 0.3315 - val_acc: 0.9638\nEpoch 266/1000\n - 0s - loss: 0.1556 - acc: 0.9917 - val_loss: 0.3619 - val_acc: 0.9593\nEpoch 267/1000\n - 0s - loss: 0.1448 - acc: 0.9940 - val_loss: 0.3314 - val_acc: 0.9578\nEpoch 268/1000\n - 1s - loss: 0.1345 - acc: 0.9974 - val_loss: 0.3434 - val_acc: 0.9608\nEpoch 269/1000\n - 0s - loss: 0.1481 - acc: 0.9951 - val_loss: 0.3370 - val_acc: 0.9563\nEpoch 270/1000\n - 0s - loss: 0.1356 - acc: 0.9977 - val_loss: 0.3379 - val_acc: 0.9578\nEpoch 271/1000\n - 0s - loss: 0.1318 - acc: 0.9962 - val_loss: 0.4052 - val_acc: 0.9517\nEpoch 272/1000\n - 1s - loss: 0.1574 - acc: 0.9910 - val_loss: 0.3653 - val_acc: 0.9608\nEpoch 273/1000\n - 0s - loss: 0.1519 - acc: 0.9921 - val_loss: 0.3292 - val_acc: 0.9668\nEpoch 274/1000\n - 0s - loss: 0.1447 - acc: 0.9936 - val_loss: 0.3284 - val_acc: 0.9563\nEpoch 275/1000\n - 1s - loss: 0.1427 - acc: 0.9947 - val_loss: 0.3551 - val_acc: 0.9578\nEpoch 276/1000\n - 1s - loss: 0.1304 - acc: 0.9974 - val_loss: 0.3087 - val_acc: 0.9638\nEpoch 277/1000\n - 0s - loss: 0.1625 - acc: 0.9906 - val_loss: 0.3356 - val_acc: 0.9532\nEpoch 278/1000\n - 0s - loss: 0.1367 - acc: 0.9959 - val_loss: 0.3235 - val_acc: 0.9578\nEpoch 279/1000\n - 0s - loss: 0.1443 - acc: 0.9944 - val_loss: 0.3466 - val_acc: 0.9578\nEpoch 280/1000\n - 0s - loss: 0.1306 - acc: 0.9970 - val_loss: 0.3464 - val_acc: 0.9442\nEpoch 281/1000\n - 0s - loss: 0.1274 - acc: 0.9974 - val_loss: 0.3434 - val_acc: 0.9578\nEpoch 282/1000\n - 1s - loss: 0.1379 - acc: 0.9966 - val_loss: 0.3739 - val_acc: 0.9578\nEpoch 283/1000\n - 0s - loss: 0.1433 - acc: 0.9921 - val_loss: 0.3488 - val_acc: 0.9532\nEpoch 284/1000\n - 1s - loss: 0.1436 - acc: 0.9947 - val_loss: 0.3327 - val_acc: 0.9563\nEpoch 285/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.1285 - acc: 0.9970 - val_loss: 0.3278 - val_acc: 0.9578\nEpoch 286/1000\n - 0s - loss: 0.1575 - acc: 0.9913 - val_loss: 0.3432 - val_acc: 0.9548\nEpoch 287/1000\n - 0s - loss: 0.1324 - acc: 0.9962 - val_loss: 0.3671 - val_acc: 0.9563\nEpoch 288/1000\n - 0s - loss: 0.1248 - acc: 0.9977 - val_loss: 0.3255 - val_acc: 0.9532\nEpoch 289/1000\n - 0s - loss: 0.1458 - acc: 0.9925 - val_loss: 0.3523 - val_acc: 0.9638\nEpoch 290/1000\n - 1s - loss: 0.1311 - acc: 0.9951 - val_loss: 0.3915 - val_acc: 0.9548\nEpoch 291/1000\n - 1s - loss: 0.1403 - acc: 0.9940 - val_loss: 0.3228 - val_acc: 0.9502\nEpoch 292/1000\n - 1s - loss: 0.1504 - acc: 0.9910 - val_loss: 0.3513 - val_acc: 0.9457\nEpoch 293/1000\n - 1s - loss: 0.1322 - acc: 0.9955 - val_loss: 0.3205 - val_acc: 0.9578\nEpoch 294/1000\n - 1s - loss: 0.1448 - acc: 0.9936 - val_loss: 0.3267 - val_acc: 0.9502\nEpoch 295/1000\n - 1s - loss: 0.1356 - acc: 0.9955 - val_loss: 0.3293 - val_acc: 0.9593\nEpoch 296/1000\n - 1s - loss: 0.1264 - acc: 0.9966 - val_loss: 0.3720 - val_acc: 0.9548\nEpoch 297/1000\n - 1s - loss: 0.1340 - acc: 0.9944 - val_loss: 0.3232 - val_acc: 0.9578\nEpoch 298/1000\n - 1s - loss: 0.1510 - acc: 0.9913 - val_loss: 0.4507 - val_acc: 0.9487\nEpoch 299/1000\n - 1s - loss: 0.1361 - acc: 0.9966 - val_loss: 0.3577 - val_acc: 0.9563\nEpoch 300/1000\n - 1s - loss: 0.1336 - acc: 0.9944 - val_loss: 0.3492 - val_acc: 0.9578\nEpoch 301/1000\n - 1s - loss: 0.1509 - acc: 0.9921 - val_loss: 0.3175 - val_acc: 0.9578\nEpoch 302/1000\n - 1s - loss: 0.1397 - acc: 0.9936 - val_loss: 0.3437 - val_acc: 0.9593\nEpoch 303/1000\n - 1s - loss: 0.1355 - acc: 0.9955 - val_loss: 0.3194 - val_acc: 0.9517\nEpoch 304/1000\n - 1s - loss: 0.1232 - acc: 0.9962 - val_loss: 0.3267 - val_acc: 0.9623\nEpoch 305/1000\n - 1s - loss: 0.1347 - acc: 0.9955 - val_loss: 0.3257 - val_acc: 0.9593\nEpoch 306/1000\n - 1s - loss: 0.1236 - acc: 0.9959 - val_loss: 0.3680 - val_acc: 0.9548\nEpoch 307/1000\n - 1s - loss: 0.1493 - acc: 0.9925 - val_loss: 0.3188 - val_acc: 0.9502\nEpoch 308/1000\n - 1s - loss: 0.1438 - acc: 0.9921 - val_loss: 0.3206 - val_acc: 0.9638\nEpoch 309/1000\n - 1s - loss: 0.1312 - acc: 0.9962 - val_loss: 0.3337 - val_acc: 0.9502\nEpoch 310/1000\n - 1s - loss: 0.1394 - acc: 0.9932 - val_loss: 0.3291 - val_acc: 0.9593\nEpoch 311/1000\n - 1s - loss: 0.1353 - acc: 0.9936 - val_loss: 0.3355 - val_acc: 0.9563\nEpoch 312/1000\n - 0s - loss: 0.1302 - acc: 0.9962 - val_loss: 0.3389 - val_acc: 0.9683\nEpoch 313/1000\n - 0s - loss: 0.1418 - acc: 0.9925 - val_loss: 0.3974 - val_acc: 0.9593\nEpoch 314/1000\n - 1s - loss: 0.1525 - acc: 0.9906 - val_loss: 0.3252 - val_acc: 0.9578\nEpoch 315/1000\n - 0s - loss: 0.1388 - acc: 0.9947 - val_loss: 0.3449 - val_acc: 0.9638\nEpoch 316/1000\n - 0s - loss: 0.1338 - acc: 0.9947 - val_loss: 0.3659 - val_acc: 0.9593\nEpoch 317/1000\n - 1s - loss: 0.1330 - acc: 0.9959 - val_loss: 0.3308 - val_acc: 0.9548\nEpoch 318/1000\n - 0s - loss: 0.1373 - acc: 0.9940 - val_loss: 0.3780 - val_acc: 0.9412\nEpoch 319/1000\n - 0s - loss: 0.1402 - acc: 0.9936 - val_loss: 0.3201 - val_acc: 0.9638\nEpoch 320/1000\n - 0s - loss: 0.1268 - acc: 0.9966 - val_loss: 0.3208 - val_acc: 0.9517\nEpoch 321/1000\n - 1s - loss: 0.1382 - acc: 0.9940 - val_loss: 0.4250 - val_acc: 0.9517\nEpoch 322/1000\n - 1s - loss: 0.1301 - acc: 0.9959 - val_loss: 0.3115 - val_acc: 0.9563\nEpoch 323/1000\n - 0s - loss: 0.1243 - acc: 0.9974 - val_loss: 0.3014 - val_acc: 0.9563\nEpoch 324/1000\n - 1s - loss: 0.1407 - acc: 0.9940 - val_loss: 0.3177 - val_acc: 0.9623\nEpoch 325/1000\n - 1s - loss: 0.1390 - acc: 0.9940 - val_loss: 0.3125 - val_acc: 0.9517\nEpoch 326/1000\n - 0s - loss: 0.1433 - acc: 0.9928 - val_loss: 0.3184 - val_acc: 0.9608\nEpoch 327/1000\n - 0s - loss: 0.1219 - acc: 0.9974 - val_loss: 0.3356 - val_acc: 0.9502\nEpoch 328/1000\n - 0s - loss: 0.1209 - acc: 0.9974 - val_loss: 0.3222 - val_acc: 0.9623\nEpoch 329/1000\n - 0s - loss: 0.1288 - acc: 0.9955 - val_loss: 0.3176 - val_acc: 0.9517\nEpoch 330/1000\n - 0s - loss: 0.1236 - acc: 0.9951 - val_loss: 0.3885 - val_acc: 0.9548\nEpoch 331/1000\n - 0s - loss: 0.1174 - acc: 0.9977 - val_loss: 0.3382 - val_acc: 0.9532\nEpoch 332/1000\n - 0s - loss: 0.1263 - acc: 0.9970 - val_loss: 0.3430 - val_acc: 0.9593\nEpoch 333/1000\n - 0s - loss: 0.1288 - acc: 0.9932 - val_loss: 0.3389 - val_acc: 0.9457\nEpoch 334/1000\n - 0s - loss: 0.1507 - acc: 0.9895 - val_loss: 0.3479 - val_acc: 0.9502\nEpoch 335/1000\n - 0s - loss: 0.1345 - acc: 0.9947 - val_loss: 0.3274 - val_acc: 0.9608\nEpoch 336/1000\n - 0s - loss: 0.1344 - acc: 0.9936 - val_loss: 0.3244 - val_acc: 0.9593\nEpoch 337/1000\n - 1s - loss: 0.1260 - acc: 0.9959 - val_loss: 0.3223 - val_acc: 0.9517\nEpoch 338/1000\n - 0s - loss: 0.1558 - acc: 0.9917 - val_loss: 0.3345 - val_acc: 0.9623\nEpoch 339/1000\n - 0s - loss: 0.1491 - acc: 0.9906 - val_loss: 0.3222 - val_acc: 0.9668\nEpoch 340/1000\n - 0s - loss: 0.1293 - acc: 0.9947 - val_loss: 0.3308 - val_acc: 0.9608\nEpoch 341/1000\n - 0s - loss: 0.1471 - acc: 0.9925 - val_loss: 0.3430 - val_acc: 0.9472\nEpoch 342/1000\n - 0s - loss: 0.1216 - acc: 0.9970 - val_loss: 0.3086 - val_acc: 0.9548\nEpoch 343/1000\n - 0s - loss: 0.1245 - acc: 0.9947 - val_loss: 0.3762 - val_acc: 0.9517\nEpoch 344/1000\n - 0s - loss: 0.1413 - acc: 0.9925 - val_loss: 0.3116 - val_acc: 0.9563\nEpoch 345/1000\n - 0s - loss: 0.1249 - acc: 0.9955 - val_loss: 0.3145 - val_acc: 0.9517\nEpoch 346/1000\n - 1s - loss: 0.1163 - acc: 0.9977 - val_loss: 0.3250 - val_acc: 0.9517\nEpoch 347/1000\n - 0s - loss: 0.1308 - acc: 0.9944 - val_loss: 0.3298 - val_acc: 0.9517\nEpoch 348/1000\n - 1s - loss: 0.1229 - acc: 0.9947 - val_loss: 0.3513 - val_acc: 0.9593\nEpoch 349/1000\n - 0s - loss: 0.1142 - acc: 0.9966 - val_loss: 0.3292 - val_acc: 0.9548\nEpoch 350/1000\n - 0s - loss: 0.1175 - acc: 0.9970 - val_loss: 0.3323 - val_acc: 0.9502\nEpoch 351/1000\n - 1s - loss: 0.1204 - acc: 0.9966 - val_loss: 0.3478 - val_acc: 0.9608\nEpoch 352/1000\n - 0s - loss: 0.1146 - acc: 0.9962 - val_loss: 0.3438 - val_acc: 0.9517\nEpoch 353/1000\n - 0s - loss: 0.1186 - acc: 0.9966 - val_loss: 0.3420 - val_acc: 0.9563\nEpoch 354/1000\n - 1s - loss: 0.1421 - acc: 0.9910 - val_loss: 0.3144 - val_acc: 0.9578\nEpoch 355/1000\n - 1s - loss: 0.1513 - acc: 0.9887 - val_loss: 0.3394 - val_acc: 0.9608\nEpoch 356/1000\n - 1s - loss: 0.1320 - acc: 0.9947 - val_loss: 0.3565 - val_acc: 0.9472\nEpoch 357/1000\n - 1s - loss: 0.1224 - acc: 0.9962 - val_loss: 0.3303 - val_acc: 0.9517\nEpoch 358/1000\n - 0s - loss: 0.1391 - acc: 0.9910 - val_loss: 0.3452 - val_acc: 0.9457\nEpoch 359/1000\n - 0s - loss: 0.1222 - acc: 0.9970 - val_loss: 0.3124 - val_acc: 0.9578\nEpoch 360/1000\n - 0s - loss: 0.1337 - acc: 0.9902 - val_loss: 0.3711 - val_acc: 0.9608\nEpoch 361/1000\n - 1s - loss: 0.1364 - acc: 0.9944 - val_loss: 0.3294 - val_acc: 0.9608\nEpoch 362/1000\n - 1s - loss: 0.1191 - acc: 0.9955 - val_loss: 0.3187 - val_acc: 0.9623\nEpoch 363/1000\n - 1s - loss: 0.1226 - acc: 0.9951 - val_loss: 0.3081 - val_acc: 0.9532\nEpoch 364/1000\n - 1s - loss: 0.1352 - acc: 0.9921 - val_loss: 0.3502 - val_acc: 0.9593\nEpoch 365/1000\n - 1s - loss: 0.1256 - acc: 0.9962 - val_loss: 0.3339 - val_acc: 0.9487\nEpoch 366/1000\n - 1s - loss: 0.1324 - acc: 0.9947 - val_loss: 0.3186 - val_acc: 0.9578\nEpoch 367/1000\n - 1s - loss: 0.1150 - acc: 0.9959 - val_loss: 0.3167 - val_acc: 0.9578\nEpoch 368/1000\n - 0s - loss: 0.1272 - acc: 0.9940 - val_loss: 0.3270 - val_acc: 0.9623\nEpoch 369/1000\n - 0s - loss: 0.1166 - acc: 0.9966 - val_loss: 0.3249 - val_acc: 0.9608\nEpoch 370/1000\n - 1s - loss: 0.1243 - acc: 0.9947 - val_loss: 0.3348 - val_acc: 0.9517\nEpoch 371/1000\n - 1s - loss: 0.1239 - acc: 0.9955 - val_loss: 0.3452 - val_acc: 0.9487\nEpoch 372/1000\n - 1s - loss: 0.1245 - acc: 0.9959 - val_loss: 0.3160 - val_acc: 0.9548\nEpoch 373/1000\n - 1s - loss: 0.1183 - acc: 0.9962 - val_loss: 0.3461 - val_acc: 0.9608\nEpoch 374/1000\n - 1s - loss: 0.1294 - acc: 0.9940 - val_loss: 0.3157 - val_acc: 0.9578\nEpoch 375/1000\n - 0s - loss: 0.1298 - acc: 0.9936 - val_loss: 0.4044 - val_acc: 0.9608\nEpoch 376/1000\n - 0s - loss: 0.1427 - acc: 0.9921 - val_loss: 0.3373 - val_acc: 0.9517\nEpoch 377/1000\n - 1s - loss: 0.1184 - acc: 0.9966 - val_loss: 0.3294 - val_acc: 0.9593\nEpoch 378/1000\n - 0s - loss: 0.1076 - acc: 0.9977 - val_loss: 0.3131 - val_acc: 0.9563\nEpoch 379/1000\n - 0s - loss: 0.1301 - acc: 0.9925 - val_loss: 0.3443 - val_acc: 0.9608\n","name":"stdout"},{"output_type":"stream","text":"Epoch 380/1000\n - 1s - loss: 0.1275 - acc: 0.9947 - val_loss: 0.3483 - val_acc: 0.9608\nEpoch 381/1000\n - 1s - loss: 0.1361 - acc: 0.9917 - val_loss: 0.3263 - val_acc: 0.9623\nEpoch 382/1000\n - 0s - loss: 0.1278 - acc: 0.9947 - val_loss: 0.3191 - val_acc: 0.9593\nEpoch 383/1000\n - 1s - loss: 0.1154 - acc: 0.9959 - val_loss: 0.3175 - val_acc: 0.9593\nEpoch 384/1000\n - 1s - loss: 0.1243 - acc: 0.9936 - val_loss: 0.3184 - val_acc: 0.9563\nEpoch 385/1000\n - 1s - loss: 0.1229 - acc: 0.9940 - val_loss: 0.3572 - val_acc: 0.9623\nEpoch 386/1000\n - 1s - loss: 0.1189 - acc: 0.9970 - val_loss: 0.3853 - val_acc: 0.9578\nEpoch 387/1000\n - 1s - loss: 0.1438 - acc: 0.9902 - val_loss: 0.3068 - val_acc: 0.9548\nEpoch 388/1000\n - 1s - loss: 0.1165 - acc: 0.9955 - val_loss: 0.4759 - val_acc: 0.9427\nEpoch 389/1000\n - 1s - loss: 0.1428 - acc: 0.9910 - val_loss: 0.3258 - val_acc: 0.9563\nEpoch 390/1000\n - 0s - loss: 0.1241 - acc: 0.9955 - val_loss: 0.3527 - val_acc: 0.9563\nEpoch 391/1000\n - 1s - loss: 0.1210 - acc: 0.9947 - val_loss: 0.3183 - val_acc: 0.9593\nEpoch 392/1000\n - 1s - loss: 0.1306 - acc: 0.9925 - val_loss: 0.3381 - val_acc: 0.9532\nEpoch 393/1000\n - 1s - loss: 0.1209 - acc: 0.9951 - val_loss: 0.3003 - val_acc: 0.9578\nEpoch 394/1000\n - 1s - loss: 0.1122 - acc: 0.9966 - val_loss: 0.3106 - val_acc: 0.9548\nEpoch 395/1000\n - 0s - loss: 0.1140 - acc: 0.9951 - val_loss: 0.3249 - val_acc: 0.9532\nEpoch 396/1000\n - 0s - loss: 0.1153 - acc: 0.9966 - val_loss: 0.3140 - val_acc: 0.9563\nEpoch 397/1000\n - 1s - loss: 0.1264 - acc: 0.9932 - val_loss: 0.3674 - val_acc: 0.9578\nEpoch 398/1000\n - 0s - loss: 0.1248 - acc: 0.9951 - val_loss: 0.3269 - val_acc: 0.9548\nEpoch 399/1000\n - 0s - loss: 0.1053 - acc: 0.9981 - val_loss: 0.3681 - val_acc: 0.9442\nEpoch 400/1000\n - 1s - loss: 0.1095 - acc: 0.9959 - val_loss: 0.3365 - val_acc: 0.9517\nEpoch 401/1000\n - 1s - loss: 0.1254 - acc: 0.9947 - val_loss: 0.3775 - val_acc: 0.9638\nEpoch 402/1000\n - 1s - loss: 0.1191 - acc: 0.9940 - val_loss: 0.3874 - val_acc: 0.9608\nEpoch 403/1000\n - 0s - loss: 0.1117 - acc: 0.9977 - val_loss: 0.3399 - val_acc: 0.9593\nEpoch 404/1000\n - 1s - loss: 0.1270 - acc: 0.9928 - val_loss: 0.3520 - val_acc: 0.9593\nEpoch 405/1000\n - 0s - loss: 0.1122 - acc: 0.9955 - val_loss: 0.3201 - val_acc: 0.9623\nEpoch 406/1000\n - 0s - loss: 0.1170 - acc: 0.9951 - val_loss: 0.3236 - val_acc: 0.9578\nEpoch 407/1000\n - 0s - loss: 0.1241 - acc: 0.9932 - val_loss: 0.3406 - val_acc: 0.9578\nEpoch 408/1000\n - 0s - loss: 0.1229 - acc: 0.9947 - val_loss: 0.3376 - val_acc: 0.9578\nEpoch 409/1000\n - 0s - loss: 0.1154 - acc: 0.9959 - val_loss: 0.3524 - val_acc: 0.9442\nEpoch 410/1000\n - 0s - loss: 0.1233 - acc: 0.9947 - val_loss: 0.3760 - val_acc: 0.9427\nEpoch 411/1000\n - 1s - loss: 0.1278 - acc: 0.9940 - val_loss: 0.3292 - val_acc: 0.9638\nEpoch 412/1000\n - 1s - loss: 0.1254 - acc: 0.9932 - val_loss: 0.3929 - val_acc: 0.9578\nEpoch 413/1000\n - 1s - loss: 0.1265 - acc: 0.9940 - val_loss: 0.3439 - val_acc: 0.9608\nEpoch 414/1000\n - 1s - loss: 0.1166 - acc: 0.9959 - val_loss: 0.3288 - val_acc: 0.9563\nEpoch 415/1000\n - 1s - loss: 0.1379 - acc: 0.9906 - val_loss: 0.3332 - val_acc: 0.9517\nEpoch 416/1000\n - 1s - loss: 0.1323 - acc: 0.9928 - val_loss: 0.3281 - val_acc: 0.9593\nEpoch 417/1000\n - 0s - loss: 0.1210 - acc: 0.9955 - val_loss: 0.3351 - val_acc: 0.9457\nEpoch 418/1000\n - 0s - loss: 0.1329 - acc: 0.9902 - val_loss: 0.3427 - val_acc: 0.9608\nEpoch 419/1000\n - 1s - loss: 0.1117 - acc: 0.9970 - val_loss: 0.3706 - val_acc: 0.9593\nEpoch 420/1000\n - 0s - loss: 0.1118 - acc: 0.9974 - val_loss: 0.3373 - val_acc: 0.9532\nEpoch 421/1000\n - 1s - loss: 0.1092 - acc: 0.9970 - val_loss: 0.3532 - val_acc: 0.9472\nEpoch 422/1000\n - 1s - loss: 0.1133 - acc: 0.9955 - val_loss: 0.3335 - val_acc: 0.9623\nEpoch 423/1000\n - 0s - loss: 0.1229 - acc: 0.9936 - val_loss: 0.3753 - val_acc: 0.9608\nEpoch 424/1000\n - 0s - loss: 0.1095 - acc: 0.9962 - val_loss: 0.3607 - val_acc: 0.9427\nEpoch 425/1000\n - 1s - loss: 0.1228 - acc: 0.9936 - val_loss: 0.3479 - val_acc: 0.9578\nEpoch 426/1000\n - 0s - loss: 0.1132 - acc: 0.9966 - val_loss: 0.3384 - val_acc: 0.9608\nEpoch 427/1000\n - 1s - loss: 0.1127 - acc: 0.9959 - val_loss: 0.3592 - val_acc: 0.9638\nEpoch 428/1000\n - 0s - loss: 0.1233 - acc: 0.9936 - val_loss: 0.3442 - val_acc: 0.9578\nEpoch 429/1000\n - 1s - loss: 0.1132 - acc: 0.9951 - val_loss: 0.3718 - val_acc: 0.9548\nEpoch 430/1000\n - 1s - loss: 0.1228 - acc: 0.9936 - val_loss: 0.3192 - val_acc: 0.9548\nEpoch 431/1000\n - 1s - loss: 0.1173 - acc: 0.9944 - val_loss: 0.3417 - val_acc: 0.9548\nEpoch 432/1000\n - 0s - loss: 0.1205 - acc: 0.9951 - val_loss: 0.3475 - val_acc: 0.9487\nEpoch 433/1000\n - 1s - loss: 0.1190 - acc: 0.9955 - val_loss: 0.3464 - val_acc: 0.9442\nEpoch 434/1000\n - 1s - loss: 0.1187 - acc: 0.9966 - val_loss: 0.3402 - val_acc: 0.9548\nEpoch 435/1000\n - 1s - loss: 0.1221 - acc: 0.9944 - val_loss: 0.3554 - val_acc: 0.9563\nEpoch 436/1000\n - 1s - loss: 0.1126 - acc: 0.9966 - val_loss: 0.3333 - val_acc: 0.9532\nEpoch 437/1000\n - 0s - loss: 0.1136 - acc: 0.9959 - val_loss: 0.3423 - val_acc: 0.9548\nEpoch 438/1000\n - 0s - loss: 0.1075 - acc: 0.9962 - val_loss: 0.3419 - val_acc: 0.9578\nEpoch 439/1000\n - 1s - loss: 0.1170 - acc: 0.9951 - val_loss: 0.3885 - val_acc: 0.9593\nEpoch 440/1000\n - 1s - loss: 0.1274 - acc: 0.9940 - val_loss: 0.3058 - val_acc: 0.9517\nEpoch 441/1000\n - 1s - loss: 0.1062 - acc: 0.9970 - val_loss: 0.3612 - val_acc: 0.9442\nEpoch 442/1000\n - 1s - loss: 0.1190 - acc: 0.9936 - val_loss: 0.4187 - val_acc: 0.9517\nEpoch 443/1000\n - 1s - loss: 0.1317 - acc: 0.9925 - val_loss: 0.3470 - val_acc: 0.9563\nEpoch 444/1000\n - 1s - loss: 0.1106 - acc: 0.9962 - val_loss: 0.4020 - val_acc: 0.9578\nEpoch 445/1000\n - 1s - loss: 0.1294 - acc: 0.9940 - val_loss: 0.3501 - val_acc: 0.9593\nEpoch 446/1000\n - 1s - loss: 0.0977 - acc: 0.9981 - val_loss: 0.3379 - val_acc: 0.9517\nEpoch 447/1000\n - 1s - loss: 0.1079 - acc: 0.9955 - val_loss: 0.3243 - val_acc: 0.9578\nEpoch 448/1000\n - 0s - loss: 0.1258 - acc: 0.9928 - val_loss: 0.3199 - val_acc: 0.9578\nEpoch 449/1000\n - 1s - loss: 0.1154 - acc: 0.9951 - val_loss: 0.3404 - val_acc: 0.9472\nEpoch 450/1000\n - 1s - loss: 0.1131 - acc: 0.9955 - val_loss: 0.3458 - val_acc: 0.9502\nEpoch 451/1000\n - 1s - loss: 0.1321 - acc: 0.9932 - val_loss: 0.3257 - val_acc: 0.9532\nEpoch 452/1000\n - 1s - loss: 0.1195 - acc: 0.9947 - val_loss: 0.3442 - val_acc: 0.9517\nEpoch 453/1000\n - 1s - loss: 0.1057 - acc: 0.9962 - val_loss: 0.3356 - val_acc: 0.9532\nEpoch 454/1000\n - 1s - loss: 0.1205 - acc: 0.9944 - val_loss: 0.3299 - val_acc: 0.9517\nEpoch 455/1000\n - 1s - loss: 0.1034 - acc: 0.9966 - val_loss: 0.3045 - val_acc: 0.9563\nEpoch 456/1000\n - 1s - loss: 0.0945 - acc: 0.9981 - val_loss: 0.3033 - val_acc: 0.9548\nEpoch 457/1000\n - 1s - loss: 0.1069 - acc: 0.9970 - val_loss: 0.4053 - val_acc: 0.9532\nEpoch 458/1000\n - 0s - loss: 0.1057 - acc: 0.9959 - val_loss: 0.3607 - val_acc: 0.9578\nEpoch 459/1000\n - 1s - loss: 0.1105 - acc: 0.9947 - val_loss: 0.4253 - val_acc: 0.9487\nEpoch 460/1000\n - 0s - loss: 0.1114 - acc: 0.9959 - val_loss: 0.3415 - val_acc: 0.9593\nEpoch 461/1000\n - 1s - loss: 0.1180 - acc: 0.9936 - val_loss: 0.3511 - val_acc: 0.9532\nEpoch 462/1000\n - 1s - loss: 0.1030 - acc: 0.9970 - val_loss: 0.3459 - val_acc: 0.9608\nEpoch 463/1000\n - 0s - loss: 0.1116 - acc: 0.9951 - val_loss: 0.3632 - val_acc: 0.9593\nEpoch 464/1000\n - 0s - loss: 0.1284 - acc: 0.9928 - val_loss: 0.3673 - val_acc: 0.9563\nEpoch 465/1000\n - 0s - loss: 0.1344 - acc: 0.9917 - val_loss: 0.3357 - val_acc: 0.9563\nEpoch 466/1000\n - 0s - loss: 0.1085 - acc: 0.9959 - val_loss: 0.3209 - val_acc: 0.9532\nEpoch 467/1000\n - 1s - loss: 0.1091 - acc: 0.9962 - val_loss: 0.3393 - val_acc: 0.9517\nEpoch 468/1000\n - 0s - loss: 0.1195 - acc: 0.9947 - val_loss: 0.3558 - val_acc: 0.9548\nEpoch 469/1000\n - 1s - loss: 0.1136 - acc: 0.9936 - val_loss: 0.3342 - val_acc: 0.9502\nEpoch 470/1000\n - 0s - loss: 0.1082 - acc: 0.9959 - val_loss: 0.3620 - val_acc: 0.9548\nEpoch 471/1000\n - 1s - loss: 0.0972 - acc: 0.9981 - val_loss: 0.3269 - val_acc: 0.9593\nEpoch 472/1000\n - 1s - loss: 0.1107 - acc: 0.9959 - val_loss: 0.3358 - val_acc: 0.9548\nEpoch 473/1000\n - 1s - loss: 0.1114 - acc: 0.9955 - val_loss: 0.3356 - val_acc: 0.9457\nEpoch 474/1000\n","name":"stdout"},{"output_type":"stream","text":" - 1s - loss: 0.1043 - acc: 0.9966 - val_loss: 0.3354 - val_acc: 0.9578\nEpoch 475/1000\n - 1s - loss: 0.1074 - acc: 0.9944 - val_loss: 0.3516 - val_acc: 0.9548\nEpoch 476/1000\n - 1s - loss: 0.0991 - acc: 0.9970 - val_loss: 0.3954 - val_acc: 0.9563\nEpoch 477/1000\n - 1s - loss: 0.1096 - acc: 0.9955 - val_loss: 0.3311 - val_acc: 0.9548\nEpoch 478/1000\n - 1s - loss: 0.1152 - acc: 0.9936 - val_loss: 0.3150 - val_acc: 0.9487\nEpoch 479/1000\n - 1s - loss: 0.1345 - acc: 0.9910 - val_loss: 0.3496 - val_acc: 0.9563\nEpoch 480/1000\n - 1s - loss: 0.1257 - acc: 0.9944 - val_loss: 0.3378 - val_acc: 0.9472\nEpoch 481/1000\n - 1s - loss: 0.1333 - acc: 0.9887 - val_loss: 0.3441 - val_acc: 0.9472\nEpoch 482/1000\n - 0s - loss: 0.1122 - acc: 0.9947 - val_loss: 0.3518 - val_acc: 0.9578\nEpoch 483/1000\n - 1s - loss: 0.1430 - acc: 0.9891 - val_loss: 0.3372 - val_acc: 0.9563\nEpoch 484/1000\n - 1s - loss: 0.1137 - acc: 0.9951 - val_loss: 0.3280 - val_acc: 0.9548\nEpoch 485/1000\n - 1s - loss: 0.1015 - acc: 0.9970 - val_loss: 0.3259 - val_acc: 0.9517\nEpoch 486/1000\n - 1s - loss: 0.0961 - acc: 0.9977 - val_loss: 0.3387 - val_acc: 0.9472\nEpoch 487/1000\n - 1s - loss: 0.1092 - acc: 0.9951 - val_loss: 0.3370 - val_acc: 0.9487\nEpoch 488/1000\n - 1s - loss: 0.1130 - acc: 0.9944 - val_loss: 0.3497 - val_acc: 0.9548\nEpoch 489/1000\n - 1s - loss: 0.0969 - acc: 0.9962 - val_loss: 0.3478 - val_acc: 0.9517\nEpoch 490/1000\n - 1s - loss: 0.1047 - acc: 0.9955 - val_loss: 0.4500 - val_acc: 0.9487\nEpoch 491/1000\n - 1s - loss: 0.1149 - acc: 0.9951 - val_loss: 0.3977 - val_acc: 0.9563\nEpoch 492/1000\n - 1s - loss: 0.1162 - acc: 0.9944 - val_loss: 0.3271 - val_acc: 0.9532\nEpoch 493/1000\n - 1s - loss: 0.1200 - acc: 0.9928 - val_loss: 0.3423 - val_acc: 0.9593\nEpoch 494/1000\n - 0s - loss: 0.0991 - acc: 0.9981 - val_loss: 0.3416 - val_acc: 0.9502\nEpoch 495/1000\n - 0s - loss: 0.1107 - acc: 0.9947 - val_loss: 0.3529 - val_acc: 0.9457\nEpoch 496/1000\n - 0s - loss: 0.1123 - acc: 0.9932 - val_loss: 0.3349 - val_acc: 0.9593\nEpoch 497/1000\n - 1s - loss: 0.1026 - acc: 0.9955 - val_loss: 0.3584 - val_acc: 0.9532\nEpoch 498/1000\n - 1s - loss: 0.1126 - acc: 0.9936 - val_loss: 0.3676 - val_acc: 0.9427\nEpoch 499/1000\n - 1s - loss: 0.1142 - acc: 0.9951 - val_loss: 0.3611 - val_acc: 0.9517\nEpoch 500/1000\n - 0s - loss: 0.1102 - acc: 0.9959 - val_loss: 0.3342 - val_acc: 0.9502\nEpoch 501/1000\n - 1s - loss: 0.1118 - acc: 0.9944 - val_loss: 0.3224 - val_acc: 0.9548\nEpoch 502/1000\n - 1s - loss: 0.1174 - acc: 0.9947 - val_loss: 0.3552 - val_acc: 0.9608\nEpoch 503/1000\n - 1s - loss: 0.1206 - acc: 0.9928 - val_loss: 0.3219 - val_acc: 0.9593\nEpoch 504/1000\n - 0s - loss: 0.1292 - acc: 0.9917 - val_loss: 0.3706 - val_acc: 0.9532\nEpoch 505/1000\n - 0s - loss: 0.1380 - acc: 0.9928 - val_loss: 0.3266 - val_acc: 0.9638\nEpoch 506/1000\n - 1s - loss: 0.1068 - acc: 0.9959 - val_loss: 0.3556 - val_acc: 0.9472\nEpoch 507/1000\n - 1s - loss: 0.1122 - acc: 0.9947 - val_loss: 0.3582 - val_acc: 0.9578\nEpoch 508/1000\n - 0s - loss: 0.1184 - acc: 0.9932 - val_loss: 0.3259 - val_acc: 0.9578\nEpoch 509/1000\n - 0s - loss: 0.1166 - acc: 0.9940 - val_loss: 0.3669 - val_acc: 0.9608\nEpoch 510/1000\n - 0s - loss: 0.1174 - acc: 0.9932 - val_loss: 0.3678 - val_acc: 0.9457\nEpoch 511/1000\n - 1s - loss: 0.1185 - acc: 0.9944 - val_loss: 0.3160 - val_acc: 0.9578\nEpoch 512/1000\n - 0s - loss: 0.1082 - acc: 0.9955 - val_loss: 0.3641 - val_acc: 0.9563\nEpoch 513/1000\n - 1s - loss: 0.0957 - acc: 0.9977 - val_loss: 0.3440 - val_acc: 0.9517\nEpoch 514/1000\n - 0s - loss: 0.1159 - acc: 0.9925 - val_loss: 0.4238 - val_acc: 0.9397\nEpoch 515/1000\n - 0s - loss: 0.1100 - acc: 0.9951 - val_loss: 0.3555 - val_acc: 0.9548\nEpoch 516/1000\n - 1s - loss: 0.1099 - acc: 0.9955 - val_loss: 0.3271 - val_acc: 0.9532\nEpoch 517/1000\n - 1s - loss: 0.1017 - acc: 0.9955 - val_loss: 0.3337 - val_acc: 0.9532\nEpoch 518/1000\n - 0s - loss: 0.1159 - acc: 0.9940 - val_loss: 0.3635 - val_acc: 0.9548\nEpoch 519/1000\n - 1s - loss: 0.1094 - acc: 0.9925 - val_loss: 0.3934 - val_acc: 0.9457\nEpoch 520/1000\n - 0s - loss: 0.1227 - acc: 0.9932 - val_loss: 0.3770 - val_acc: 0.9578\nEpoch 521/1000\n - 0s - loss: 0.1220 - acc: 0.9936 - val_loss: 0.4540 - val_acc: 0.9548\nEpoch 522/1000\n - 1s - loss: 0.1235 - acc: 0.9913 - val_loss: 0.3454 - val_acc: 0.9563\nEpoch 523/1000\n - 0s - loss: 0.1324 - acc: 0.9925 - val_loss: 0.3487 - val_acc: 0.9472\nEpoch 524/1000\n - 0s - loss: 0.1170 - acc: 0.9947 - val_loss: 0.3152 - val_acc: 0.9548\nEpoch 525/1000\n - 1s - loss: 0.1251 - acc: 0.9925 - val_loss: 0.3350 - val_acc: 0.9517\nEpoch 526/1000\n - 1s - loss: 0.1348 - acc: 0.9910 - val_loss: 0.3579 - val_acc: 0.9487\nEpoch 527/1000\n - 0s - loss: 0.1206 - acc: 0.9932 - val_loss: 0.3551 - val_acc: 0.9502\nEpoch 528/1000\n - 0s - loss: 0.1142 - acc: 0.9947 - val_loss: 0.3800 - val_acc: 0.9563\nEpoch 529/1000\n - 1s - loss: 0.1077 - acc: 0.9955 - val_loss: 0.3924 - val_acc: 0.9532\nEpoch 530/1000\n - 0s - loss: 0.1106 - acc: 0.9944 - val_loss: 0.3285 - val_acc: 0.9593\nEpoch 531/1000\n - 1s - loss: 0.1099 - acc: 0.9951 - val_loss: 0.3523 - val_acc: 0.9502\nEpoch 532/1000\n - 0s - loss: 0.1216 - acc: 0.9913 - val_loss: 0.3572 - val_acc: 0.9487\nEpoch 533/1000\n - 1s - loss: 0.0970 - acc: 0.9981 - val_loss: 0.3297 - val_acc: 0.9593\nEpoch 534/1000\n - 0s - loss: 0.1061 - acc: 0.9966 - val_loss: 0.3512 - val_acc: 0.9487\nEpoch 535/1000\n - 1s - loss: 0.1189 - acc: 0.9925 - val_loss: 0.3545 - val_acc: 0.9517\nEpoch 536/1000\n - 1s - loss: 0.1016 - acc: 0.9951 - val_loss: 0.3405 - val_acc: 0.9548\nEpoch 537/1000\n - 1s - loss: 0.1026 - acc: 0.9955 - val_loss: 0.3380 - val_acc: 0.9563\nEpoch 538/1000\n - 0s - loss: 0.1044 - acc: 0.9970 - val_loss: 0.3484 - val_acc: 0.9548\nEpoch 539/1000\n - 1s - loss: 0.1003 - acc: 0.9974 - val_loss: 0.3456 - val_acc: 0.9548\nEpoch 540/1000\n - 0s - loss: 0.1133 - acc: 0.9928 - val_loss: 0.3695 - val_acc: 0.9563\nEpoch 541/1000\n - 1s - loss: 0.1331 - acc: 0.9921 - val_loss: 0.3496 - val_acc: 0.9608\nEpoch 542/1000\n - 0s - loss: 0.1109 - acc: 0.9947 - val_loss: 0.3514 - val_acc: 0.9593\nEpoch 543/1000\n - 1s - loss: 0.1117 - acc: 0.9940 - val_loss: 0.3665 - val_acc: 0.9487\nEpoch 544/1000\n - 0s - loss: 0.1023 - acc: 0.9966 - val_loss: 0.3627 - val_acc: 0.9563\nEpoch 545/1000\n - 1s - loss: 0.1152 - acc: 0.9928 - val_loss: 0.3511 - val_acc: 0.9532\nEpoch 546/1000\n - 0s - loss: 0.0919 - acc: 0.9970 - val_loss: 0.3674 - val_acc: 0.9502\nEpoch 547/1000\n - 0s - loss: 0.1253 - acc: 0.9902 - val_loss: 0.3972 - val_acc: 0.9608\nEpoch 548/1000\n - 1s - loss: 0.1078 - acc: 0.9947 - val_loss: 0.3382 - val_acc: 0.9502\nEpoch 549/1000\n - 1s - loss: 0.1120 - acc: 0.9940 - val_loss: 0.3660 - val_acc: 0.9578\nEpoch 550/1000\n - 1s - loss: 0.1022 - acc: 0.9966 - val_loss: 0.3808 - val_acc: 0.9472\nEpoch 551/1000\n - 1s - loss: 0.1169 - acc: 0.9936 - val_loss: 0.3791 - val_acc: 0.9502\nEpoch 552/1000\n - 0s - loss: 0.1032 - acc: 0.9966 - val_loss: 0.3338 - val_acc: 0.9563\nEpoch 553/1000\n - 1s - loss: 0.1145 - acc: 0.9936 - val_loss: 0.3479 - val_acc: 0.9532\nEpoch 554/1000\n - 1s - loss: 0.1077 - acc: 0.9947 - val_loss: 0.3805 - val_acc: 0.9351\nEpoch 555/1000\n - 1s - loss: 0.1233 - acc: 0.9910 - val_loss: 0.3808 - val_acc: 0.9412\nEpoch 556/1000\n - 0s - loss: 0.1034 - acc: 0.9955 - val_loss: 0.3872 - val_acc: 0.9502\nEpoch 557/1000\n - 1s - loss: 0.1124 - acc: 0.9936 - val_loss: 0.3945 - val_acc: 0.9593\nEpoch 558/1000\n - 0s - loss: 0.1006 - acc: 0.9977 - val_loss: 0.3632 - val_acc: 0.9442\nEpoch 559/1000\n - 1s - loss: 0.1093 - acc: 0.9936 - val_loss: 0.3627 - val_acc: 0.9578\nEpoch 560/1000\n - 1s - loss: 0.1089 - acc: 0.9951 - val_loss: 0.3591 - val_acc: 0.9517\nEpoch 561/1000\n - 0s - loss: 0.1031 - acc: 0.9951 - val_loss: 0.3601 - val_acc: 0.9608\nEpoch 562/1000\n - 1s - loss: 0.0994 - acc: 0.9962 - val_loss: 0.3393 - val_acc: 0.9487\nEpoch 563/1000\n - 0s - loss: 0.1111 - acc: 0.9947 - val_loss: 0.3507 - val_acc: 0.9608\nEpoch 564/1000\n - 1s - loss: 0.0937 - acc: 0.9962 - val_loss: 0.3517 - val_acc: 0.9502\nEpoch 565/1000\n - 1s - loss: 0.1142 - acc: 0.9925 - val_loss: 0.3764 - val_acc: 0.9593\nEpoch 566/1000\n - 1s - loss: 0.1065 - acc: 0.9940 - val_loss: 0.3825 - val_acc: 0.9457\nEpoch 567/1000\n - 1s - loss: 0.1114 - acc: 0.9940 - val_loss: 0.3702 - val_acc: 0.9563\nEpoch 568/1000\n - 1s - loss: 0.1113 - acc: 0.9936 - val_loss: 0.3538 - val_acc: 0.9548\n","name":"stdout"},{"output_type":"stream","text":"Epoch 569/1000\n - 1s - loss: 0.0826 - acc: 0.9981 - val_loss: 0.3649 - val_acc: 0.9442\nEpoch 570/1000\n - 1s - loss: 0.0876 - acc: 0.9974 - val_loss: 0.3649 - val_acc: 0.9532\nEpoch 571/1000\n - 1s - loss: 0.0938 - acc: 0.9959 - val_loss: 0.3429 - val_acc: 0.9487\nEpoch 572/1000\n - 1s - loss: 0.1324 - acc: 0.9902 - val_loss: 0.3491 - val_acc: 0.9563\nEpoch 573/1000\n - 1s - loss: 0.1055 - acc: 0.9940 - val_loss: 0.3658 - val_acc: 0.9472\nEpoch 574/1000\n - 1s - loss: 0.0868 - acc: 0.9966 - val_loss: 0.3540 - val_acc: 0.9487\nEpoch 575/1000\n - 1s - loss: 0.0992 - acc: 0.9970 - val_loss: 0.3598 - val_acc: 0.9532\nEpoch 576/1000\n - 1s - loss: 0.0906 - acc: 0.9966 - val_loss: 0.3451 - val_acc: 0.9548\nEpoch 577/1000\n - 0s - loss: 0.0916 - acc: 0.9966 - val_loss: 0.3658 - val_acc: 0.9502\nEpoch 578/1000\n - 0s - loss: 0.1042 - acc: 0.9962 - val_loss: 0.3307 - val_acc: 0.9578\nEpoch 579/1000\n - 0s - loss: 0.0898 - acc: 0.9962 - val_loss: 0.3489 - val_acc: 0.9548\nEpoch 580/1000\n - 1s - loss: 0.0973 - acc: 0.9966 - val_loss: 0.3406 - val_acc: 0.9517\nEpoch 581/1000\n - 0s - loss: 0.1240 - acc: 0.9917 - val_loss: 0.3303 - val_acc: 0.9578\nEpoch 582/1000\n - 1s - loss: 0.0982 - acc: 0.9955 - val_loss: 0.3580 - val_acc: 0.9532\nEpoch 583/1000\n - 0s - loss: 0.1112 - acc: 0.9928 - val_loss: 0.3267 - val_acc: 0.9563\nEpoch 584/1000\n - 0s - loss: 0.1095 - acc: 0.9925 - val_loss: 0.3824 - val_acc: 0.9517\nEpoch 585/1000\n - 1s - loss: 0.1106 - acc: 0.9947 - val_loss: 0.3271 - val_acc: 0.9532\nEpoch 586/1000\n - 1s - loss: 0.0935 - acc: 0.9977 - val_loss: 0.3431 - val_acc: 0.9487\nEpoch 587/1000\n - 0s - loss: 0.0935 - acc: 0.9966 - val_loss: 0.3660 - val_acc: 0.9563\nEpoch 588/1000\n - 1s - loss: 0.1149 - acc: 0.9936 - val_loss: 0.3696 - val_acc: 0.9593\nEpoch 589/1000\n - 1s - loss: 0.1118 - acc: 0.9925 - val_loss: 0.3520 - val_acc: 0.9532\nEpoch 590/1000\n - 1s - loss: 0.1299 - acc: 0.9913 - val_loss: 0.3446 - val_acc: 0.9532\nEpoch 591/1000\n - 0s - loss: 0.1025 - acc: 0.9936 - val_loss: 0.3907 - val_acc: 0.9517\nEpoch 592/1000\n - 0s - loss: 0.1157 - acc: 0.9951 - val_loss: 0.3481 - val_acc: 0.9563\nEpoch 593/1000\n - 1s - loss: 0.0947 - acc: 0.9966 - val_loss: 0.3490 - val_acc: 0.9502\nEpoch 594/1000\n - 1s - loss: 0.1242 - acc: 0.9913 - val_loss: 0.3563 - val_acc: 0.9487\nEpoch 595/1000\n - 1s - loss: 0.1014 - acc: 0.9962 - val_loss: 0.3562 - val_acc: 0.9517\nEpoch 596/1000\n - 1s - loss: 0.0957 - acc: 0.9962 - val_loss: 0.3867 - val_acc: 0.9487\nEpoch 597/1000\n - 0s - loss: 0.1243 - acc: 0.9925 - val_loss: 0.3838 - val_acc: 0.9548\nEpoch 598/1000\n - 1s - loss: 0.1066 - acc: 0.9947 - val_loss: 0.3493 - val_acc: 0.9548\nEpoch 599/1000\n - 0s - loss: 0.0983 - acc: 0.9955 - val_loss: 0.3430 - val_acc: 0.9548\nEpoch 600/1000\n - 1s - loss: 0.1053 - acc: 0.9940 - val_loss: 0.3505 - val_acc: 0.9578\nEpoch 601/1000\n - 0s - loss: 0.1271 - acc: 0.9913 - val_loss: 0.3469 - val_acc: 0.9502\nEpoch 602/1000\n - 1s - loss: 0.1024 - acc: 0.9951 - val_loss: 0.3699 - val_acc: 0.9578\nEpoch 603/1000\n - 0s - loss: 0.1019 - acc: 0.9959 - val_loss: 0.3737 - val_acc: 0.9548\nEpoch 604/1000\n - 1s - loss: 0.1091 - acc: 0.9955 - val_loss: 0.3592 - val_acc: 0.9517\nEpoch 605/1000\n - 1s - loss: 0.0969 - acc: 0.9959 - val_loss: 0.3483 - val_acc: 0.9502\nEpoch 606/1000\n - 1s - loss: 0.0937 - acc: 0.9962 - val_loss: 0.3646 - val_acc: 0.9487\nEpoch 607/1000\n - 0s - loss: 0.1080 - acc: 0.9936 - val_loss: 0.3554 - val_acc: 0.9502\nEpoch 608/1000\n - 0s - loss: 0.1031 - acc: 0.9955 - val_loss: 0.3838 - val_acc: 0.9517\nEpoch 609/1000\n - 1s - loss: 0.1093 - acc: 0.9944 - val_loss: 0.3624 - val_acc: 0.9532\nEpoch 610/1000\n - 1s - loss: 0.1061 - acc: 0.9936 - val_loss: 0.3557 - val_acc: 0.9517\nEpoch 611/1000\n - 0s - loss: 0.1115 - acc: 0.9940 - val_loss: 0.3640 - val_acc: 0.9502\nEpoch 612/1000\n - 1s - loss: 0.1042 - acc: 0.9951 - val_loss: 0.3575 - val_acc: 0.9487\nEpoch 613/1000\n - 1s - loss: 0.1117 - acc: 0.9947 - val_loss: 0.3633 - val_acc: 0.9502\nEpoch 614/1000\n - 1s - loss: 0.0970 - acc: 0.9959 - val_loss: 0.3511 - val_acc: 0.9487\nEpoch 615/1000\n - 1s - loss: 0.0990 - acc: 0.9951 - val_loss: 0.3347 - val_acc: 0.9517\nEpoch 616/1000\n - 1s - loss: 0.0949 - acc: 0.9966 - val_loss: 0.3618 - val_acc: 0.9502\nEpoch 617/1000\n - 0s - loss: 0.0981 - acc: 0.9959 - val_loss: 0.3557 - val_acc: 0.9502\nEpoch 618/1000\n - 1s - loss: 0.1074 - acc: 0.9932 - val_loss: 0.3706 - val_acc: 0.9532\nEpoch 619/1000\n - 1s - loss: 0.1261 - acc: 0.9917 - val_loss: 0.3611 - val_acc: 0.9502\nEpoch 620/1000\n - 1s - loss: 0.0991 - acc: 0.9951 - val_loss: 0.4170 - val_acc: 0.9412\nEpoch 621/1000\n - 1s - loss: 0.1002 - acc: 0.9951 - val_loss: 0.3414 - val_acc: 0.9548\nEpoch 622/1000\n - 1s - loss: 0.0996 - acc: 0.9955 - val_loss: 0.3842 - val_acc: 0.9563\nEpoch 623/1000\n - 0s - loss: 0.1044 - acc: 0.9955 - val_loss: 0.3511 - val_acc: 0.9472\nEpoch 624/1000\n - 1s - loss: 0.1025 - acc: 0.9959 - val_loss: 0.3597 - val_acc: 0.9548\nEpoch 625/1000\n - 0s - loss: 0.1131 - acc: 0.9928 - val_loss: 0.3917 - val_acc: 0.9532\nEpoch 626/1000\n - 1s - loss: 0.0995 - acc: 0.9959 - val_loss: 0.3654 - val_acc: 0.9517\nEpoch 627/1000\n - 1s - loss: 0.1072 - acc: 0.9940 - val_loss: 0.3830 - val_acc: 0.9457\nEpoch 628/1000\n - 1s - loss: 0.1320 - acc: 0.9917 - val_loss: 0.3441 - val_acc: 0.9487\nEpoch 629/1000\n - 1s - loss: 0.1033 - acc: 0.9955 - val_loss: 0.3878 - val_acc: 0.9472\nEpoch 630/1000\n - 1s - loss: 0.0950 - acc: 0.9959 - val_loss: 0.3739 - val_acc: 0.9487\nEpoch 631/1000\n - 1s - loss: 0.1018 - acc: 0.9936 - val_loss: 0.3811 - val_acc: 0.9502\nEpoch 632/1000\n - 1s - loss: 0.0896 - acc: 0.9959 - val_loss: 0.3809 - val_acc: 0.9487\nEpoch 633/1000\n - 1s - loss: 0.1048 - acc: 0.9955 - val_loss: 0.3997 - val_acc: 0.9502\nEpoch 634/1000\n - 1s - loss: 0.1075 - acc: 0.9944 - val_loss: 0.4626 - val_acc: 0.9382\nEpoch 635/1000\n - 1s - loss: 0.1027 - acc: 0.9947 - val_loss: 0.4555 - val_acc: 0.9367\nEpoch 636/1000\n - 1s - loss: 0.1032 - acc: 0.9947 - val_loss: 0.3774 - val_acc: 0.9502\nEpoch 637/1000\n - 1s - loss: 0.0926 - acc: 0.9962 - val_loss: 0.3459 - val_acc: 0.9563\nEpoch 638/1000\n - 1s - loss: 0.1024 - acc: 0.9951 - val_loss: 0.3505 - val_acc: 0.9487\nEpoch 639/1000\n - 1s - loss: 0.1092 - acc: 0.9947 - val_loss: 0.3540 - val_acc: 0.9532\nEpoch 640/1000\n - 1s - loss: 0.0977 - acc: 0.9951 - val_loss: 0.3548 - val_acc: 0.9517\nEpoch 641/1000\n - 0s - loss: 0.1019 - acc: 0.9932 - val_loss: 0.3744 - val_acc: 0.9502\nEpoch 642/1000\n - 1s - loss: 0.1029 - acc: 0.9951 - val_loss: 0.3639 - val_acc: 0.9487\nEpoch 643/1000\n - 1s - loss: 0.1021 - acc: 0.9947 - val_loss: 0.3842 - val_acc: 0.9487\nEpoch 644/1000\n - 0s - loss: 0.1007 - acc: 0.9951 - val_loss: 0.3532 - val_acc: 0.9548\nEpoch 645/1000\n - 1s - loss: 0.0959 - acc: 0.9962 - val_loss: 0.3744 - val_acc: 0.9487\nEpoch 646/1000\n - 1s - loss: 0.1030 - acc: 0.9947 - val_loss: 0.3903 - val_acc: 0.9472\nEpoch 647/1000\n - 0s - loss: 0.1089 - acc: 0.9932 - val_loss: 0.3882 - val_acc: 0.9532\nEpoch 648/1000\n - 1s - loss: 0.1030 - acc: 0.9940 - val_loss: 0.3482 - val_acc: 0.9532\nEpoch 649/1000\n - 1s - loss: 0.0912 - acc: 0.9962 - val_loss: 0.3580 - val_acc: 0.9472\nEpoch 650/1000\n - 1s - loss: 0.0923 - acc: 0.9974 - val_loss: 0.4414 - val_acc: 0.9517\nEpoch 651/1000\n - 1s - loss: 0.1055 - acc: 0.9947 - val_loss: 0.3793 - val_acc: 0.9563\nEpoch 652/1000\n - 1s - loss: 0.1058 - acc: 0.9944 - val_loss: 0.3699 - val_acc: 0.9472\nEpoch 653/1000\n - 1s - loss: 0.0905 - acc: 0.9962 - val_loss: 0.3833 - val_acc: 0.9517\nEpoch 654/1000\n - 0s - loss: 0.1004 - acc: 0.9947 - val_loss: 0.3996 - val_acc: 0.9563\nEpoch 655/1000\n - 1s - loss: 0.1053 - acc: 0.9944 - val_loss: 0.4235 - val_acc: 0.9532\nEpoch 656/1000\n - 1s - loss: 0.1001 - acc: 0.9959 - val_loss: 0.3655 - val_acc: 0.9472\nEpoch 657/1000\n - 0s - loss: 0.0917 - acc: 0.9962 - val_loss: 0.3628 - val_acc: 0.9487\nEpoch 658/1000\n - 1s - loss: 0.1089 - acc: 0.9936 - val_loss: 0.3751 - val_acc: 0.9427\nEpoch 659/1000\n - 1s - loss: 0.0963 - acc: 0.9970 - val_loss: 0.3573 - val_acc: 0.9532\nEpoch 660/1000\n - 1s - loss: 0.0896 - acc: 0.9977 - val_loss: 0.3830 - val_acc: 0.9502\nEpoch 661/1000\n - 1s - loss: 0.0846 - acc: 0.9977 - val_loss: 0.3679 - val_acc: 0.9532\nEpoch 662/1000\n - 1s - loss: 0.1243 - acc: 0.9917 - val_loss: 0.3662 - val_acc: 0.9487\nEpoch 663/1000\n","name":"stdout"},{"output_type":"stream","text":" - 1s - loss: 0.1174 - acc: 0.9928 - val_loss: 0.3682 - val_acc: 0.9457\nEpoch 664/1000\n - 0s - loss: 0.0816 - acc: 0.9966 - val_loss: 0.4001 - val_acc: 0.9397\nEpoch 665/1000\n - 0s - loss: 0.1054 - acc: 0.9936 - val_loss: 0.3714 - val_acc: 0.9457\nEpoch 666/1000\n - 0s - loss: 0.0912 - acc: 0.9951 - val_loss: 0.3763 - val_acc: 0.9472\nEpoch 667/1000\n - 0s - loss: 0.0871 - acc: 0.9962 - val_loss: 0.3446 - val_acc: 0.9502\nEpoch 668/1000\n - 0s - loss: 0.0929 - acc: 0.9951 - val_loss: 0.3636 - val_acc: 0.9472\nEpoch 669/1000\n - 0s - loss: 0.0882 - acc: 0.9970 - val_loss: 0.3490 - val_acc: 0.9532\nEpoch 670/1000\n - 1s - loss: 0.0960 - acc: 0.9947 - val_loss: 0.3876 - val_acc: 0.9382\nEpoch 671/1000\n - 0s - loss: 0.1003 - acc: 0.9951 - val_loss: 0.4300 - val_acc: 0.9563\nEpoch 672/1000\n - 1s - loss: 0.0975 - acc: 0.9959 - val_loss: 0.3639 - val_acc: 0.9563\nEpoch 673/1000\n - 0s - loss: 0.0987 - acc: 0.9955 - val_loss: 0.3968 - val_acc: 0.9517\nEpoch 674/1000\n - 1s - loss: 0.0916 - acc: 0.9966 - val_loss: 0.3543 - val_acc: 0.9548\nEpoch 675/1000\n - 1s - loss: 0.0919 - acc: 0.9970 - val_loss: 0.3471 - val_acc: 0.9532\nEpoch 676/1000\n - 0s - loss: 0.0947 - acc: 0.9962 - val_loss: 0.3646 - val_acc: 0.9532\nEpoch 677/1000\n - 1s - loss: 0.1055 - acc: 0.9936 - val_loss: 0.3815 - val_acc: 0.9548\nEpoch 678/1000\n - 0s - loss: 0.1106 - acc: 0.9925 - val_loss: 0.3560 - val_acc: 0.9517\nEpoch 679/1000\n - 0s - loss: 0.1153 - acc: 0.9917 - val_loss: 0.3566 - val_acc: 0.9487\nEpoch 680/1000\n - 1s - loss: 0.1052 - acc: 0.9928 - val_loss: 0.4069 - val_acc: 0.9397\nEpoch 681/1000\n - 1s - loss: 0.0932 - acc: 0.9966 - val_loss: 0.3613 - val_acc: 0.9442\nEpoch 682/1000\n - 0s - loss: 0.0853 - acc: 0.9977 - val_loss: 0.3686 - val_acc: 0.9487\nEpoch 683/1000\n - 0s - loss: 0.1105 - acc: 0.9917 - val_loss: 0.4175 - val_acc: 0.9457\nEpoch 684/1000\n - 0s - loss: 0.0952 - acc: 0.9966 - val_loss: 0.3658 - val_acc: 0.9487\nEpoch 685/1000\n - 1s - loss: 0.0995 - acc: 0.9944 - val_loss: 0.3589 - val_acc: 0.9502\nEpoch 686/1000\n - 0s - loss: 0.1033 - acc: 0.9944 - val_loss: 0.3589 - val_acc: 0.9532\nEpoch 687/1000\n - 1s - loss: 0.0940 - acc: 0.9944 - val_loss: 0.3679 - val_acc: 0.9502\nEpoch 688/1000\n - 1s - loss: 0.0965 - acc: 0.9951 - val_loss: 0.3733 - val_acc: 0.9487\nEpoch 689/1000\n - 1s - loss: 0.1107 - acc: 0.9940 - val_loss: 0.3802 - val_acc: 0.9578\nEpoch 690/1000\n - 0s - loss: 0.1190 - acc: 0.9936 - val_loss: 0.3834 - val_acc: 0.9487\nEpoch 691/1000\n - 0s - loss: 0.0957 - acc: 0.9970 - val_loss: 0.3666 - val_acc: 0.9502\nEpoch 692/1000\n - 1s - loss: 0.0939 - acc: 0.9966 - val_loss: 0.3733 - val_acc: 0.9457\nEpoch 693/1000\n - 1s - loss: 0.0939 - acc: 0.9962 - val_loss: 0.3637 - val_acc: 0.9502\nEpoch 694/1000\n - 1s - loss: 0.0893 - acc: 0.9966 - val_loss: 0.3804 - val_acc: 0.9487\nEpoch 695/1000\n - 1s - loss: 0.1103 - acc: 0.9921 - val_loss: 0.3979 - val_acc: 0.9578\nEpoch 696/1000\n - 0s - loss: 0.1078 - acc: 0.9940 - val_loss: 0.3773 - val_acc: 0.9532\nEpoch 697/1000\n - 1s - loss: 0.1126 - acc: 0.9917 - val_loss: 0.3742 - val_acc: 0.9502\nEpoch 698/1000\n - 1s - loss: 0.0971 - acc: 0.9940 - val_loss: 0.3892 - val_acc: 0.9472\nEpoch 699/1000\n - 0s - loss: 0.1000 - acc: 0.9955 - val_loss: 0.3801 - val_acc: 0.9517\nEpoch 700/1000\n - 0s - loss: 0.0961 - acc: 0.9951 - val_loss: 0.3881 - val_acc: 0.9457\nEpoch 701/1000\n - 0s - loss: 0.1099 - acc: 0.9928 - val_loss: 0.3810 - val_acc: 0.9472\nEpoch 702/1000\n - 0s - loss: 0.0848 - acc: 0.9977 - val_loss: 0.3820 - val_acc: 0.9532\nEpoch 703/1000\n - 1s - loss: 0.1083 - acc: 0.9936 - val_loss: 0.3628 - val_acc: 0.9502\nEpoch 704/1000\n - 0s - loss: 0.0924 - acc: 0.9947 - val_loss: 0.4104 - val_acc: 0.9578\nEpoch 705/1000\n - 1s - loss: 0.1034 - acc: 0.9944 - val_loss: 0.3718 - val_acc: 0.9487\nEpoch 706/1000\n - 0s - loss: 0.0942 - acc: 0.9951 - val_loss: 0.4122 - val_acc: 0.9412\nEpoch 707/1000\n - 1s - loss: 0.0941 - acc: 0.9947 - val_loss: 0.3696 - val_acc: 0.9502\nEpoch 708/1000\n - 1s - loss: 0.0822 - acc: 0.9985 - val_loss: 0.3739 - val_acc: 0.9548\nEpoch 709/1000\n - 1s - loss: 0.0814 - acc: 0.9970 - val_loss: 0.4171 - val_acc: 0.9517\nEpoch 710/1000\n - 1s - loss: 0.0943 - acc: 0.9955 - val_loss: 0.4131 - val_acc: 0.9457\nEpoch 711/1000\n - 0s - loss: 0.0976 - acc: 0.9947 - val_loss: 0.4115 - val_acc: 0.9517\nEpoch 712/1000\n - 1s - loss: 0.0879 - acc: 0.9974 - val_loss: 0.3560 - val_acc: 0.9548\nEpoch 713/1000\n - 0s - loss: 0.0945 - acc: 0.9944 - val_loss: 0.4026 - val_acc: 0.9487\nEpoch 714/1000\n - 0s - loss: 0.1006 - acc: 0.9951 - val_loss: 0.3749 - val_acc: 0.9563\nEpoch 715/1000\n - 1s - loss: 0.0947 - acc: 0.9962 - val_loss: 0.4087 - val_acc: 0.9412\nEpoch 716/1000\n - 0s - loss: 0.1039 - acc: 0.9940 - val_loss: 0.3804 - val_acc: 0.9532\nEpoch 717/1000\n - 1s - loss: 0.0822 - acc: 0.9977 - val_loss: 0.3696 - val_acc: 0.9563\nEpoch 718/1000\n - 0s - loss: 0.1027 - acc: 0.9944 - val_loss: 0.3630 - val_acc: 0.9578\nEpoch 719/1000\n - 1s - loss: 0.0808 - acc: 0.9970 - val_loss: 0.3909 - val_acc: 0.9517\nEpoch 720/1000\n - 0s - loss: 0.1005 - acc: 0.9940 - val_loss: 0.4058 - val_acc: 0.9397\nEpoch 721/1000\n - 0s - loss: 0.1003 - acc: 0.9944 - val_loss: 0.3687 - val_acc: 0.9593\nEpoch 722/1000\n - 0s - loss: 0.0863 - acc: 0.9966 - val_loss: 0.4224 - val_acc: 0.9532\nEpoch 723/1000\n - 1s - loss: 0.0916 - acc: 0.9966 - val_loss: 0.3784 - val_acc: 0.9442\nEpoch 724/1000\n - 0s - loss: 0.1000 - acc: 0.9947 - val_loss: 0.3575 - val_acc: 0.9532\nEpoch 725/1000\n - 1s - loss: 0.0825 - acc: 0.9981 - val_loss: 0.3616 - val_acc: 0.9563\nEpoch 726/1000\n - 0s - loss: 0.0997 - acc: 0.9962 - val_loss: 0.3922 - val_acc: 0.9593\nEpoch 727/1000\n - 0s - loss: 0.0958 - acc: 0.9959 - val_loss: 0.3620 - val_acc: 0.9517\nEpoch 728/1000\n - 0s - loss: 0.0997 - acc: 0.9928 - val_loss: 0.4286 - val_acc: 0.9367\nEpoch 729/1000\n - 1s - loss: 0.0913 - acc: 0.9951 - val_loss: 0.3720 - val_acc: 0.9502\nEpoch 730/1000\n - 1s - loss: 0.0817 - acc: 0.9985 - val_loss: 0.3582 - val_acc: 0.9532\nEpoch 731/1000\n - 1s - loss: 0.0937 - acc: 0.9951 - val_loss: 0.3900 - val_acc: 0.9487\nEpoch 732/1000\n - 0s - loss: 0.0841 - acc: 0.9966 - val_loss: 0.3888 - val_acc: 0.9472\nEpoch 733/1000\n - 1s - loss: 0.0753 - acc: 0.9989 - val_loss: 0.4149 - val_acc: 0.9517\nEpoch 734/1000\n - 0s - loss: 0.0904 - acc: 0.9940 - val_loss: 0.3942 - val_acc: 0.9548\nEpoch 735/1000\n - 1s - loss: 0.1184 - acc: 0.9913 - val_loss: 0.3863 - val_acc: 0.9487\nEpoch 736/1000\n - 0s - loss: 0.0962 - acc: 0.9951 - val_loss: 0.3842 - val_acc: 0.9517\nEpoch 737/1000\n - 1s - loss: 0.0856 - acc: 0.9970 - val_loss: 0.3767 - val_acc: 0.9563\nEpoch 738/1000\n - 0s - loss: 0.0787 - acc: 0.9974 - val_loss: 0.3715 - val_acc: 0.9472\nEpoch 739/1000\n - 1s - loss: 0.0863 - acc: 0.9959 - val_loss: 0.3606 - val_acc: 0.9517\nEpoch 740/1000\n - 0s - loss: 0.0967 - acc: 0.9947 - val_loss: 0.3851 - val_acc: 0.9487\nEpoch 741/1000\n - 1s - loss: 0.0994 - acc: 0.9947 - val_loss: 0.3735 - val_acc: 0.9517\nEpoch 742/1000\n - 0s - loss: 0.1192 - acc: 0.9917 - val_loss: 0.3562 - val_acc: 0.9517\nEpoch 743/1000\n - 0s - loss: 0.1055 - acc: 0.9928 - val_loss: 0.3673 - val_acc: 0.9517\nEpoch 744/1000\n - 0s - loss: 0.0994 - acc: 0.9951 - val_loss: 0.4172 - val_acc: 0.9578\nEpoch 745/1000\n - 1s - loss: 0.0886 - acc: 0.9974 - val_loss: 0.3580 - val_acc: 0.9532\nEpoch 746/1000\n - 0s - loss: 0.0866 - acc: 0.9962 - val_loss: 0.3775 - val_acc: 0.9548\nEpoch 747/1000\n - 1s - loss: 0.0966 - acc: 0.9944 - val_loss: 0.3985 - val_acc: 0.9472\nEpoch 748/1000\n - 1s - loss: 0.0953 - acc: 0.9951 - val_loss: 0.4661 - val_acc: 0.9351\nEpoch 749/1000\n - 1s - loss: 0.0969 - acc: 0.9947 - val_loss: 0.3951 - val_acc: 0.9502\nEpoch 750/1000\n - 1s - loss: 0.0849 - acc: 0.9959 - val_loss: 0.3880 - val_acc: 0.9502\nEpoch 751/1000\n - 0s - loss: 0.0882 - acc: 0.9962 - val_loss: 0.3924 - val_acc: 0.9517\nEpoch 752/1000\n - 1s - loss: 0.0942 - acc: 0.9951 - val_loss: 0.3840 - val_acc: 0.9517\nEpoch 753/1000\n - 1s - loss: 0.0850 - acc: 0.9959 - val_loss: 0.4033 - val_acc: 0.9517\nEpoch 754/1000\n - 1s - loss: 0.1041 - acc: 0.9936 - val_loss: 0.3820 - val_acc: 0.9502\nEpoch 755/1000\n - 1s - loss: 0.0941 - acc: 0.9940 - val_loss: 0.4062 - val_acc: 0.9457\nEpoch 756/1000\n - 0s - loss: 0.0945 - acc: 0.9932 - val_loss: 0.3921 - val_acc: 0.9532\nEpoch 757/1000\n - 1s - loss: 0.0990 - acc: 0.9955 - val_loss: 0.3998 - val_acc: 0.9517\n","name":"stdout"},{"output_type":"stream","text":"Epoch 758/1000\n - 0s - loss: 0.0755 - acc: 0.9981 - val_loss: 0.3836 - val_acc: 0.9487\nEpoch 759/1000\n - 0s - loss: 0.0775 - acc: 0.9981 - val_loss: 0.3778 - val_acc: 0.9442\nEpoch 760/1000\n - 0s - loss: 0.0968 - acc: 0.9955 - val_loss: 0.3833 - val_acc: 0.9487\nEpoch 761/1000\n - 1s - loss: 0.1047 - acc: 0.9932 - val_loss: 0.3948 - val_acc: 0.9487\nEpoch 762/1000\n - 0s - loss: 0.1067 - acc: 0.9947 - val_loss: 0.3900 - val_acc: 0.9457\nEpoch 763/1000\n - 1s - loss: 0.1021 - acc: 0.9944 - val_loss: 0.3977 - val_acc: 0.9502\nEpoch 764/1000\n - 1s - loss: 0.0902 - acc: 0.9962 - val_loss: 0.3917 - val_acc: 0.9487\nEpoch 765/1000\n - 1s - loss: 0.0957 - acc: 0.9940 - val_loss: 0.4100 - val_acc: 0.9427\nEpoch 766/1000\n - 0s - loss: 0.0933 - acc: 0.9966 - val_loss: 0.3979 - val_acc: 0.9472\nEpoch 767/1000\n - 0s - loss: 0.0910 - acc: 0.9951 - val_loss: 0.3673 - val_acc: 0.9532\nEpoch 768/1000\n - 0s - loss: 0.0866 - acc: 0.9974 - val_loss: 0.3805 - val_acc: 0.9517\nEpoch 769/1000\n - 1s - loss: 0.0968 - acc: 0.9951 - val_loss: 0.3759 - val_acc: 0.9563\nEpoch 770/1000\n - 1s - loss: 0.0790 - acc: 0.9974 - val_loss: 0.4037 - val_acc: 0.9472\nEpoch 771/1000\n - 1s - loss: 0.0982 - acc: 0.9944 - val_loss: 0.3722 - val_acc: 0.9532\nEpoch 772/1000\n - 1s - loss: 0.0796 - acc: 0.9977 - val_loss: 0.3874 - val_acc: 0.9427\nEpoch 773/1000\n - 1s - loss: 0.0939 - acc: 0.9955 - val_loss: 0.4491 - val_acc: 0.9517\nEpoch 774/1000\n - 1s - loss: 0.1051 - acc: 0.9932 - val_loss: 0.3951 - val_acc: 0.9487\nEpoch 775/1000\n - 1s - loss: 0.0920 - acc: 0.9955 - val_loss: 0.3846 - val_acc: 0.9502\nEpoch 776/1000\n - 0s - loss: 0.0978 - acc: 0.9940 - val_loss: 0.3784 - val_acc: 0.9502\nEpoch 777/1000\n - 1s - loss: 0.1011 - acc: 0.9944 - val_loss: 0.4263 - val_acc: 0.9442\nEpoch 778/1000\n - 0s - loss: 0.1013 - acc: 0.9947 - val_loss: 0.4297 - val_acc: 0.9397\nEpoch 779/1000\n - 1s - loss: 0.0736 - acc: 0.9985 - val_loss: 0.3729 - val_acc: 0.9532\nEpoch 780/1000\n - 1s - loss: 0.0980 - acc: 0.9944 - val_loss: 0.3820 - val_acc: 0.9532\nEpoch 781/1000\n - 1s - loss: 0.0817 - acc: 0.9974 - val_loss: 0.3979 - val_acc: 0.9487\nEpoch 782/1000\n - 0s - loss: 0.1101 - acc: 0.9936 - val_loss: 0.4045 - val_acc: 0.9548\nEpoch 783/1000\n - 1s - loss: 0.0950 - acc: 0.9936 - val_loss: 0.4014 - val_acc: 0.9487\nEpoch 784/1000\n - 0s - loss: 0.0807 - acc: 0.9977 - val_loss: 0.3891 - val_acc: 0.9457\nEpoch 785/1000\n - 1s - loss: 0.0886 - acc: 0.9962 - val_loss: 0.3803 - val_acc: 0.9517\nEpoch 786/1000\n - 0s - loss: 0.0808 - acc: 0.9970 - val_loss: 0.4019 - val_acc: 0.9457\nEpoch 787/1000\n - 0s - loss: 0.0901 - acc: 0.9951 - val_loss: 0.3976 - val_acc: 0.9487\nEpoch 788/1000\n - 1s - loss: 0.1032 - acc: 0.9940 - val_loss: 0.4073 - val_acc: 0.9457\nEpoch 789/1000\n - 1s - loss: 0.0903 - acc: 0.9959 - val_loss: 0.3936 - val_acc: 0.9517\nEpoch 790/1000\n - 1s - loss: 0.0924 - acc: 0.9959 - val_loss: 0.3972 - val_acc: 0.9457\nEpoch 791/1000\n - 1s - loss: 0.0863 - acc: 0.9970 - val_loss: 0.3766 - val_acc: 0.9517\nEpoch 792/1000\n - 1s - loss: 0.0961 - acc: 0.9944 - val_loss: 0.3980 - val_acc: 0.9487\nEpoch 793/1000\n - 1s - loss: 0.0710 - acc: 0.9981 - val_loss: 0.3787 - val_acc: 0.9532\nEpoch 794/1000\n - 1s - loss: 0.0858 - acc: 0.9959 - val_loss: 0.4089 - val_acc: 0.9502\nEpoch 795/1000\n - 1s - loss: 0.0901 - acc: 0.9955 - val_loss: 0.4078 - val_acc: 0.9487\nEpoch 796/1000\n - 1s - loss: 0.0946 - acc: 0.9944 - val_loss: 0.4110 - val_acc: 0.9532\nEpoch 797/1000\n - 1s - loss: 0.1078 - acc: 0.9932 - val_loss: 0.4388 - val_acc: 0.9532\nEpoch 798/1000\n - 0s - loss: 0.0927 - acc: 0.9959 - val_loss: 0.3671 - val_acc: 0.9532\nEpoch 799/1000\n - 1s - loss: 0.0953 - acc: 0.9962 - val_loss: 0.3919 - val_acc: 0.9502\nEpoch 800/1000\n - 1s - loss: 0.0995 - acc: 0.9947 - val_loss: 0.3894 - val_acc: 0.9578\nEpoch 801/1000\n - 1s - loss: 0.1195 - acc: 0.9917 - val_loss: 0.4875 - val_acc: 0.9517\nEpoch 802/1000\n - 0s - loss: 0.0969 - acc: 0.9947 - val_loss: 0.3786 - val_acc: 0.9532\nEpoch 803/1000\n - 1s - loss: 0.0940 - acc: 0.9944 - val_loss: 0.3995 - val_acc: 0.9442\nEpoch 804/1000\n - 0s - loss: 0.0973 - acc: 0.9955 - val_loss: 0.4159 - val_acc: 0.9502\nEpoch 805/1000\n - 0s - loss: 0.1014 - acc: 0.9940 - val_loss: 0.3797 - val_acc: 0.9502\nEpoch 806/1000\n - 1s - loss: 0.0918 - acc: 0.9959 - val_loss: 0.4121 - val_acc: 0.9487\nEpoch 807/1000\n - 1s - loss: 0.0836 - acc: 0.9970 - val_loss: 0.4168 - val_acc: 0.9502\nEpoch 808/1000\n - 1s - loss: 0.0802 - acc: 0.9977 - val_loss: 0.4211 - val_acc: 0.9397\nEpoch 809/1000\n - 1s - loss: 0.0715 - acc: 0.9985 - val_loss: 0.4029 - val_acc: 0.9502\nEpoch 810/1000\n - 1s - loss: 0.0776 - acc: 0.9970 - val_loss: 0.4166 - val_acc: 0.9502\nEpoch 811/1000\n - 1s - loss: 0.0799 - acc: 0.9970 - val_loss: 0.3949 - val_acc: 0.9487\nEpoch 812/1000\n - 1s - loss: 0.0800 - acc: 0.9966 - val_loss: 0.4243 - val_acc: 0.9397\nEpoch 813/1000\n - 1s - loss: 0.0938 - acc: 0.9944 - val_loss: 0.4138 - val_acc: 0.9517\nEpoch 814/1000\n - 0s - loss: 0.0885 - acc: 0.9955 - val_loss: 0.3966 - val_acc: 0.9457\nEpoch 815/1000\n - 0s - loss: 0.0804 - acc: 0.9974 - val_loss: 0.4395 - val_acc: 0.9548\nEpoch 816/1000\n - 0s - loss: 0.0836 - acc: 0.9959 - val_loss: 0.3986 - val_acc: 0.9502\nEpoch 817/1000\n - 1s - loss: 0.0920 - acc: 0.9951 - val_loss: 0.4542 - val_acc: 0.9487\nEpoch 818/1000\n - 1s - loss: 0.1060 - acc: 0.9928 - val_loss: 0.4362 - val_acc: 0.9442\nEpoch 819/1000\n - 1s - loss: 0.0830 - acc: 0.9974 - val_loss: 0.4120 - val_acc: 0.9502\nEpoch 820/1000\n - 1s - loss: 0.0861 - acc: 0.9962 - val_loss: 0.3957 - val_acc: 0.9502\nEpoch 821/1000\n - 1s - loss: 0.0922 - acc: 0.9951 - val_loss: 0.3973 - val_acc: 0.9472\nEpoch 822/1000\n - 1s - loss: 0.0757 - acc: 0.9962 - val_loss: 0.4531 - val_acc: 0.9502\nEpoch 823/1000\n - 1s - loss: 0.1008 - acc: 0.9944 - val_loss: 0.4037 - val_acc: 0.9457\nEpoch 824/1000\n - 1s - loss: 0.0932 - acc: 0.9959 - val_loss: 0.5273 - val_acc: 0.9261\nEpoch 825/1000\n - 1s - loss: 0.1059 - acc: 0.9936 - val_loss: 0.4328 - val_acc: 0.9517\nEpoch 826/1000\n - 1s - loss: 0.1027 - acc: 0.9940 - val_loss: 0.3869 - val_acc: 0.9502\nEpoch 827/1000\n - 1s - loss: 0.0893 - acc: 0.9955 - val_loss: 0.4184 - val_acc: 0.9487\nEpoch 828/1000\n - 1s - loss: 0.0892 - acc: 0.9959 - val_loss: 0.4174 - val_acc: 0.9472\nEpoch 829/1000\n - 1s - loss: 0.0853 - acc: 0.9966 - val_loss: 0.4356 - val_acc: 0.9397\nEpoch 830/1000\n - 1s - loss: 0.1036 - acc: 0.9928 - val_loss: 0.3833 - val_acc: 0.9502\nEpoch 831/1000\n - 1s - loss: 0.0944 - acc: 0.9944 - val_loss: 0.4267 - val_acc: 0.9517\nEpoch 832/1000\n - 1s - loss: 0.0804 - acc: 0.9959 - val_loss: 0.4078 - val_acc: 0.9472\nEpoch 833/1000\n - 1s - loss: 0.1025 - acc: 0.9928 - val_loss: 0.3946 - val_acc: 0.9472\nEpoch 834/1000\n - 1s - loss: 0.0992 - acc: 0.9951 - val_loss: 0.4018 - val_acc: 0.9502\nEpoch 835/1000\n - 0s - loss: 0.0905 - acc: 0.9940 - val_loss: 0.4098 - val_acc: 0.9517\nEpoch 836/1000\n - 1s - loss: 0.0871 - acc: 0.9955 - val_loss: 0.4119 - val_acc: 0.9517\nEpoch 837/1000\n - 0s - loss: 0.0896 - acc: 0.9951 - val_loss: 0.4262 - val_acc: 0.9457\nEpoch 838/1000\n - 0s - loss: 0.0914 - acc: 0.9947 - val_loss: 0.4294 - val_acc: 0.9532\nEpoch 839/1000\n - 0s - loss: 0.0942 - acc: 0.9955 - val_loss: 0.4050 - val_acc: 0.9532\nEpoch 840/1000\n - 1s - loss: 0.0867 - acc: 0.9959 - val_loss: 0.4150 - val_acc: 0.9442\nEpoch 841/1000\n - 1s - loss: 0.0977 - acc: 0.9932 - val_loss: 0.3793 - val_acc: 0.9502\nEpoch 842/1000\n - 1s - loss: 0.0705 - acc: 0.9981 - val_loss: 0.4239 - val_acc: 0.9472\nEpoch 843/1000\n - 0s - loss: 0.0972 - acc: 0.9947 - val_loss: 0.4306 - val_acc: 0.9457\nEpoch 844/1000\n - 0s - loss: 0.0895 - acc: 0.9959 - val_loss: 0.4315 - val_acc: 0.9487\nEpoch 845/1000\n - 1s - loss: 0.0804 - acc: 0.9970 - val_loss: 0.4480 - val_acc: 0.9427\nEpoch 846/1000\n - 1s - loss: 0.0876 - acc: 0.9951 - val_loss: 0.3924 - val_acc: 0.9517\nEpoch 847/1000\n - 0s - loss: 0.0863 - acc: 0.9955 - val_loss: 0.4316 - val_acc: 0.9532\nEpoch 848/1000\n - 1s - loss: 0.0823 - acc: 0.9959 - val_loss: 0.4108 - val_acc: 0.9502\nEpoch 849/1000\n - 1s - loss: 0.0908 - acc: 0.9951 - val_loss: 0.4753 - val_acc: 0.9487\nEpoch 850/1000\n - 0s - loss: 0.0997 - acc: 0.9951 - val_loss: 0.4027 - val_acc: 0.9548\nEpoch 851/1000\n - 1s - loss: 0.0785 - acc: 0.9974 - val_loss: 0.3980 - val_acc: 0.9502\nEpoch 852/1000\n","name":"stdout"},{"output_type":"stream","text":" - 0s - loss: 0.0904 - acc: 0.9959 - val_loss: 0.3946 - val_acc: 0.9517\nEpoch 853/1000\n - 1s - loss: 0.1000 - acc: 0.9936 - val_loss: 0.3899 - val_acc: 0.9502\nEpoch 854/1000\n - 1s - loss: 0.0731 - acc: 0.9977 - val_loss: 0.4167 - val_acc: 0.9563\nEpoch 855/1000\n - 1s - loss: 0.1055 - acc: 0.9925 - val_loss: 0.4322 - val_acc: 0.9487\nEpoch 856/1000\n - 1s - loss: 0.0938 - acc: 0.9962 - val_loss: 0.4082 - val_acc: 0.9487\nEpoch 857/1000\n - 1s - loss: 0.0844 - acc: 0.9951 - val_loss: 0.4070 - val_acc: 0.9487\nEpoch 858/1000\n - 1s - loss: 0.0860 - acc: 0.9970 - val_loss: 0.3944 - val_acc: 0.9517\nEpoch 859/1000\n - 1s - loss: 0.0734 - acc: 0.9977 - val_loss: 0.4079 - val_acc: 0.9502\nEpoch 860/1000\n - 1s - loss: 0.0958 - acc: 0.9928 - val_loss: 0.4063 - val_acc: 0.9487\nEpoch 861/1000\n - 0s - loss: 0.0988 - acc: 0.9928 - val_loss: 0.4359 - val_acc: 0.9487\nEpoch 862/1000\n - 1s - loss: 0.0958 - acc: 0.9940 - val_loss: 0.4249 - val_acc: 0.9487\nEpoch 863/1000\n - 0s - loss: 0.0893 - acc: 0.9955 - val_loss: 0.4385 - val_acc: 0.9517\nEpoch 864/1000\n - 1s - loss: 0.1047 - acc: 0.9925 - val_loss: 0.3905 - val_acc: 0.9548\nEpoch 865/1000\n - 0s - loss: 0.0897 - acc: 0.9959 - val_loss: 0.4252 - val_acc: 0.9412\nEpoch 866/1000\n - 0s - loss: 0.1050 - acc: 0.9921 - val_loss: 0.3945 - val_acc: 0.9517\nEpoch 867/1000\n - 0s - loss: 0.0873 - acc: 0.9962 - val_loss: 0.4006 - val_acc: 0.9502\nEpoch 868/1000\n - 0s - loss: 0.0822 - acc: 0.9955 - val_loss: 0.4289 - val_acc: 0.9517\nEpoch 869/1000\n - 1s - loss: 0.0751 - acc: 0.9970 - val_loss: 0.4060 - val_acc: 0.9472\nEpoch 870/1000\n - 1s - loss: 0.0884 - acc: 0.9951 - val_loss: 0.4105 - val_acc: 0.9472\nEpoch 871/1000\n - 1s - loss: 0.0922 - acc: 0.9944 - val_loss: 0.4202 - val_acc: 0.9472\nEpoch 872/1000\n - 1s - loss: 0.0800 - acc: 0.9966 - val_loss: 0.4451 - val_acc: 0.9457\nEpoch 873/1000\n - 0s - loss: 0.1130 - acc: 0.9917 - val_loss: 0.3964 - val_acc: 0.9532\nEpoch 874/1000\n - 0s - loss: 0.0816 - acc: 0.9962 - val_loss: 0.3923 - val_acc: 0.9487\nEpoch 875/1000\n - 0s - loss: 0.0911 - acc: 0.9962 - val_loss: 0.4174 - val_acc: 0.9517\nEpoch 876/1000\n - 0s - loss: 0.1203 - acc: 0.9910 - val_loss: 0.3968 - val_acc: 0.9502\nEpoch 877/1000\n - 0s - loss: 0.0792 - acc: 0.9955 - val_loss: 0.4018 - val_acc: 0.9472\nEpoch 878/1000\n - 1s - loss: 0.0860 - acc: 0.9955 - val_loss: 0.4592 - val_acc: 0.9351\nEpoch 879/1000\n - 1s - loss: 0.0955 - acc: 0.9947 - val_loss: 0.3961 - val_acc: 0.9472\nEpoch 880/1000\n - 0s - loss: 0.0834 - acc: 0.9974 - val_loss: 0.3789 - val_acc: 0.9502\nEpoch 881/1000\n - 1s - loss: 0.0676 - acc: 0.9977 - val_loss: 0.4018 - val_acc: 0.9517\nEpoch 882/1000\n - 1s - loss: 0.0853 - acc: 0.9955 - val_loss: 0.4012 - val_acc: 0.9502\nEpoch 883/1000\n - 1s - loss: 0.0697 - acc: 0.9985 - val_loss: 0.3896 - val_acc: 0.9502\nEpoch 884/1000\n - 0s - loss: 0.0974 - acc: 0.9940 - val_loss: 0.4255 - val_acc: 0.9457\nEpoch 885/1000\n - 0s - loss: 0.1003 - acc: 0.9928 - val_loss: 0.4318 - val_acc: 0.9532\nEpoch 886/1000\n - 0s - loss: 0.0854 - acc: 0.9966 - val_loss: 0.4582 - val_acc: 0.9351\nEpoch 887/1000\n - 1s - loss: 0.1121 - acc: 0.9921 - val_loss: 0.3939 - val_acc: 0.9548\nEpoch 888/1000\n - 1s - loss: 0.0831 - acc: 0.9955 - val_loss: 0.4109 - val_acc: 0.9517\nEpoch 889/1000\n - 0s - loss: 0.0957 - acc: 0.9936 - val_loss: 0.3785 - val_acc: 0.9502\nEpoch 890/1000\n - 1s - loss: 0.0954 - acc: 0.9955 - val_loss: 0.3866 - val_acc: 0.9502\nEpoch 891/1000\n - 1s - loss: 0.0724 - acc: 0.9970 - val_loss: 0.3749 - val_acc: 0.9532\nEpoch 892/1000\n - 1s - loss: 0.0904 - acc: 0.9947 - val_loss: 0.3973 - val_acc: 0.9502\nEpoch 893/1000\n - 1s - loss: 0.0776 - acc: 0.9962 - val_loss: 0.4052 - val_acc: 0.9487\nEpoch 894/1000\n - 0s - loss: 0.0990 - acc: 0.9932 - val_loss: 0.4048 - val_acc: 0.9502\nEpoch 895/1000\n - 1s - loss: 0.0759 - acc: 0.9974 - val_loss: 0.3963 - val_acc: 0.9487\nEpoch 896/1000\n - 1s - loss: 0.0633 - acc: 0.9977 - val_loss: 0.4075 - val_acc: 0.9472\nEpoch 897/1000\n - 1s - loss: 0.0687 - acc: 0.9981 - val_loss: 0.4018 - val_acc: 0.9472\nEpoch 898/1000\n - 1s - loss: 0.0839 - acc: 0.9962 - val_loss: 0.4759 - val_acc: 0.9321\nEpoch 899/1000\n - 0s - loss: 0.0873 - acc: 0.9966 - val_loss: 0.3975 - val_acc: 0.9487\nEpoch 900/1000\n - 0s - loss: 0.0869 - acc: 0.9951 - val_loss: 0.3999 - val_acc: 0.9487\nEpoch 901/1000\n - 0s - loss: 0.0741 - acc: 0.9974 - val_loss: 0.4086 - val_acc: 0.9487\nEpoch 902/1000\n - 0s - loss: 0.0665 - acc: 0.9970 - val_loss: 0.4277 - val_acc: 0.9367\nEpoch 903/1000\n - 0s - loss: 0.0844 - acc: 0.9944 - val_loss: 0.4156 - val_acc: 0.9502\nEpoch 904/1000\n - 0s - loss: 0.1012 - acc: 0.9928 - val_loss: 0.4503 - val_acc: 0.9487\nEpoch 905/1000\n - 0s - loss: 0.1188 - acc: 0.9906 - val_loss: 0.4218 - val_acc: 0.9472\nEpoch 906/1000\n - 0s - loss: 0.0771 - acc: 0.9974 - val_loss: 0.4243 - val_acc: 0.9472\nEpoch 907/1000\n - 1s - loss: 0.0702 - acc: 0.9977 - val_loss: 0.4650 - val_acc: 0.9472\nEpoch 908/1000\n - 1s - loss: 0.0854 - acc: 0.9944 - val_loss: 0.4296 - val_acc: 0.9472\nEpoch 909/1000\n - 1s - loss: 0.0897 - acc: 0.9944 - val_loss: 0.4374 - val_acc: 0.9487\nEpoch 910/1000\n - 1s - loss: 0.0871 - acc: 0.9955 - val_loss: 0.4226 - val_acc: 0.9487\nEpoch 911/1000\n - 1s - loss: 0.0897 - acc: 0.9944 - val_loss: 0.4319 - val_acc: 0.9472\nEpoch 912/1000\n - 1s - loss: 0.0775 - acc: 0.9959 - val_loss: 0.4262 - val_acc: 0.9502\nEpoch 913/1000\n - 1s - loss: 0.0850 - acc: 0.9951 - val_loss: 0.4221 - val_acc: 0.9487\nEpoch 914/1000\n - 1s - loss: 0.0927 - acc: 0.9947 - val_loss: 0.4025 - val_acc: 0.9517\nEpoch 915/1000\n - 1s - loss: 0.0860 - acc: 0.9966 - val_loss: 0.4104 - val_acc: 0.9517\nEpoch 916/1000\n - 1s - loss: 0.0704 - acc: 0.9981 - val_loss: 0.4338 - val_acc: 0.9532\nEpoch 917/1000\n - 1s - loss: 0.0862 - acc: 0.9947 - val_loss: 0.4018 - val_acc: 0.9502\nEpoch 918/1000\n - 1s - loss: 0.0764 - acc: 0.9970 - val_loss: 0.4259 - val_acc: 0.9472\nEpoch 919/1000\n - 1s - loss: 0.0729 - acc: 0.9966 - val_loss: 0.3985 - val_acc: 0.9517\nEpoch 920/1000\n - 1s - loss: 0.0982 - acc: 0.9928 - val_loss: 0.4371 - val_acc: 0.9427\nEpoch 921/1000\n - 1s - loss: 0.1026 - acc: 0.9932 - val_loss: 0.4113 - val_acc: 0.9487\nEpoch 922/1000\n - 1s - loss: 0.1006 - acc: 0.9944 - val_loss: 0.4732 - val_acc: 0.9367\nEpoch 923/1000\n - 1s - loss: 0.0966 - acc: 0.9932 - val_loss: 0.4394 - val_acc: 0.9502\nEpoch 924/1000\n - 1s - loss: 0.0800 - acc: 0.9966 - val_loss: 0.4252 - val_acc: 0.9502\nEpoch 925/1000\n - 1s - loss: 0.0862 - acc: 0.9959 - val_loss: 0.4220 - val_acc: 0.9502\nEpoch 926/1000\n - 1s - loss: 0.1088 - acc: 0.9928 - val_loss: 0.4210 - val_acc: 0.9472\nEpoch 927/1000\n - 1s - loss: 0.0933 - acc: 0.9936 - val_loss: 0.4134 - val_acc: 0.9487\nEpoch 928/1000\n - 1s - loss: 0.0829 - acc: 0.9959 - val_loss: 0.4068 - val_acc: 0.9487\nEpoch 929/1000\n - 1s - loss: 0.0831 - acc: 0.9962 - val_loss: 0.4085 - val_acc: 0.9487\nEpoch 930/1000\n - 1s - loss: 0.0784 - acc: 0.9962 - val_loss: 0.4289 - val_acc: 0.9502\nEpoch 931/1000\n - 0s - loss: 0.0821 - acc: 0.9962 - val_loss: 0.4287 - val_acc: 0.9502\nEpoch 932/1000\n - 1s - loss: 0.0679 - acc: 0.9981 - val_loss: 0.4384 - val_acc: 0.9532\nEpoch 933/1000\n - 0s - loss: 0.0760 - acc: 0.9970 - val_loss: 0.4141 - val_acc: 0.9517\nEpoch 934/1000\n - 1s - loss: 0.0896 - acc: 0.9940 - val_loss: 0.4408 - val_acc: 0.9472\nEpoch 935/1000\n - 0s - loss: 0.0853 - acc: 0.9951 - val_loss: 0.4193 - val_acc: 0.9532\nEpoch 936/1000\n - 1s - loss: 0.0786 - acc: 0.9970 - val_loss: 0.3926 - val_acc: 0.9532\nEpoch 937/1000\n - 0s - loss: 0.0780 - acc: 0.9970 - val_loss: 0.4190 - val_acc: 0.9517\nEpoch 938/1000\n - 0s - loss: 0.0803 - acc: 0.9944 - val_loss: 0.4145 - val_acc: 0.9517\nEpoch 939/1000\n - 0s - loss: 0.0711 - acc: 0.9977 - val_loss: 0.4796 - val_acc: 0.9457\nEpoch 940/1000\n - 0s - loss: 0.0806 - acc: 0.9955 - val_loss: 0.4324 - val_acc: 0.9487\nEpoch 941/1000\n - 0s - loss: 0.0845 - acc: 0.9959 - val_loss: 0.4523 - val_acc: 0.9427\nEpoch 942/1000\n - 1s - loss: 0.0829 - acc: 0.9962 - val_loss: 0.4370 - val_acc: 0.9517\nEpoch 943/1000\n - 1s - loss: 0.0813 - acc: 0.9962 - val_loss: 0.4316 - val_acc: 0.9457\nEpoch 944/1000\n - 0s - loss: 0.0977 - acc: 0.9944 - val_loss: 0.4351 - val_acc: 0.9472\nEpoch 945/1000\n - 1s - loss: 0.0832 - acc: 0.9947 - val_loss: 0.4470 - val_acc: 0.9412\nEpoch 946/1000\n - 1s - loss: 0.1039 - acc: 0.9936 - val_loss: 0.4217 - val_acc: 0.9487\n","name":"stdout"},{"output_type":"stream","text":"Epoch 947/1000\n - 1s - loss: 0.0821 - acc: 0.9962 - val_loss: 0.4505 - val_acc: 0.9502\nEpoch 948/1000\n - 1s - loss: 0.0980 - acc: 0.9925 - val_loss: 0.4468 - val_acc: 0.9457\nEpoch 949/1000\n - 0s - loss: 0.1040 - acc: 0.9913 - val_loss: 0.4790 - val_acc: 0.9548\nEpoch 950/1000\n - 1s - loss: 0.1157 - acc: 0.9910 - val_loss: 0.4580 - val_acc: 0.9412\nEpoch 951/1000\n - 1s - loss: 0.0833 - acc: 0.9959 - val_loss: 0.4160 - val_acc: 0.9578\nEpoch 952/1000\n - 1s - loss: 0.0850 - acc: 0.9951 - val_loss: 0.4819 - val_acc: 0.9442\nEpoch 953/1000\n - 0s - loss: 0.0991 - acc: 0.9932 - val_loss: 0.4418 - val_acc: 0.9502\nEpoch 954/1000\n - 0s - loss: 0.0864 - acc: 0.9959 - val_loss: 0.4395 - val_acc: 0.9502\nEpoch 955/1000\n - 1s - loss: 0.0657 - acc: 0.9977 - val_loss: 0.4217 - val_acc: 0.9472\nEpoch 956/1000\n - 1s - loss: 0.0800 - acc: 0.9947 - val_loss: 0.4300 - val_acc: 0.9487\nEpoch 957/1000\n - 1s - loss: 0.1004 - acc: 0.9940 - val_loss: 0.4381 - val_acc: 0.9532\nEpoch 958/1000\n - 1s - loss: 0.0879 - acc: 0.9962 - val_loss: 0.4140 - val_acc: 0.9517\nEpoch 959/1000\n - 1s - loss: 0.0815 - acc: 0.9955 - val_loss: 0.4580 - val_acc: 0.9427\nEpoch 960/1000\n - 0s - loss: 0.0821 - acc: 0.9962 - val_loss: 0.4135 - val_acc: 0.9487\nEpoch 961/1000\n - 0s - loss: 0.0783 - acc: 0.9962 - val_loss: 0.4417 - val_acc: 0.9457\nEpoch 962/1000\n - 1s - loss: 0.0848 - acc: 0.9955 - val_loss: 0.4456 - val_acc: 0.9397\nEpoch 963/1000\n - 1s - loss: 0.0753 - acc: 0.9959 - val_loss: 0.4229 - val_acc: 0.9457\nEpoch 964/1000\n - 1s - loss: 0.0777 - acc: 0.9962 - val_loss: 0.4335 - val_acc: 0.9502\nEpoch 965/1000\n - 1s - loss: 0.0933 - acc: 0.9947 - val_loss: 0.4229 - val_acc: 0.9472\nEpoch 966/1000\n - 0s - loss: 0.0655 - acc: 0.9977 - val_loss: 0.4375 - val_acc: 0.9487\nEpoch 967/1000\n - 1s - loss: 0.0961 - acc: 0.9936 - val_loss: 0.4030 - val_acc: 0.9517\nEpoch 968/1000\n - 1s - loss: 0.0872 - acc: 0.9959 - val_loss: 0.4138 - val_acc: 0.9472\nEpoch 969/1000\n - 0s - loss: 0.0662 - acc: 0.9981 - val_loss: 0.4113 - val_acc: 0.9487\nEpoch 970/1000\n - 1s - loss: 0.0905 - acc: 0.9947 - val_loss: 0.4458 - val_acc: 0.9487\nEpoch 971/1000\n - 0s - loss: 0.0869 - acc: 0.9959 - val_loss: 0.4282 - val_acc: 0.9457\nEpoch 972/1000\n - 1s - loss: 0.0813 - acc: 0.9955 - val_loss: 0.4509 - val_acc: 0.9472\nEpoch 973/1000\n - 0s - loss: 0.0923 - acc: 0.9936 - val_loss: 0.4375 - val_acc: 0.9502\nEpoch 974/1000\n - 1s - loss: 0.0755 - acc: 0.9966 - val_loss: 0.4431 - val_acc: 0.9502\nEpoch 975/1000\n - 1s - loss: 0.0685 - acc: 0.9966 - val_loss: 0.4644 - val_acc: 0.9442\nEpoch 976/1000\n - 1s - loss: 0.0791 - acc: 0.9959 - val_loss: 0.4201 - val_acc: 0.9502\nEpoch 977/1000\n - 1s - loss: 0.0980 - acc: 0.9928 - val_loss: 0.4222 - val_acc: 0.9442\nEpoch 978/1000\n - 1s - loss: 0.0882 - acc: 0.9947 - val_loss: 0.4620 - val_acc: 0.9517\nEpoch 979/1000\n - 1s - loss: 0.0917 - acc: 0.9940 - val_loss: 0.4730 - val_acc: 0.9427\nEpoch 980/1000\n - 1s - loss: 0.0856 - acc: 0.9962 - val_loss: 0.4266 - val_acc: 0.9472\nEpoch 981/1000\n - 1s - loss: 0.0869 - acc: 0.9955 - val_loss: 0.4318 - val_acc: 0.9457\nEpoch 982/1000\n - 0s - loss: 0.1030 - acc: 0.9917 - val_loss: 0.4824 - val_acc: 0.9427\nEpoch 983/1000\n - 1s - loss: 0.0809 - acc: 0.9959 - val_loss: 0.4499 - val_acc: 0.9427\nEpoch 984/1000\n - 0s - loss: 0.0690 - acc: 0.9981 - val_loss: 0.4548 - val_acc: 0.9442\nEpoch 985/1000\n - 0s - loss: 0.0839 - acc: 0.9955 - val_loss: 0.4374 - val_acc: 0.9487\nEpoch 986/1000\n - 1s - loss: 0.0872 - acc: 0.9944 - val_loss: 0.4386 - val_acc: 0.9427\nEpoch 987/1000\n - 1s - loss: 0.0859 - acc: 0.9955 - val_loss: 0.4804 - val_acc: 0.9382\nEpoch 988/1000\n - 0s - loss: 0.0767 - acc: 0.9970 - val_loss: 0.4316 - val_acc: 0.9502\nEpoch 989/1000\n - 1s - loss: 0.0792 - acc: 0.9959 - val_loss: 0.4342 - val_acc: 0.9502\nEpoch 990/1000\n - 1s - loss: 0.0945 - acc: 0.9944 - val_loss: 0.4260 - val_acc: 0.9502\nEpoch 991/1000\n - 1s - loss: 0.0792 - acc: 0.9966 - val_loss: 0.4565 - val_acc: 0.9472\nEpoch 992/1000\n - 1s - loss: 0.0782 - acc: 0.9962 - val_loss: 0.4359 - val_acc: 0.9412\nEpoch 993/1000\n - 0s - loss: 0.1018 - acc: 0.9917 - val_loss: 0.4199 - val_acc: 0.9457\nEpoch 994/1000\n - 0s - loss: 0.1179 - acc: 0.9925 - val_loss: 0.4285 - val_acc: 0.9442\nEpoch 995/1000\n - 0s - loss: 0.0917 - acc: 0.9940 - val_loss: 0.4478 - val_acc: 0.9427\nEpoch 996/1000\n - 0s - loss: 0.0997 - acc: 0.9928 - val_loss: 0.4137 - val_acc: 0.9502\nEpoch 997/1000\n - 0s - loss: 0.0868 - acc: 0.9936 - val_loss: 0.4546 - val_acc: 0.9457\nEpoch 998/1000\n - 0s - loss: 0.0947 - acc: 0.9947 - val_loss: 0.4563 - val_acc: 0.9442\nEpoch 999/1000\n - 1s - loss: 0.0786 - acc: 0.9962 - val_loss: 0.4317 - val_acc: 0.9487\nEpoch 1000/1000\n - 0s - loss: 0.0867 - acc: 0.9951 - val_loss: 0.4347 - val_acc: 0.9502\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size= batch_size, verbose=1)\n\nprint(\"Validation Accuracy: {:.4f}%\".format(eval_accuracy * 100))\nprint(\"Validation Loss: {}\".format(eval_loss))","execution_count":24,"outputs":[{"output_type":"stream","text":"663/663 [==============================] - 0s 67us/step\nValidation Accuracy: 95.0226%\nValidation Loss: 0.43467738082685803\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = test_generator.filenames\ntruth = test_generator.classes\nlabel = test_generator.class_indices\nindexlabel = dict((value, key) for key, value in label.items())\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\npreds = model.predict(test_data)\n\npredictions = [i.argmax() for i in preds]\ny_true = [i.argmax() for i in test_labels]\ncm = confusion_matrix(y_pred=predictions, y_true=y_true)\n\nprint('Test Accuracy: {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))","execution_count":25,"outputs":[{"output_type":"stream","text":"Test Accuracy: 0.9387755102040817\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"axes.grid\"] = False\nplt.rcParams.update({'font.size': 20})\n\nlabels = []\n\nlabel = test_generator.class_indices\nindexlabel = dict((value, key) for key, value in label.items())\n\nfor k,v in indexlabel.items():\n    labels.append(v)\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion Matrix')\n\n    print(cm)\n#     fig = plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n#     plt.title(title)\n#     plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n#     plt.savefig('plots/3.InceptionV3-2-Private-DataSet-CM.png', bbox_inches='tight', dpi = 100) \n\n\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cm, classes=labels, title=' ')","execution_count":26,"outputs":[{"output_type":"stream","text":"Confusion Matrix\n[[161   7]\n [ 14 161]]\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\ny_pred=predictions\ny_pred_probabilities=y_pred\n\n# y_pred = np.argmax(y_pred,axis = 1) \ny_actual = y_true\n\nclassnames=[]\nfor classname in test_generator.class_indices:\n    classnames.append(classname)\n\nconfusion_mtx = confusion_matrix(y_actual, y_pred) \nprint(confusion_mtx)\ntarget_names = classnames\nprint(classification_report(y_actual, y_pred, target_names=target_names))","execution_count":27,"outputs":[{"output_type":"stream","text":"[[161   7]\n [ 14 161]]\n              precision    recall  f1-score   support\n\n          DR       0.92      0.96      0.94       168\n       No_DR       0.96      0.92      0.94       175\n\n    accuracy                           0.94       343\n   macro avg       0.94      0.94      0.94       343\nweighted avg       0.94      0.94      0.94       343\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total=sum(sum(cm))\n\nsensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\nprint('Sensitivity : ', sensitivity )\n\nSpecificity = cm[1,1]/(cm[1,1]+cm[0,1])\nprint('Specificity : ', Specificity )","execution_count":28,"outputs":[{"output_type":"stream","text":"Sensitivity :  0.92\nSpecificity :  0.9583333333333334\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_class = model.predict(test_data, verbose=1)\n\ny_pred_class = [np.argmax(r) for r in y_pred_class]\ntest_y = [np.argmax(r) for r in test_labels]\n\n\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, roc_curve\n\n# Precision\nprint('Precision = ', precision_score(test_y, y_pred_class, average='weighted'))\n# (None, 'micro', 'macro', 'weighted', 'samples')\n\n# Recall\nprint('Recall = ', recall_score(test_y, y_pred_class, average='weighted'))\n\n# f1_score\nprint('f1_score = ', f1_score(test_y, y_pred_class, average='weighted'))","execution_count":29,"outputs":[{"output_type":"stream","text":"343/343 [==============================] - 0s 62us/step\nPrecision =  0.9395578231292516\nRecall =  0.9387755102040817\nf1_score =  0.9387755102040817\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\n\ndef multiclass_roc_auc_score(y_test, y_pred, average=\"weighted\"):\n    label_binarizer = LabelBinarizer()\n    label_binarizer.fit(y_test)\n\n    truth = label_binarizer.transform(y_test)\n    pred = label_binarizer.transform(y_pred)\n    return roc_auc_score(truth, pred, average=average)\n# roc_auc_score\nprint('roc_auc_score = ', multiclass_roc_auc_score(test_y, y_pred_class))","execution_count":30,"outputs":[{"output_type":"stream","text":"roc_auc_score =  0.9391666666666667\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score, classification_report\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\nk_fold = KFold(n_splits=10, shuffle=True, random_state=5)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.asarray(test_labels)\ny_test = np.argmax(y_test, axis=1)\n\ny_train = np.asarray(train_labels)\ny_train = np.argmax(y_train, axis=1)","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BaggingClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('AdaBoost Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":33,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9887048192771084\nTest accuracy 0.9416909620991254\nAdaBoost Classifier test accuracies 0.9417\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94       168\n           1       0.97      0.91      0.94       175\n\n    accuracy                           0.94       343\n   macro avg       0.94      0.94      0.94       343\nweighted avg       0.94      0.94      0.94       343\n\n0.9416909620991254\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**BaggingClassifier - KFold:10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nscoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"AdaBoost - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":34,"outputs":[{"output_type":"stream","text":"Scores Mean: 92.3950 and (STDEV 0.0273)\nBest result for fold 2\nBest accuracy is 0.9714285714285714\nScores of all folds: [0.94285714 0.94285714 0.97142857 0.94117647 0.88235294 0.88235294\n 0.91176471 0.91176471 0.91176471 0.94117647]\nAdaBoost - Test Accuracy on all folds: 0.92 (+/- 0.05)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**AdaBoostClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('AdaBoost Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":35,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9996234939759037\nTest accuracy 0.9475218658892128\nAdaBoost Classifier test accuracies 0.9475\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95       168\n           1       0.97      0.93      0.95       175\n\n    accuracy                           0.95       343\n   macro avg       0.95      0.95      0.95       343\nweighted avg       0.95      0.95      0.95       343\n\n0.9475218658892128\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**AdaBoostClassifier - KFold:10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(AdaBoost Classifier) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":36,"outputs":[{"output_type":"stream","text":"Scores Mean: 87.7311 and (STDEV 0.0437)\nBest result for fold 6\nBest accuracy is 0.9411764705882353\nScores of all folds: [0.88571429 0.91428571 0.91428571 0.85294118 0.79411765 0.88235294\n 0.94117647 0.85294118 0.82352941 0.91176471]\n(AdaBoost Classifier) Test Accuracy on all folds: 0.88 (+/- 0.09)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**XGBClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('XGB Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":37,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9969879518072289\nTest accuracy 0.9591836734693877\nXGB Classifier test accuracies 0.9592\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       168\n           1       0.96      0.96      0.96       175\n\n    accuracy                           0.96       343\n   macro avg       0.96      0.96      0.96       343\nweighted avg       0.96      0.96      0.96       343\n\n0.9591836734693877\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**XGBClassifier - KFold: 10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"XGB - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":38,"outputs":[{"output_type":"stream","text":"Scores Mean: 95.0420 and (STDEV 0.0323)\nBest result for fold 8\nBest accuracy is 1.0\nScores of all folds: [0.94285714 0.94285714 0.97142857 0.97058824 0.94117647 0.91176471\n 0.97058824 0.88235294 1.         0.97058824]\nXGB - Test Accuracy on all folds: 0.95 (+/- 0.06)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**DecisionTreeClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('DecisionTree Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":39,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9996234939759037\nTest accuracy 0.9300291545189504\nDecisionTree Classifier test accuracies 0.9300\n              precision    recall  f1-score   support\n\n           0       0.92      0.93      0.93       168\n           1       0.94      0.93      0.93       175\n\n    accuracy                           0.93       343\n   macro avg       0.93      0.93      0.93       343\nweighted avg       0.93      0.93      0.93       343\n\n0.9300291545189504\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**DecisionTreeClassifier - KFold: 10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"DecisionTree - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":40,"outputs":[{"output_type":"stream","text":"Scores Mean: 84.8319 and (STDEV 0.0521)\nBest result for fold 3\nBest accuracy is 0.9411764705882353\nScores of all folds: [0.85714286 0.8        0.91428571 0.94117647 0.79411765 0.91176471\n 0.82352941 0.82352941 0.79411765 0.82352941]\nDecisionTree - Test Accuracy on all folds: 0.85 (+/- 0.10)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**RandomForestClassifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=5)\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('RandomForest Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":41,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9909638554216867\nTest accuracy 0.9387755102040817\nRandomForest Classifier test accuracies 0.9388\n              precision    recall  f1-score   support\n\n           0       0.92      0.95      0.94       168\n           1       0.95      0.93      0.94       175\n\n    accuracy                           0.94       343\n   macro avg       0.94      0.94      0.94       343\nweighted avg       0.94      0.94      0.94       343\n\n0.9387755102040817\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**RandomForestClassifier - KFold: 10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=5)\nscoring = 'accuracy'\nscores = cross_val_score(clf, test_data, test_labels, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(RandomForest) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":42,"outputs":[{"output_type":"stream","text":"Scores Mean: 91.5378 and (STDEV 0.0424)\nBest result for fold 8\nBest accuracy is 1.0\nScores of all folds: [0.91428571 0.97142857 0.88571429 0.91176471 0.88235294 0.91176471\n 0.88235294 0.85294118 1.         0.94117647]\n(RandomForest) Test Accuracy on all folds: 0.92 (+/- 0.08)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**LightGBM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nlgbm=lgb.LGBMClassifier(n_estimators=1000, class_weight=\"balanced\", reg_alpha=0.1, reg_lambda=0.1, learning_rate=0.001, num_leaves=400,\n                        random_state=523, boosting='dart')\n\nlgbm_scores=cross_val_score(lgbm,train_data, y_train, cv=10)\nprint(lgbm_scores)\nprint(\"Train accuracy mean and std %.2f\" %np.mean(lgbm_scores),\"+/- %.2f\"%np.std(lgbm_scores))","execution_count":43,"outputs":[{"output_type":"stream","text":"[0.92883895 0.91385768 0.94360902 0.94736842 0.93584906 0.93207547\n 0.94716981 0.94339623 0.93962264 0.95471698]\nTrain accuracy mean and std 0.94 +/- 0.01\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm.fit(train_data, y_train)\ny_pred=lgbm.predict(test_data)\nprint(classification_report(y_test, y_pred))\n\nprint(accuracy_score(y_test, y_test_pred))","execution_count":48,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.95      0.98      0.96       168\n           1       0.98      0.95      0.97       175\n\n    accuracy                           0.97       343\n   macro avg       0.97      0.97      0.97       343\nweighted avg       0.97      0.97      0.97       343\n\n0.9387755102040817\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**LightGBM - KFold: 10**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(lgbm, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(LightGBM) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":50,"outputs":[{"output_type":"stream","text":"Scores Mean: 88.6218 and (STDEV 0.0495)\nBest result for fold 1\nBest accuracy is 0.9714285714285714\nScores of all folds: [0.82857143 0.97142857 0.88571429 0.91176471 0.94117647 0.85294118\n 0.91176471 0.88235294 0.88235294 0.79411765]\n(LightGBM) Test Accuracy on all folds: 0.89 (+/- 0.10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'font.size': 12})\n\nimport seaborn\nplt.style.use('seaborn-white')\n\nplt.figure()\nN = epochs\nplt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"center right\")","execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f4fd231d9b0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}
