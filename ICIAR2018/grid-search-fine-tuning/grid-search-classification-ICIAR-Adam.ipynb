{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "b9a993d180f1a520a86812c8b22f7143bbc0020a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  return f(*args, **kwds)\n",
      "/home/sara/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\n",
    "from sklearn.metrics import log_loss\n",
    "import sys\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "import keras.callbacks as kcall\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import optimizers, metrics, models\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.optimizers import SGD, Adam, rmsprop\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "469bc6705d524cb8739eefffb752dc19cbdd089c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InSitu', 'Invasive', 'Normal', 'Benign']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"data/Reinhard\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "74272bb10a59f27d58600920f7b08105477b587b"
   },
   "outputs": [],
   "source": [
    "## Intilizing variables\n",
    "output_classes = 4\n",
    "# batch_size = 32 \n",
    "# epochs = 30\n",
    "\n",
    "# sgd_opt = SGD(lr=1E-2, decay=1E-4, momentum=0.9, nesterov=True)\n",
    "# adam_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-4)\n",
    "# eve_opt = Eve(lr=1E-4, decay=1E-4, beta_1=0.9, beta_2=0.999, beta_3=0.999, small_k=0.1, big_K=10, epsilon=1e-08)\n",
    "\n",
    "resume_model = False\n",
    "xception_weights = 'pretrained-models/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "31fab7218083e8aa3b9e9fc88cf10045cb47167c"
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "         'lr': hp.choice('lr',[0.001, 0.0001, 0.00001, 0.000001]),\n",
    "#          'dropout': hp.choice('dropout', [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]),\n",
    "         'batch_size': hp.choice('batch_size', [32, 64]),\n",
    "         'epochs': hp.choice('epochs', [20]),\n",
    "#        'optimizer': hp.choice('optimizer',['sgd','adam','rmsprop']),\n",
    "#          'activation': hp.choice('activation',['relu',\n",
    "#                                                 'tanh']),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "5cd9fc51951d479e3ad77627926397cf4e5ed9e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Reinhard 0\n",
      "data/Reinhard/InSitu 3500\n",
      "data/Reinhard/Invasive 3500\n",
      "data/Reinhard/Normal 3500\n",
      "data/Reinhard/Benign 3500\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/Reinhard'\n",
    "for root,dirs,files in os.walk(train_dir):\n",
    "    print (root, len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "378d5d9184e33383a6c6ae0531a699df9079b404"
   },
   "outputs": [],
   "source": [
    "def f_nn(params):   \n",
    "    print ('Parameters testing: ', params)\n",
    "    batch_size=params['batch_size']\n",
    "    epochs=params['epochs']\n",
    "\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            fill_mode='nearest',\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.25)\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "    #target_size: Tuple of integers (height, width), default: (256, 256). \n",
    "    #The dimensions to which all images found will be resized.\n",
    "    target_size = (256, 256)\n",
    "    #target_size = (height, width)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size = target_size,       \n",
    "            class_mode = 'categorical',\n",
    "            batch_size=32,\n",
    "            subset=\"training\",\n",
    "            shuffle = True)\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size = target_size,        \n",
    "            class_mode = 'categorical',\n",
    "            batch_size=32,\n",
    "            subset = \"validation\",\n",
    "            shuffle = True)\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Xception(weights = xception_weights , include_top=False, pooling = 'avg'))\n",
    "    model.add(Dense(units=output_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = Adam(lr=params['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1E-4),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch = 2000,\n",
    "      epochs = epochs,\n",
    "      validation_data = validation_generator,\n",
    "#       validation_steps = 100,\n",
    "      verbose = 1, callbacks=get_callbacks(params))\n",
    "    \n",
    "    \n",
    "    best_epoch = np.argmax(history.history['val_acc'])\n",
    "    best_val_acc = np.max(history.history['val_acc'])\n",
    "    print('Epoch {} - val acc: {}'.format(best_epoch, best_val_acc))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "    return {'val_acc': best_val_acc, 'best_epoch': best_epoch, 'eval_time': time.time(), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e1abd5240499d10fe36059fad182ee6749c72cc5"
   },
   "outputs": [],
   "source": [
    "output_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "9e3dcb0aca71f03a85b6e4a677a54803937b2341"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(params):\n",
    "    callbacks =[EarlyStopping(monitor='val_acc', patience=5, verbose=1),\n",
    "                ModelCheckpoint('callbacks/{}.h5'.format(params['batch_size']), save_best_only=True),\n",
    "             TensorBoard('tensor-logs/logs-gridsearch', write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad58082fb3528fe4a93b17b2f2aa5e87712eaded"
   },
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58ba906fe86e164a127b64490f6cf81d91955737"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters testing:  {'batch_size': 64, 'epochs': 20, 'lr': 1e-05}\n",
      "Found 10500 images belonging to 4 classes.\n",
      "Found 3500 images belonging to 4 classes.\n",
      "Epoch 1/20\n",
      " 225/2000 [==>...........................] - ETA: 10:45:24 - loss: 1.2163 - acc: 0.4817"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "307852ca436f3b6f178564883600c2b73240b7f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "831421fc8eed78a8f2a9299b299543e3ab70b293"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
