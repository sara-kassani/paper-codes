{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 9212259107649648837,\n name: \"/device:XLA_CPU:0\"\n device_type: \"XLA_CPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 14349385063696313470\n physical_device_desc: \"device: XLA_CPU device\",\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 15695549568\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 14694015851225181755\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\",\n name: \"/device:XLA_GPU:0\"\n device_type: \"XLA_GPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 3615640280207632478\n physical_device_desc: \"device: XLA_GPU device\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\nfrom sklearn.metrics import log_loss\nimport sys\nimport time\nimport math\nimport os\nimport pandas as pd\nfrom keras.callbacks import EarlyStopping\nimport numpy as np\nfrom glob import glob\nimport cv2\nimport skimage\nfrom skimage.transform import resize\nfrom keras.utils.np_utils import to_categorical\nimport keras\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.models import load_model\n# import keras.callbacks as kcall\nfrom keras.optimizers import Adam, RMSprop,SGD\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\nfrom keras.applications.vgg19 import VGG19\nfrom keras.regularizers import l2, l1\n\nimport matplotlib.pyplot as plt\nfrom keras.layers import Input, concatenate\nfrom keras import optimizers, metrics, models\nfrom keras.layers import Input, Flatten, Dense\n\n%matplotlib inline","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\n\nprint(\"Keras Version\", keras.__version__)\nprint(\"tensorflow Version\", tf.__version__)\n# print(\"dim_ordering:\", K.image_dim_ordering())","execution_count":4,"outputs":[{"output_type":"stream","text":"Keras Version 2.4.3\ntensorflow Version 2.3.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_height, img_width = 800, 600\ninput_shape = (img_height, img_width, 3)\nepochs = 1000","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/data-pig/\"))","execution_count":6,"outputs":[{"output_type":"stream","text":"['train', 'test']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/data-pig/train/'\ntest_dir = '../input/data-pig/test/'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(x):\n    # 'RGB'->'BGR'\n    x = x[:, :, ::-1]\n    # Zero-center by imagenet mean pixel\n    x[:, :, 0] -= 103.939\n    x[:, :, 1] -= 116.779\n    x[:, :, 2] -= 123.68\n    return x","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = np.random.seed(1142)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n    preprocessing_function = preprocess_input,\n    rotation_range=90,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.3,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='reflect',\n    validation_split= 0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'training',\n    class_mode='categorical')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'validation',\n    class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    class_mode='categorical')","execution_count":9,"outputs":[{"output_type":"stream","text":"Found 8061 images belonging to 4 classes.\nFound 2012 images belonging to 4 classes.\nFound 1776 images belonging to 4 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(train_generator.filenames)\nnb_validation_samples = len(validation_generator.filenames)\nnb_test_samples = len(test_generator.filenames)\n\npredict_size_train = int(math.ceil(nb_train_samples / batch_size))\npredict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\npredict_size_test = int(math.ceil(nb_test_samples / batch_size))\n\nnum_classes = len(train_generator.class_indices)\n\nprint(\"nb_train_samples:\", nb_train_samples)\nprint(\"nb_validation_samples:\", nb_validation_samples)\nprint(\"nb_test_samples:\", nb_test_samples)\n\nprint(\"\\npredict_size_train:\", predict_size_train)\nprint(\"predict_size_validation:\", predict_size_validation)\nprint(\"predict_size_test:\", predict_size_test)\n\nprint(\"\\n num_classes:\", num_classes)","execution_count":10,"outputs":[{"output_type":"stream","text":"nb_train_samples: 8061\nnb_validation_samples: 2012\nnb_test_samples: 1776\n\npredict_size_train: 252\npredict_size_validation: 63\npredict_size_test: 56\n\n num_classes: 4\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"extracted_features\")\nextracted_features_dir = \"extracted_features/\"\nmodel_name = \"VGG19_descriptors\"","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg19_weights =\"../input/full-keras-pretrained-no-top/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_weights =\"../input/full-keras-pretrained-no-top//inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nvgg16_weights =\"../input/full-keras-pretrained-no-top/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet201_weights =\"../input/full-keras-pretrained-no-top/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet121_weights =\"../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nresenet50_weights =\"../input/full-keras-pretrained-no-top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_resnet_v2_weights =\"../input/full-keras-pretrained-no-top/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nnasnet_weights =\"../input/full-keras-pretrained-no-top/nasnet_large_no_top.h5\"\nnasnet_mobile_weights =\"../input/full-keras-pretrained-no-top/nasnet_mobile_no_top.h5\"\nmobilenet_weights =\"../input/full-keras-pretrained-no-top/mobilenet_1_0_224_tf_no_top.h5\"\nxception_weights = \"../input/full-keras-pretrained-no-top/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.applications import DenseNet201\nfrom keras.applications import DenseNet121\nfrom keras.applications import ResNet50\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications import NASNetLarge, NASNetMobile\nfrom keras.applications import MobileNet","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor = Input(shape = input_shape)  \n\nbase_model1=InceptionResNetV2(weights=inception_resnet_v2_weights, include_top=False, pooling = \"avg\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base_model1.summary()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i, layer in enumerate(model.layers):\n#     print(i, layer.name)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_final_model = base_model1","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train)\nnp.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\nnp.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\nnp.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import tensorflow as tf\n# from keras.backend.tensorflow_backend import get_session\n# from keras.backend.tensorflow_backend import clear_session\n# from keras.backend.tensorflow_backend import set_session\n\n# def reset_keras_tf_session():\n#     \"\"\"\n#     this function clears the gpu memory and set the \n#     tf session to not use the whole gpu\n#     \"\"\"\n#     sess = get_session()\n#     clear_session()\n#     sess.close()\n#     sess = get_session()\n\n# #     config = tf.ConfigProto()\n# #     config.gpu_options.allow_growth = True\n# #     set_session(tf.Session(config=config))\n\n# reset_keras_tf_session()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\nvalidation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\ntest_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n\ntrain_labels = train_generator.classes\ntrain_labels = to_categorical(train_labels, num_classes=num_classes)\n\nvalidation_labels = validation_generator.classes\nvalidation_labels = to_categorical(validation_labels, num_classes=num_classes)\n\ntest_labels = test_generator.classes\ntest_labels = to_categorical(test_labels, num_classes=num_classes)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = {\n         'lr': hp.choice('lr',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n#          'dropout': hp.choice('dropout', [0.4, 0.5, 0.6, 0.7]),\n#          'batch_size': hp.choice('batch_size', [64]),\n#          'epochs': hp.choice('epochs', [15, 20, 25, 30, 50]),\n#          'optimizer': hp.choice('optimizer',['sgd','adam','rmsprop']),\n#          'optimizer': hp.choice('optimizer',['rmsprop']),\n#          'optimizer': hp.choice('optimizer',['adam']),\n         'beta_1':hp.choice('beta_1',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8, 0.9]),\n         'beta_2':hp.choice('beta_2',[0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.999,0.99,0.995]),\n#          'momentum':hp.choice('momentum',[0.3,0.5,0.7,0.9,1]),\n#          'amsgrad':hp.choice('amsgrad',[False,True]),\n#          'nesterov':hp.choice('nesterov',[False,True]),\n#          'rho':hp.choice('rho',[0.4,0.5,0.6,0.7,0.8,0.9,1]),\n        'hidden1':hp.choice('hidden1',[4096,2048,1024,512,256]),\n#         'hidden2':hp.choice('hidden2',[2048,1024,512,256,128,]),\n#         'hidden3':hp.choice('hidden3',[1024,512,256,128,64,32]),    \n        'bias_reg': hp.choice('bias_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n        'act_reg': hp.choice('act_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n        'ker_reg': hp.choice('ker_reg',[0.1, 0.01, 0.001, 0.0001, 0.00001,0.000001, 0.0000001]),\n        'activation_function':hp.choice('activation_function',[\"relu\",\"elu\",\"selu\",\"softplus\",\"tanh\",])\n        }","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndef f_nn(params):   \n    print ('Parameters testing: ', params)\n#     dropout_rate = 0.5\n    adam_opt=Adam(lr=params[\"lr\"], beta_1=params[\"beta_1\"], beta_2=params['beta_2'])\n#     sgd=SGD(lr=params[\"lr\"], momentum=params['momentum'], decay=0.0, nesterov=params['nesterov'])\n#     rmsprop=RMSprop(lr=params[\"lr\"], rho=params['rho'], epsilon=None, decay=0.0)\n\n    model = Sequential()\n    # model.add(Flatten(input_shape=train_data.shape[1:]))\n    model.add(Dense(params[\"hidden1\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n#     model.add(Dropout(0.25))\n\n#     model.add(Dense(params[\"hidden2\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n    \n    \n#     model.add(Dense(params[\"hidden3\"], activation=params[\"activation_function\"], kernel_regularizer=l2(params[\"ker_reg\"]), bias_regularizer=l2(params[\"bias_reg\"]), activity_regularizer=l1(params[\"act_reg\"])))\n    \n    \n    model.add(Dropout(0.5))\n    \n\n    model.add(Dense(num_classes, activation=\"softmax\"))\n\n    model.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    history = model.fit(train_data, train_labels,\n                        epochs=epochs,\n                        batch_size=batch_size,\n                        validation_data=(validation_data, validation_labels),\n                        verbose= 2,\n                        callbacks=get_callbacks(params))\n\n    (eval_loss, eval_accuracy) = model.evaluate(validation_data, validation_labels, batch_size= batch_size, verbose=1)\n\n    print(\"Validation Accuracy: {:.4f}%\".format(eval_accuracy * 100))\n    print(\"Validation Loss: {}\".format(eval_loss))\n    \n    filename = test_generator.filenames\n    truth = test_generator.classes\n    label = test_generator.class_indices\n    indexlabel = dict((value, key) for key, value in label.items())\n\n    preds = model.predict(test_data)\n\n    predictions = [i.argmax() for i in preds]\n    y_true = [i.argmax() for i in test_labels]\n#     cm = confusion_matrix(y_pred=predictions, y_true=y_true)\n\n    print('Test Accuracy: {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))\n\n    print(\"*_*\" * 50)\n#     best_epoch = np.argmax(history.history['val_acc'])\n#     best_val_acc = np.max(history.history['val_acc'])\n#     print('Epoch {} - val acc: {}'.format(best_epoch, best_val_acc))\n    sys.stdout.flush() \n    \n    return {'loss': eval_loss, 'status': STATUS_OK, 'model': model}","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_callbacks(params):\n    callbacks =[EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n    return callbacks","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nbest = fmin(f_nn, space, algo=tpe.suggest, max_evals = 300, trials=trials)\nprint(best)","execution_count":26,"outputs":[{"output_type":"stream","text":"Parameters testing:                                    \n{'act_reg': 1e-05, 'activation_function': 'relu', 'beta_1': 0.4, 'beta_2': 0.4, 'bias_reg': 0.0001, 'hidden1': 256, 'ker_reg': 0.01, 'lr': 0.001}\nEpoch 1/1000                                           \n252/252 - 1s - loss: 1.6157 - accuracy: 0.6427 - val_loss: 1.1776 - val_accuracy: 0.5835\n\nEpoch 2/1000                                           \n252/252 - 1s - loss: 0.7500 - accuracy: 0.8155 - val_loss: 0.7455 - val_accuracy: 0.8141\n\nEpoch 3/1000                                           \n252/252 - 1s - loss: 0.5920 - accuracy: 0.8642 - val_loss: 0.5524 - val_accuracy: 0.9100\n\nEpoch 4/1000                                           \n252/252 - 1s - loss: 0.5292 - accuracy: 0.8773 - val_loss: 0.7448 - val_accuracy: 0.7450\n\nEpoch 5/1000                                           \n252/252 - 1s - loss: 0.4838 - accuracy: 0.8877 - val_loss: 0.4049 - val_accuracy: 0.9543\n\nEpoch 6/1000                                           \n252/252 - 1s - loss: 0.4476 - accuracy: 0.8970 - val_loss: 0.4617 - val_accuracy: 0.8986\n\nEpoch 7/1000                                           \n252/252 - 1s - loss: 0.4363 - accuracy: 0.8973 - val_loss: 0.3466 - val_accuracy: 0.9518\n\nEpoch 8/1000                                           \n252/252 - 1s - loss: 0.4143 - accuracy: 0.9051 - val_loss: 0.4976 - val_accuracy: 0.8603\n\nEpoch 9/1000                                           \n252/252 - 1s - loss: 0.4208 - accuracy: 0.8969 - val_loss: 0.5577 - val_accuracy: 0.8171\n\nEpoch 10/1000                                          \n252/252 - 1s - loss: 0.4080 - accuracy: 0.9008 - val_loss: 0.7235 - val_accuracy: 0.7406\n\nEpoch 11/1000                                          \n252/252 - 1s - loss: 0.3986 - accuracy: 0.9010 - val_loss: 0.4217 - val_accuracy: 0.8832\n\nEpoch 12/1000                                          \n252/252 - 1s - loss: 0.3935 - accuracy: 0.8968 - val_loss: 0.3997 - val_accuracy: 0.9155\n\nEpoch 13/1000                                          \n252/252 - 1s - loss: 0.3958 - accuracy: 0.8986 - val_loss: 0.3058 - val_accuracy: 0.9563\n\nEpoch 14/1000                                          \n252/252 - 1s - loss: 0.3981 - accuracy: 0.8941 - val_loss: 0.2962 - val_accuracy: 0.9563\n\nEpoch 15/1000                                          \n252/252 - 1s - loss: 0.3934 - accuracy: 0.8943 - val_loss: 0.4071 - val_accuracy: 0.8897\n\nEpoch 16/1000                                          \n252/252 - 1s - loss: 0.3936 - accuracy: 0.8955 - val_loss: 0.2849 - val_accuracy: 0.9682\n\nEpoch 17/1000                                          \n252/252 - 1s - loss: 0.3889 - accuracy: 0.9014 - val_loss: 0.3776 - val_accuracy: 0.9085\n\nEpoch 18/1000                                          \n252/252 - 1s - loss: 0.3933 - accuracy: 0.8931 - val_loss: 0.4282 - val_accuracy: 0.8792\n\nEpoch 19/1000                                          \n252/252 - 1s - loss: 0.3907 - accuracy: 0.8977 - val_loss: 0.3787 - val_accuracy: 0.8956\n\nEpoch 20/1000                                          \n252/252 - 1s - loss: 0.4029 - accuracy: 0.8877 - val_loss: 0.2531 - val_accuracy: 0.9702\n\nEpoch 21/1000                                          \n252/252 - 1s - loss: 0.3973 - accuracy: 0.8887 - val_loss: 0.2599 - val_accuracy: 0.9657\n\nEpoch 22/1000                                          \n252/252 - 1s - loss: 0.3962 - accuracy: 0.8856 - val_loss: 0.2702 - val_accuracy: 0.9553\n\nEpoch 23/1000                                          \n252/252 - 1s - loss: 0.3969 - accuracy: 0.8877 - val_loss: 0.2520 - val_accuracy: 0.9583\n\nEpoch 24/1000                                          \n252/252 - 1s - loss: 0.3917 - accuracy: 0.8876 - val_loss: 0.2579 - val_accuracy: 0.9687\n\nEpoch 25/1000                                          \n252/252 - 1s - loss: 0.4010 - accuracy: 0.8876 - val_loss: 0.5116 - val_accuracy: 0.8380\n\nEpoch 26/1000                                          \n252/252 - 1s - loss: 0.3882 - accuracy: 0.8936 - val_loss: 0.2553 - val_accuracy: 0.9573\n\nEpoch 27/1000                                          \n252/252 - 1s - loss: 0.3970 - accuracy: 0.8882 - val_loss: 0.2403 - val_accuracy: 0.9612\n\nEpoch 28/1000                                          \n252/252 - 1s - loss: 0.3869 - accuracy: 0.8907 - val_loss: 0.2443 - val_accuracy: 0.9617\n\nEpoch 29/1000                                          \n252/252 - 1s - loss: 0.4076 - accuracy: 0.8840 - val_loss: 0.3278 - val_accuracy: 0.9205\n\nEpoch 30/1000                                          \n252/252 - 1s - loss: 0.4085 - accuracy: 0.8788 - val_loss: 0.2699 - val_accuracy: 0.9538\n\nEpoch 31/1000                                          \n252/252 - 1s - loss: 0.3994 - accuracy: 0.8860 - val_loss: 0.2344 - val_accuracy: 0.9617\n\nEpoch 32/1000                                          \n252/252 - 1s - loss: 0.4297 - accuracy: 0.8628 - val_loss: 0.3827 - val_accuracy: 0.8921\n\nEpoch 33/1000                                          \n252/252 - 1s - loss: 0.4696 - accuracy: 0.8408 - val_loss: 0.4307 - val_accuracy: 0.8713\n\nEpoch 34/1000                                          \n252/252 - 1s - loss: 0.4588 - accuracy: 0.8416 - val_loss: 0.2497 - val_accuracy: 0.9607\n\nEpoch 35/1000                                          \n252/252 - 1s - loss: 0.4467 - accuracy: 0.8488 - val_loss: 0.4338 - val_accuracy: 0.8882\n\nEpoch 36/1000                                          \n252/252 - 1s - loss: 0.4542 - accuracy: 0.8488 - val_loss: 0.3130 - val_accuracy: 0.9344\n\nEpoch 37/1000                                          \n252/252 - 1s - loss: 0.4531 - accuracy: 0.8437 - val_loss: 0.3227 - val_accuracy: 0.9145\n\nEpoch 38/1000                                          \n252/252 - 1s - loss: 0.4904 - accuracy: 0.8294 - val_loss: 0.2552 - val_accuracy: 0.9587\n\nEpoch 39/1000                                          \n252/252 - 1s - loss: 0.4748 - accuracy: 0.8318 - val_loss: 0.6426 - val_accuracy: 0.7768\n\nEpoch 40/1000                                          \n252/252 - 1s - loss: 0.4962 - accuracy: 0.8303 - val_loss: 0.4394 - val_accuracy: 0.8579\n\nEpoch 41/1000                                          \n252/252 - 1s - loss: 0.4710 - accuracy: 0.8418 - val_loss: 0.2269 - val_accuracy: 0.9682\n\nEpoch 42/1000                                          \n252/252 - 1s - loss: 0.4749 - accuracy: 0.8429 - val_loss: 0.2900 - val_accuracy: 0.9339\n\nEpoch 43/1000                                          \n252/252 - 1s - loss: 0.4775 - accuracy: 0.8417 - val_loss: 0.3252 - val_accuracy: 0.9155\n\nEpoch 44/1000                                          \n252/252 - 1s - loss: 0.4628 - accuracy: 0.8468 - val_loss: 0.2627 - val_accuracy: 0.9488\n\nEpoch 45/1000                                          \n252/252 - 1s - loss: 0.4725 - accuracy: 0.8460 - val_loss: 0.2804 - val_accuracy: 0.9433\n\nEpoch 46/1000                                          \n252/252 - 1s - loss: 0.4726 - accuracy: 0.8447 - val_loss: 0.2862 - val_accuracy: 0.9428\n\nEpoch 47/1000                                          \n252/252 - 1s - loss: 0.4595 - accuracy: 0.8505 - val_loss: 0.3388 - val_accuracy: 0.9180\n\nEpoch 48/1000                                          \n252/252 - 1s - loss: 0.4601 - accuracy: 0.8506 - val_loss: 0.3211 - val_accuracy: 0.9190\n\nEpoch 49/1000                                          \n252/252 - 1s - loss: 0.4672 - accuracy: 0.8408 - val_loss: 0.3971 - val_accuracy: 0.8847\n\nEpoch 50/1000                                          \n252/252 - 1s - loss: 0.4724 - accuracy: 0.8422 - val_loss: 0.3040 - val_accuracy: 0.9240\n\nEpoch 51/1000                                          \n252/252 - 1s - loss: 0.4672 - accuracy: 0.8437 - val_loss: 0.3222 - val_accuracy: 0.9269\n\nEpoch 52/1000                                          \n252/252 - 1s - loss: 0.4620 - accuracy: 0.8453 - val_loss: 0.2367 - val_accuracy: 0.9647\n\nEpoch 53/1000                                          \n252/252 - 1s - loss: 0.4622 - accuracy: 0.8448 - val_loss: 0.3295 - val_accuracy: 0.9195\n\nEpoch 54/1000                                          \n252/252 - 1s - loss: 0.4641 - accuracy: 0.8408 - val_loss: 0.2546 - val_accuracy: 0.9543\n\nEpoch 55/1000                                          \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.4467 - accuracy: 0.8521 - val_loss: 0.2282 - val_accuracy: 0.9677\n\nEpoch 56/1000                                          \n252/252 - 1s - loss: 0.4391 - accuracy: 0.8526 - val_loss: 0.2207 - val_accuracy: 0.9662\n\nEpoch 57/1000                                          \n252/252 - 1s - loss: 0.4555 - accuracy: 0.8504 - val_loss: 0.4102 - val_accuracy: 0.8767\n\nEpoch 58/1000                                          \n252/252 - 1s - loss: 0.4634 - accuracy: 0.8436 - val_loss: 0.3007 - val_accuracy: 0.9230\n\nEpoch 59/1000                                          \n252/252 - 1s - loss: 0.4713 - accuracy: 0.8407 - val_loss: 0.2288 - val_accuracy: 0.9597\n\nEpoch 60/1000                                          \n252/252 - 1s - loss: 0.4548 - accuracy: 0.8489 - val_loss: 0.2332 - val_accuracy: 0.9642\n\nEpoch 61/1000                                          \n252/252 - 1s - loss: 0.4865 - accuracy: 0.8267 - val_loss: 0.2714 - val_accuracy: 0.9329\n\nEpoch 62/1000                                          \n252/252 - 1s - loss: 0.4780 - accuracy: 0.8291 - val_loss: 0.4792 - val_accuracy: 0.8285\n\nEpoch 63/1000                                          \n252/252 - 1s - loss: 0.4805 - accuracy: 0.8304 - val_loss: 0.2609 - val_accuracy: 0.9498\n\nEpoch 64/1000                                          \n252/252 - 1s - loss: 0.4693 - accuracy: 0.8379 - val_loss: 0.2032 - val_accuracy: 0.9732\n\nEpoch 65/1000                                          \n252/252 - 1s - loss: 0.4683 - accuracy: 0.8398 - val_loss: 0.2471 - val_accuracy: 0.9508\n\nEpoch 66/1000                                          \n252/252 - 1s - loss: 0.4860 - accuracy: 0.8323 - val_loss: 0.1962 - val_accuracy: 0.9747\n\nEpoch 67/1000                                          \n252/252 - 1s - loss: 0.4971 - accuracy: 0.8288 - val_loss: 0.3674 - val_accuracy: 0.9026\n\nEpoch 68/1000                                          \n252/252 - 1s - loss: 0.4878 - accuracy: 0.8314 - val_loss: 0.2424 - val_accuracy: 0.9583\n\nEpoch 69/1000                                          \n252/252 - 1s - loss: 0.5045 - accuracy: 0.8220 - val_loss: 0.2445 - val_accuracy: 0.9553\n\nEpoch 70/1000                                          \n252/252 - 1s - loss: 0.4943 - accuracy: 0.8287 - val_loss: 0.2824 - val_accuracy: 0.9324\n\nEpoch 71/1000                                          \n252/252 - 1s - loss: 0.4897 - accuracy: 0.8350 - val_loss: 0.2841 - val_accuracy: 0.9364\n\nEpoch 72/1000                                          \n252/252 - 1s - loss: 0.5057 - accuracy: 0.8266 - val_loss: 0.4412 - val_accuracy: 0.8733\n\nEpoch 73/1000                                          \n252/252 - 1s - loss: 0.5030 - accuracy: 0.8336 - val_loss: 0.3028 - val_accuracy: 0.9732\n\nEpoch 74/1000                                          \n252/252 - 1s - loss: 0.4929 - accuracy: 0.8323 - val_loss: 0.2456 - val_accuracy: 0.9538\n\nEpoch 75/1000                                          \n252/252 - 1s - loss: 0.5145 - accuracy: 0.8155 - val_loss: 0.2335 - val_accuracy: 0.9612\n\nEpoch 76/1000                                          \n252/252 - 1s - loss: 0.5162 - accuracy: 0.8102 - val_loss: 0.3771 - val_accuracy: 0.9021\n\nEpoch 77/1000                                          \n252/252 - 1s - loss: 0.5170 - accuracy: 0.8160 - val_loss: 0.3085 - val_accuracy: 0.9543\n\nEpoch 78/1000                                          \n252/252 - 1s - loss: 0.5260 - accuracy: 0.8126 - val_loss: 0.2130 - val_accuracy: 0.9672\n\nEpoch 79/1000                                          \n252/252 - 1s - loss: 0.5230 - accuracy: 0.8147 - val_loss: 0.2610 - val_accuracy: 0.9384\n\nEpoch 80/1000                                          \n252/252 - 1s - loss: 0.5281 - accuracy: 0.8103 - val_loss: 0.3750 - val_accuracy: 0.8917\n\nEpoch 81/1000                                          \n252/252 - 1s - loss: 0.5166 - accuracy: 0.8173 - val_loss: 0.2357 - val_accuracy: 0.9602\n\nEpoch 82/1000                                          \n252/252 - 1s - loss: 0.5324 - accuracy: 0.8190 - val_loss: 0.2896 - val_accuracy: 0.9587\n\nEpoch 83/1000                                          \n252/252 - 1s - loss: 0.5377 - accuracy: 0.8076 - val_loss: 0.2088 - val_accuracy: 0.9737\n\nEpoch 84/1000                                          \n252/252 - 1s - loss: 0.5196 - accuracy: 0.8166 - val_loss: 0.3426 - val_accuracy: 0.9483\n\nEpoch 85/1000                                          \n252/252 - 1s - loss: 0.5145 - accuracy: 0.8219 - val_loss: 0.5630 - val_accuracy: 0.8246\n\nEpoch 86/1000                                          \n252/252 - 1s - loss: 0.5480 - accuracy: 0.8128 - val_loss: 0.2874 - val_accuracy: 0.9548\n\nEpoch 87/1000                                          \n252/252 - 1s - loss: 0.5364 - accuracy: 0.8204 - val_loss: 0.7230 - val_accuracy: 0.7734\n\nEpoch 88/1000                                          \n252/252 - 1s - loss: 0.5604 - accuracy: 0.8118 - val_loss: 0.6593 - val_accuracy: 0.8196\n\nEpoch 89/1000                                          \n252/252 - 1s - loss: 0.5416 - accuracy: 0.8195 - val_loss: 0.2829 - val_accuracy: 0.9513\n\nEpoch 90/1000                                          \n252/252 - 1s - loss: 0.5494 - accuracy: 0.8173 - val_loss: 0.5137 - val_accuracy: 0.8246\n\nEpoch 91/1000                                          \n252/252 - 1s - loss: 0.5595 - accuracy: 0.8144 - val_loss: 0.2883 - val_accuracy: 0.9508\n\nEpoch 92/1000                                          \n252/252 - 1s - loss: 0.5285 - accuracy: 0.8274 - val_loss: 0.2837 - val_accuracy: 0.9503\n\nEpoch 93/1000                                          \n252/252 - 1s - loss: 0.5062 - accuracy: 0.8300 - val_loss: 0.2501 - val_accuracy: 0.9707\n\nEpoch 94/1000                                          \n252/252 - 1s - loss: 0.5103 - accuracy: 0.8345 - val_loss: 0.2857 - val_accuracy: 0.9458\n\nEpoch 95/1000                                          \n252/252 - 1s - loss: 0.5070 - accuracy: 0.8330 - val_loss: 0.2782 - val_accuracy: 0.9622\n\nEpoch 96/1000                                          \n252/252 - 1s - loss: 0.5151 - accuracy: 0.8348 - val_loss: 0.4355 - val_accuracy: 0.8683\n\nEpoch 97/1000                                          \n252/252 - 1s - loss: 0.5160 - accuracy: 0.8351 - val_loss: 0.3313 - val_accuracy: 0.9553\n\nEpoch 98/1000                                          \n252/252 - 1s - loss: 0.5704 - accuracy: 0.8070 - val_loss: 0.3152 - val_accuracy: 0.9423\n\nEpoch 99/1000                                          \n252/252 - 1s - loss: 0.5648 - accuracy: 0.8166 - val_loss: 0.4678 - val_accuracy: 0.8623\n\nEpoch 100/1000                                         \n252/252 - 1s - loss: 0.5553 - accuracy: 0.8186 - val_loss: 0.4433 - val_accuracy: 0.8887\n\nEpoch 101/1000                                         \n252/252 - 1s - loss: 0.5531 - accuracy: 0.8226 - val_loss: 0.5639 - val_accuracy: 0.8489\n\nEpoch 102/1000                                         \n252/252 - 1s - loss: 0.5345 - accuracy: 0.8298 - val_loss: 0.4150 - val_accuracy: 0.8797\n\nEpoch 103/1000                                         \n252/252 - 1s - loss: 0.5313 - accuracy: 0.8340 - val_loss: 0.3011 - val_accuracy: 0.9384\n\nEpoch 104/1000                                         \n252/252 - 1s - loss: 0.5438 - accuracy: 0.8310 - val_loss: 0.3965 - val_accuracy: 0.8842\n\nEpoch 105/1000                                         \n252/252 - 1s - loss: 0.5322 - accuracy: 0.8299 - val_loss: 0.4108 - val_accuracy: 0.9105\n\nEpoch 106/1000                                         \n252/252 - 1s - loss: 0.5474 - accuracy: 0.8277 - val_loss: 0.5363 - val_accuracy: 0.7868\n\nEpoch 107/1000                                         \n252/252 - 1s - loss: 0.5148 - accuracy: 0.8417 - val_loss: 0.2899 - val_accuracy: 0.9463\n\nEpoch 108/1000                                         \n252/252 - 1s - loss: 0.5116 - accuracy: 0.8386 - val_loss: 0.3410 - val_accuracy: 0.9120\n\nEpoch 109/1000                                         \n252/252 - 1s - loss: 0.5436 - accuracy: 0.8282 - val_loss: 0.2648 - val_accuracy: 0.9657\n\nEpoch 110/1000                                         \n252/252 - 1s - loss: 0.5393 - accuracy: 0.8374 - val_loss: 0.3656 - val_accuracy: 0.9066\n\nEpoch 111/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.5255 - accuracy: 0.8460 - val_loss: 0.2792 - val_accuracy: 0.9488\n\nEpoch 112/1000                                         \n252/252 - 1s - loss: 0.5526 - accuracy: 0.8331 - val_loss: 0.2829 - val_accuracy: 0.9458\n\nEpoch 113/1000                                         \n252/252 - 1s - loss: 0.5191 - accuracy: 0.8389 - val_loss: 0.4234 - val_accuracy: 0.8633\n\nEpoch 114/1000                                         \n252/252 - 1s - loss: 0.5157 - accuracy: 0.8432 - val_loss: 0.3383 - val_accuracy: 0.9145\n\nEpoch 115/1000                                         \n252/252 - 1s - loss: 0.5259 - accuracy: 0.8405 - val_loss: 0.4551 - val_accuracy: 0.8653\n\nEpoch 116/1000                                         \n252/252 - 1s - loss: 0.5358 - accuracy: 0.8325 - val_loss: 0.3644 - val_accuracy: 0.9433\n\nEpoch 117/1000                                         \n252/252 - 1s - loss: 0.5561 - accuracy: 0.8096 - val_loss: 0.3568 - val_accuracy: 0.8951\n\nEpoch 118/1000                                         \n252/252 - 1s - loss: 0.5861 - accuracy: 0.8117 - val_loss: 0.2865 - val_accuracy: 0.9627\n\nEpoch 119/1000                                         \n252/252 - 1s - loss: 0.5393 - accuracy: 0.8225 - val_loss: 0.3029 - val_accuracy: 0.9533\n\nEpoch 120/1000                                         \n252/252 - 1s - loss: 0.5617 - accuracy: 0.8168 - val_loss: 0.2839 - val_accuracy: 0.9697\n\nEpoch 121/1000                                         \n252/252 - 1s - loss: 0.5340 - accuracy: 0.8227 - val_loss: 0.2823 - val_accuracy: 0.9503\n\nEpoch 122/1000                                         \n252/252 - 1s - loss: 0.5454 - accuracy: 0.8194 - val_loss: 0.3559 - val_accuracy: 0.9160\n\nEpoch 123/1000                                         \n252/252 - 1s - loss: 0.5676 - accuracy: 0.8104 - val_loss: 0.4759 - val_accuracy: 0.8653\n\nEpoch 124/1000                                         \n252/252 - 1s - loss: 0.5864 - accuracy: 0.7994 - val_loss: 0.2464 - val_accuracy: 0.9722\n\nEpoch 125/1000                                         \n252/252 - 1s - loss: 0.5684 - accuracy: 0.8046 - val_loss: 0.3831 - val_accuracy: 0.8951\n\nEpoch 126/1000                                         \n252/252 - 1s - loss: 0.5586 - accuracy: 0.8107 - val_loss: 0.2665 - val_accuracy: 0.9766\n\nEpoch 127/1000                                         \n252/252 - 1s - loss: 0.5551 - accuracy: 0.8127 - val_loss: 0.2394 - val_accuracy: 0.9707\n\nEpoch 128/1000                                         \n252/252 - 1s - loss: 0.5401 - accuracy: 0.8153 - val_loss: 0.3216 - val_accuracy: 0.9409\n\nEpoch 129/1000                                         \n252/252 - 1s - loss: 0.5550 - accuracy: 0.8107 - val_loss: 0.4003 - val_accuracy: 0.8872\n\nEpoch 130/1000                                         \n252/252 - 1s - loss: 0.5338 - accuracy: 0.8118 - val_loss: 0.3654 - val_accuracy: 0.9334\n\nEpoch 131/1000                                         \n252/252 - 1s - loss: 0.5320 - accuracy: 0.8134 - val_loss: 0.2945 - val_accuracy: 0.9374\n\nEpoch 132/1000                                         \n252/252 - 1s - loss: 0.5526 - accuracy: 0.8101 - val_loss: 0.5921 - val_accuracy: 0.7535\n\nEpoch 133/1000                                         \n252/252 - 1s - loss: 0.5615 - accuracy: 0.8019 - val_loss: 0.2185 - val_accuracy: 0.9737\n\nEpoch 134/1000                                         \n252/252 - 1s - loss: 0.5256 - accuracy: 0.8225 - val_loss: 0.2697 - val_accuracy: 0.9747\n\nEpoch 135/1000                                         \n252/252 - 1s - loss: 0.5594 - accuracy: 0.8109 - val_loss: 0.3008 - val_accuracy: 0.9334\n\nEpoch 136/1000                                         \n252/252 - 1s - loss: 0.5069 - accuracy: 0.8207 - val_loss: 0.3459 - val_accuracy: 0.9473\n\nEpoch 137/1000                                         \n252/252 - 1s - loss: 0.5529 - accuracy: 0.8068 - val_loss: 0.2698 - val_accuracy: 0.9508\n\nEpoch 138/1000                                         \n252/252 - 1s - loss: 0.5819 - accuracy: 0.7778 - val_loss: 0.3876 - val_accuracy: 0.8837\n\nEpoch 139/1000                                         \n252/252 - 1s - loss: 0.6055 - accuracy: 0.7834 - val_loss: 0.6366 - val_accuracy: 0.6576\n\nEpoch 140/1000                                         \n252/252 - 1s - loss: 0.5810 - accuracy: 0.7877 - val_loss: 0.4046 - val_accuracy: 0.9105\n\nEpoch 141/1000                                         \n252/252 - 1s - loss: 0.6205 - accuracy: 0.7808 - val_loss: 0.3465 - val_accuracy: 0.9483\n\nEpoch 142/1000                                         \n252/252 - 1s - loss: 0.6038 - accuracy: 0.7886 - val_loss: 0.2707 - val_accuracy: 0.9682\n\nEpoch 143/1000                                         \n252/252 - 1s - loss: 0.6263 - accuracy: 0.7782 - val_loss: 0.4397 - val_accuracy: 0.8897\n\nEpoch 144/1000                                         \n252/252 - 1s - loss: 0.6008 - accuracy: 0.7918 - val_loss: 0.4121 - val_accuracy: 0.9001\n\nEpoch 145/1000                                         \n252/252 - 1s - loss: 0.6193 - accuracy: 0.7778 - val_loss: 0.3246 - val_accuracy: 0.9125\n\nEpoch 146/1000                                         \n252/252 - 1s - loss: 0.6157 - accuracy: 0.7911 - val_loss: 0.6880 - val_accuracy: 0.7729\n\nEpoch 147/1000                                         \n252/252 - 1s - loss: 0.5868 - accuracy: 0.7913 - val_loss: 0.2521 - val_accuracy: 0.9647\n\nEpoch 148/1000                                         \n252/252 - 1s - loss: 0.6033 - accuracy: 0.7788 - val_loss: 0.2967 - val_accuracy: 0.9717\n\nEpoch 149/1000                                         \n252/252 - 1s - loss: 0.6387 - accuracy: 0.7629 - val_loss: 0.3662 - val_accuracy: 0.9617\n\nEpoch 150/1000                                         \n252/252 - 1s - loss: 0.6069 - accuracy: 0.7735 - val_loss: 0.5000 - val_accuracy: 0.8892\n\nEpoch 151/1000                                         \n252/252 - 1s - loss: 0.6018 - accuracy: 0.7791 - val_loss: 0.2711 - val_accuracy: 0.9617\n\nEpoch 152/1000                                         \n252/252 - 1s - loss: 0.6216 - accuracy: 0.7835 - val_loss: 0.5862 - val_accuracy: 0.8116\n\nEpoch 153/1000                                         \n252/252 - 1s - loss: 0.5870 - accuracy: 0.7835 - val_loss: 0.2686 - val_accuracy: 0.9607\n\nEpoch 154/1000                                         \n252/252 - 1s - loss: 0.5899 - accuracy: 0.7907 - val_loss: 0.2936 - val_accuracy: 0.9528\n\nEpoch 155/1000                                         \n252/252 - 1s - loss: 0.5920 - accuracy: 0.7921 - val_loss: 0.5879 - val_accuracy: 0.8236\n\nEpoch 156/1000                                         \n252/252 - 1s - loss: 0.5691 - accuracy: 0.7958 - val_loss: 0.3076 - val_accuracy: 0.9612\n\nEpoch 157/1000                                         \n252/252 - 1s - loss: 0.5583 - accuracy: 0.7998 - val_loss: 0.2863 - val_accuracy: 0.9548\n\nEpoch 158/1000                                         \n252/252 - 1s - loss: 0.5890 - accuracy: 0.7905 - val_loss: 0.2841 - val_accuracy: 0.9473\n\nEpoch 159/1000                                         \n252/252 - 1s - loss: 0.5626 - accuracy: 0.8006 - val_loss: 0.5611 - val_accuracy: 0.8295\n\nEpoch 160/1000                                         \n252/252 - 1s - loss: 0.5758 - accuracy: 0.7916 - val_loss: 0.7613 - val_accuracy: 0.7321\n\nEpoch 161/1000                                         \n252/252 - 1s - loss: 0.5795 - accuracy: 0.7889 - val_loss: 0.4333 - val_accuracy: 0.9026\n\nEpoch 162/1000                                         \n252/252 - 1s - loss: 0.5841 - accuracy: 0.7818 - val_loss: 0.5914 - val_accuracy: 0.8300\n\nEpoch 163/1000                                         \n252/252 - 1s - loss: 0.6224 - accuracy: 0.7742 - val_loss: 0.6685 - val_accuracy: 0.7967\n\nEpoch 164/1000                                         \n252/252 - 1s - loss: 0.5845 - accuracy: 0.7846 - val_loss: 0.4084 - val_accuracy: 0.8608\n\nEpoch 165/1000                                         \n252/252 - 1s - loss: 0.6539 - accuracy: 0.7567 - val_loss: 0.3515 - val_accuracy: 0.9145\n\nEpoch 166/1000                                         \n252/252 - 1s - loss: 0.6629 - accuracy: 0.7523 - val_loss: 0.4529 - val_accuracy: 0.8593\n\nEpoch 167/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6419 - accuracy: 0.7637 - val_loss: 0.4450 - val_accuracy: 0.9021\n\nEpoch 168/1000                                         \n252/252 - 1s - loss: 0.6274 - accuracy: 0.7736 - val_loss: 0.4920 - val_accuracy: 0.7808\n\nEpoch 169/1000                                         \n252/252 - 1s - loss: 0.6259 - accuracy: 0.7767 - val_loss: 0.4305 - val_accuracy: 0.8628\n\nEpoch 170/1000                                         \n252/252 - 1s - loss: 0.6228 - accuracy: 0.7762 - val_loss: 0.2999 - val_accuracy: 0.9493\n\nEpoch 171/1000                                         \n252/252 - 1s - loss: 0.6190 - accuracy: 0.7797 - val_loss: 0.3257 - val_accuracy: 0.9215\n\nEpoch 172/1000                                         \n252/252 - 1s - loss: 0.6262 - accuracy: 0.7822 - val_loss: 0.4547 - val_accuracy: 0.8469\n\nEpoch 173/1000                                         \n252/252 - 1s - loss: 0.6329 - accuracy: 0.7836 - val_loss: 0.4908 - val_accuracy: 0.8320\n\nEpoch 174/1000                                         \n252/252 - 1s - loss: 0.6426 - accuracy: 0.7870 - val_loss: 0.2917 - val_accuracy: 0.9473\n\nEpoch 175/1000                                         \n252/252 - 1s - loss: 0.6016 - accuracy: 0.8037 - val_loss: 0.3518 - val_accuracy: 0.8951\n\nEpoch 176/1000                                         \n252/252 - 1s - loss: 0.5874 - accuracy: 0.8019 - val_loss: 0.3140 - val_accuracy: 0.9493\n\nEpoch 177/1000                                         \n252/252 - 1s - loss: 0.5745 - accuracy: 0.8122 - val_loss: 0.3434 - val_accuracy: 0.9240\n\nEpoch 178/1000                                         \n252/252 - 1s - loss: 0.5618 - accuracy: 0.8204 - val_loss: 0.2964 - val_accuracy: 0.9538\n\nEpoch 179/1000                                         \n252/252 - 1s - loss: 0.5773 - accuracy: 0.8175 - val_loss: 0.2894 - val_accuracy: 0.9573\n\nEpoch 180/1000                                         \n252/252 - 1s - loss: 0.5953 - accuracy: 0.8028 - val_loss: 0.4026 - val_accuracy: 0.9250\n\nEpoch 181/1000                                         \n252/252 - 1s - loss: 0.6174 - accuracy: 0.7911 - val_loss: 0.3653 - val_accuracy: 0.9245\n\nEpoch 182/1000                                         \n252/252 - 1s - loss: 0.6190 - accuracy: 0.7969 - val_loss: 0.4502 - val_accuracy: 0.8941\n\nEpoch 183/1000                                         \n252/252 - 1s - loss: 0.5839 - accuracy: 0.8076 - val_loss: 0.3752 - val_accuracy: 0.9334\n\nEpoch 184/1000                                         \n252/252 - 1s - loss: 0.5785 - accuracy: 0.8117 - val_loss: 0.4948 - val_accuracy: 0.8429\n\nEpoch 185/1000                                         \n252/252 - 1s - loss: 0.5679 - accuracy: 0.8131 - val_loss: 0.3694 - val_accuracy: 0.9105\n\nEpoch 186/1000                                         \n252/252 - 1s - loss: 0.5725 - accuracy: 0.8127 - val_loss: 0.4051 - val_accuracy: 0.8882\n\nEpoch 187/1000                                         \n252/252 - 1s - loss: 0.5763 - accuracy: 0.8102 - val_loss: 0.2214 - val_accuracy: 0.9766\n\nEpoch 188/1000                                         \n252/252 - 1s - loss: 0.5663 - accuracy: 0.8175 - val_loss: 0.2983 - val_accuracy: 0.9538\n\nEpoch 189/1000                                         \n252/252 - 1s - loss: 0.5808 - accuracy: 0.8090 - val_loss: 0.3390 - val_accuracy: 0.9359\n\nEpoch 190/1000                                         \n252/252 - 1s - loss: 0.5746 - accuracy: 0.7962 - val_loss: 0.4381 - val_accuracy: 0.8405\n\nEpoch 191/1000                                         \n252/252 - 1s - loss: 0.5861 - accuracy: 0.7957 - val_loss: 0.3564 - val_accuracy: 0.8986\n\nEpoch 192/1000                                         \n252/252 - 1s - loss: 0.5869 - accuracy: 0.7975 - val_loss: 0.3311 - val_accuracy: 0.9384\n\nEpoch 193/1000                                         \n252/252 - 1s - loss: 0.5849 - accuracy: 0.7985 - val_loss: 0.4237 - val_accuracy: 0.8907\n\nEpoch 194/1000                                         \n252/252 - 1s - loss: 0.5759 - accuracy: 0.7972 - val_loss: 0.3231 - val_accuracy: 0.9240\n\nEpoch 195/1000                                         \n252/252 - 1s - loss: 0.5817 - accuracy: 0.7901 - val_loss: 0.3205 - val_accuracy: 0.9453\n\nEpoch 196/1000                                         \n252/252 - 1s - loss: 0.5731 - accuracy: 0.8000 - val_loss: 0.3009 - val_accuracy: 0.9294\n\nEpoch 197/1000                                         \n252/252 - 1s - loss: 0.5796 - accuracy: 0.7978 - val_loss: 0.5116 - val_accuracy: 0.8539\n\nEpoch 198/1000                                         \n252/252 - 1s - loss: 0.5506 - accuracy: 0.8066 - val_loss: 0.3659 - val_accuracy: 0.8912\n\nEpoch 199/1000                                         \n252/252 - 1s - loss: 0.5513 - accuracy: 0.8093 - val_loss: 0.6360 - val_accuracy: 0.7739\n\nEpoch 200/1000                                         \n252/252 - 1s - loss: 0.5693 - accuracy: 0.8015 - val_loss: 0.3556 - val_accuracy: 0.9110\n\nEpoch 201/1000                                         \n252/252 - 1s - loss: 0.5640 - accuracy: 0.8059 - val_loss: 0.4553 - val_accuracy: 0.8559\n\nEpoch 202/1000                                         \n252/252 - 1s - loss: 0.5972 - accuracy: 0.7915 - val_loss: 0.4145 - val_accuracy: 0.8921\n\nEpoch 203/1000                                         \n252/252 - 1s - loss: 0.6111 - accuracy: 0.7865 - val_loss: 0.3603 - val_accuracy: 0.9076\n\nEpoch 204/1000                                         \n252/252 - 1s - loss: 0.6151 - accuracy: 0.7922 - val_loss: 0.4108 - val_accuracy: 0.8772\n\nEpoch 205/1000                                         \n252/252 - 1s - loss: 0.6144 - accuracy: 0.7887 - val_loss: 0.3287 - val_accuracy: 0.9235\n\nEpoch 206/1000                                         \n252/252 - 1s - loss: 0.6113 - accuracy: 0.7881 - val_loss: 0.5837 - val_accuracy: 0.8171\n\nEpoch 207/1000                                         \n252/252 - 1s - loss: 0.5606 - accuracy: 0.7978 - val_loss: 0.4930 - val_accuracy: 0.8116\n\nEpoch 208/1000                                         \n252/252 - 1s - loss: 0.5728 - accuracy: 0.8055 - val_loss: 0.3582 - val_accuracy: 0.9001\n\nEpoch 209/1000                                         \n252/252 - 1s - loss: 0.6035 - accuracy: 0.7872 - val_loss: 0.2754 - val_accuracy: 0.9488\n\nEpoch 210/1000                                         \n252/252 - 1s - loss: 0.5569 - accuracy: 0.8015 - val_loss: 0.2568 - val_accuracy: 0.9558\n\nEpoch 211/1000                                         \n252/252 - 1s - loss: 0.5964 - accuracy: 0.7951 - val_loss: 0.2769 - val_accuracy: 0.9423\n\nEpoch 212/1000                                         \n252/252 - 1s - loss: 0.5759 - accuracy: 0.8011 - val_loss: 0.3146 - val_accuracy: 0.9165\n\nEpoch 213/1000                                         \n252/252 - 1s - loss: 0.5745 - accuracy: 0.8020 - val_loss: 0.3041 - val_accuracy: 0.9568\n\nEpoch 214/1000                                         \n252/252 - 1s - loss: 0.6275 - accuracy: 0.7820 - val_loss: 0.5217 - val_accuracy: 0.8037\n\nEpoch 215/1000                                         \n252/252 - 1s - loss: 0.6351 - accuracy: 0.7823 - val_loss: 0.3794 - val_accuracy: 0.8946\n\nEpoch 216/1000                                         \n252/252 - 1s - loss: 0.6447 - accuracy: 0.7765 - val_loss: 0.3047 - val_accuracy: 0.9493\n\nEpoch 217/1000                                         \n252/252 - 1s - loss: 0.6228 - accuracy: 0.7788 - val_loss: 0.2698 - val_accuracy: 0.9727\n\nEpoch 218/1000                                         \n252/252 - 1s - loss: 0.6047 - accuracy: 0.7921 - val_loss: 0.3649 - val_accuracy: 0.8986\n\nEpoch 219/1000                                         \n252/252 - 1s - loss: 0.6070 - accuracy: 0.7936 - val_loss: 0.3491 - val_accuracy: 0.9215\n\nEpoch 220/1000                                         \n252/252 - 1s - loss: 0.5991 - accuracy: 0.8019 - val_loss: 0.3504 - val_accuracy: 0.9314\n\nEpoch 221/1000                                         \n252/252 - 1s - loss: 0.6102 - accuracy: 0.8057 - val_loss: 0.3263 - val_accuracy: 0.9617\n\nEpoch 222/1000                                         \n252/252 - 1s - loss: 0.5857 - accuracy: 0.8054 - val_loss: 0.4426 - val_accuracy: 0.8400\n\nEpoch 223/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6207 - accuracy: 0.7863 - val_loss: 0.4087 - val_accuracy: 0.9001\n\nEpoch 224/1000                                         \n252/252 - 1s - loss: 0.6124 - accuracy: 0.7910 - val_loss: 0.3703 - val_accuracy: 0.9543\n\nEpoch 225/1000                                         \n252/252 - 1s - loss: 0.6317 - accuracy: 0.7848 - val_loss: 0.4011 - val_accuracy: 0.9289\n\nEpoch 226/1000                                         \n252/252 - 1s - loss: 0.6531 - accuracy: 0.7791 - val_loss: 0.4806 - val_accuracy: 0.9359\n\nEpoch 227/1000                                         \n252/252 - 1s - loss: 0.6316 - accuracy: 0.7920 - val_loss: 0.3159 - val_accuracy: 0.9662\n\nEpoch 228/1000                                         \n252/252 - 1s - loss: 0.6093 - accuracy: 0.7969 - val_loss: 0.2816 - val_accuracy: 0.9528\n\nEpoch 229/1000                                         \n252/252 - 1s - loss: 0.6089 - accuracy: 0.7985 - val_loss: 0.5370 - val_accuracy: 0.8275\n\nEpoch 230/1000                                         \n252/252 - 1s - loss: 0.6231 - accuracy: 0.7952 - val_loss: 0.4571 - val_accuracy: 0.8439\n\nEpoch 231/1000                                         \n252/252 - 1s - loss: 0.6357 - accuracy: 0.7930 - val_loss: 0.4876 - val_accuracy: 0.8176\n\nEpoch 232/1000                                         \n252/252 - 1s - loss: 0.5983 - accuracy: 0.8026 - val_loss: 0.5039 - val_accuracy: 0.8226\n\nEpoch 233/1000                                         \n252/252 - 1s - loss: 0.5983 - accuracy: 0.8076 - val_loss: 0.2898 - val_accuracy: 0.9717\n\nEpoch 234/1000                                         \n252/252 - 1s - loss: 0.5837 - accuracy: 0.8087 - val_loss: 0.3997 - val_accuracy: 0.8822\n\nEpoch 235/1000                                         \n252/252 - 1s - loss: 0.6133 - accuracy: 0.8004 - val_loss: 0.5690 - val_accuracy: 0.8121\n\nEpoch 236/1000                                         \n252/252 - 1s - loss: 0.6026 - accuracy: 0.8044 - val_loss: 0.4798 - val_accuracy: 0.8544\n\nEpoch 237/1000                                         \n252/252 - 1s - loss: 0.5978 - accuracy: 0.8096 - val_loss: 0.3473 - val_accuracy: 0.9095\n\nEpoch 238/1000                                         \n252/252 - 1s - loss: 0.5790 - accuracy: 0.8135 - val_loss: 0.2370 - val_accuracy: 0.9722\n\nEpoch 239/1000                                         \n252/252 - 1s - loss: 0.6001 - accuracy: 0.8086 - val_loss: 0.2707 - val_accuracy: 0.9508\n\nEpoch 240/1000                                         \n252/252 - 1s - loss: 0.5756 - accuracy: 0.8158 - val_loss: 0.4162 - val_accuracy: 0.8738\n\nEpoch 241/1000                                         \n252/252 - 1s - loss: 0.5976 - accuracy: 0.8065 - val_loss: 0.4208 - val_accuracy: 0.8748\n\nEpoch 242/1000                                         \n252/252 - 1s - loss: 0.6328 - accuracy: 0.7884 - val_loss: 0.2922 - val_accuracy: 0.9359\n\nEpoch 243/1000                                         \n252/252 - 1s - loss: 0.6619 - accuracy: 0.7884 - val_loss: 0.3028 - val_accuracy: 0.9443\n\nEpoch 244/1000                                         \n252/252 - 1s - loss: 0.6315 - accuracy: 0.7920 - val_loss: 0.3418 - val_accuracy: 0.9304\n\nEpoch 245/1000                                         \n252/252 - 1s - loss: 0.6011 - accuracy: 0.8068 - val_loss: 0.2255 - val_accuracy: 0.9727\n\nEpoch 246/1000                                         \n252/252 - 1s - loss: 0.6105 - accuracy: 0.8071 - val_loss: 0.2330 - val_accuracy: 0.9707\n\nEpoch 247/1000                                         \n252/252 - 1s - loss: 0.5656 - accuracy: 0.8183 - val_loss: 0.2492 - val_accuracy: 0.9632\n\nEpoch 248/1000                                         \n252/252 - 1s - loss: 0.5722 - accuracy: 0.8220 - val_loss: 0.8209 - val_accuracy: 0.7092\n\nEpoch 249/1000                                         \n252/252 - 1s - loss: 0.5775 - accuracy: 0.8135 - val_loss: 0.4150 - val_accuracy: 0.9145\n\nEpoch 250/1000                                         \n252/252 - 1s - loss: 0.5847 - accuracy: 0.8205 - val_loss: 0.5535 - val_accuracy: 0.8231\n\nEpoch 251/1000                                         \n252/252 - 1s - loss: 0.5552 - accuracy: 0.8245 - val_loss: 0.3624 - val_accuracy: 0.9031\n\nEpoch 252/1000                                         \n252/252 - 1s - loss: 0.5428 - accuracy: 0.8339 - val_loss: 0.4005 - val_accuracy: 0.8817\n\nEpoch 253/1000                                         \n252/252 - 1s - loss: 0.5443 - accuracy: 0.8360 - val_loss: 0.3224 - val_accuracy: 0.9409\n\nEpoch 254/1000                                         \n252/252 - 1s - loss: 0.5782 - accuracy: 0.8145 - val_loss: 0.2513 - val_accuracy: 0.9468\n\nEpoch 255/1000                                         \n252/252 - 1s - loss: 0.5359 - accuracy: 0.8309 - val_loss: 0.2861 - val_accuracy: 0.9394\n\nEpoch 256/1000                                         \n252/252 - 1s - loss: 0.5536 - accuracy: 0.8324 - val_loss: 0.2923 - val_accuracy: 0.9597\n\nEpoch 257/1000                                         \n252/252 - 1s - loss: 0.5478 - accuracy: 0.8191 - val_loss: 0.2652 - val_accuracy: 0.9578\n\nEpoch 258/1000                                         \n252/252 - 1s - loss: 0.5529 - accuracy: 0.8169 - val_loss: 0.5577 - val_accuracy: 0.7908\n\nEpoch 259/1000                                         \n252/252 - 1s - loss: 0.5533 - accuracy: 0.8248 - val_loss: 0.3875 - val_accuracy: 0.8807\n\nEpoch 260/1000                                         \n252/252 - 1s - loss: 0.5627 - accuracy: 0.8138 - val_loss: 0.2999 - val_accuracy: 0.9404\n\nEpoch 261/1000                                         \n252/252 - 1s - loss: 0.5623 - accuracy: 0.8086 - val_loss: 0.3377 - val_accuracy: 0.9130\n\nEpoch 262/1000                                         \n252/252 - 1s - loss: 0.5730 - accuracy: 0.8044 - val_loss: 0.4225 - val_accuracy: 0.9344\n\nEpoch 263/1000                                         \n252/252 - 1s - loss: 0.5999 - accuracy: 0.7954 - val_loss: 1.2670 - val_accuracy: 0.6322\n\nEpoch 264/1000                                         \n252/252 - 1s - loss: 0.6000 - accuracy: 0.7982 - val_loss: 0.2591 - val_accuracy: 0.9642\n\nEpoch 265/1000                                         \n252/252 - 1s - loss: 0.6245 - accuracy: 0.7969 - val_loss: 0.9267 - val_accuracy: 0.6735\n\nEpoch 266/1000                                         \n252/252 - 1s - loss: 0.5952 - accuracy: 0.8066 - val_loss: 0.3052 - val_accuracy: 0.9672\n\nEpoch 267/1000                                         \n252/252 - 1s - loss: 0.5820 - accuracy: 0.8106 - val_loss: 0.3730 - val_accuracy: 0.8941\n\nEpoch 268/1000                                         \n252/252 - 1s - loss: 0.5861 - accuracy: 0.8114 - val_loss: 0.3765 - val_accuracy: 0.9210\n\nEpoch 269/1000                                         \n252/252 - 1s - loss: 0.6099 - accuracy: 0.8028 - val_loss: 0.5731 - val_accuracy: 0.7729\n\nEpoch 270/1000                                         \n252/252 - 1s - loss: 0.5813 - accuracy: 0.8090 - val_loss: 0.3280 - val_accuracy: 0.9334\n\nEpoch 271/1000                                         \n252/252 - 1s - loss: 0.6113 - accuracy: 0.7997 - val_loss: 0.4775 - val_accuracy: 0.8439\n\nEpoch 272/1000                                         \n252/252 - 1s - loss: 0.5930 - accuracy: 0.8005 - val_loss: 0.2976 - val_accuracy: 0.9498\n\nEpoch 273/1000                                         \n252/252 - 1s - loss: 0.6331 - accuracy: 0.7712 - val_loss: 0.3218 - val_accuracy: 0.9607\n\nEpoch 274/1000                                         \n252/252 - 1s - loss: 0.6477 - accuracy: 0.7681 - val_loss: 0.4786 - val_accuracy: 0.8579\n\nEpoch 275/1000                                         \n252/252 - 1s - loss: 0.7077 - accuracy: 0.7523 - val_loss: 0.3289 - val_accuracy: 0.9458\n\nEpoch 276/1000                                         \n252/252 - 1s - loss: 0.6932 - accuracy: 0.7586 - val_loss: 0.4574 - val_accuracy: 0.8852\n\nEpoch 277/1000                                         \n252/252 - 1s - loss: 0.7089 - accuracy: 0.7654 - val_loss: 0.4594 - val_accuracy: 0.8867\n\nEpoch 278/1000                                         \n252/252 - 1s - loss: 0.6767 - accuracy: 0.7825 - val_loss: 0.3421 - val_accuracy: 0.9528\n\nEpoch 279/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6564 - accuracy: 0.7966 - val_loss: 0.4348 - val_accuracy: 0.9071\n\nEpoch 280/1000                                         \n252/252 - 1s - loss: 0.5887 - accuracy: 0.8233 - val_loss: 0.3087 - val_accuracy: 0.9652\n\nEpoch 281/1000                                         \n252/252 - 1s - loss: 0.6461 - accuracy: 0.8068 - val_loss: 0.5049 - val_accuracy: 0.8842\n\nEpoch 282/1000                                         \n252/252 - 1s - loss: 0.6075 - accuracy: 0.8227 - val_loss: 1.0453 - val_accuracy: 0.7157\n\nEpoch 283/1000                                         \n252/252 - 1s - loss: 0.6346 - accuracy: 0.8119 - val_loss: 0.4170 - val_accuracy: 0.9195\n\nEpoch 284/1000                                         \n252/252 - 1s - loss: 0.6369 - accuracy: 0.8039 - val_loss: 0.4029 - val_accuracy: 0.8986\n\nEpoch 285/1000                                         \n252/252 - 1s - loss: 0.6233 - accuracy: 0.8163 - val_loss: 0.3949 - val_accuracy: 0.8912\n\nEpoch 286/1000                                         \n252/252 - 1s - loss: 0.6176 - accuracy: 0.8211 - val_loss: 0.3352 - val_accuracy: 0.9389\n\nEpoch 287/1000                                         \n252/252 - 1s - loss: 0.6003 - accuracy: 0.8252 - val_loss: 0.2765 - val_accuracy: 0.9677\n\nEpoch 288/1000                                         \n252/252 - 1s - loss: 0.5971 - accuracy: 0.8302 - val_loss: 0.3667 - val_accuracy: 0.9314\n\nEpoch 289/1000                                         \n252/252 - 1s - loss: 0.5971 - accuracy: 0.8322 - val_loss: 0.4305 - val_accuracy: 0.8787\n\nEpoch 290/1000                                         \n252/252 - 1s - loss: 0.5637 - accuracy: 0.8395 - val_loss: 0.3796 - val_accuracy: 0.8917\n\nEpoch 291/1000                                         \n252/252 - 1s - loss: 0.6096 - accuracy: 0.8186 - val_loss: 0.4011 - val_accuracy: 0.9235\n\nEpoch 292/1000                                         \n252/252 - 1s - loss: 0.6339 - accuracy: 0.8080 - val_loss: 0.3566 - val_accuracy: 0.9543\n\nEpoch 293/1000                                         \n252/252 - 1s - loss: 0.6343 - accuracy: 0.8123 - val_loss: 0.4322 - val_accuracy: 0.8996\n\nEpoch 294/1000                                         \n252/252 - 1s - loss: 0.6372 - accuracy: 0.8060 - val_loss: 0.4675 - val_accuracy: 0.9125\n\nEpoch 295/1000                                         \n252/252 - 1s - loss: 0.6285 - accuracy: 0.8158 - val_loss: 0.5009 - val_accuracy: 0.8663\n\nEpoch 296/1000                                         \n252/252 - 1s - loss: 0.6497 - accuracy: 0.8140 - val_loss: 0.3177 - val_accuracy: 0.9563\n\nEpoch 297/1000                                         \n252/252 - 1s - loss: 0.6071 - accuracy: 0.8302 - val_loss: 0.4635 - val_accuracy: 0.8603\n\nEpoch 298/1000                                         \n252/252 - 1s - loss: 0.6237 - accuracy: 0.8216 - val_loss: 0.3279 - val_accuracy: 0.9573\n\nEpoch 299/1000                                         \n252/252 - 1s - loss: 0.6281 - accuracy: 0.8200 - val_loss: 0.3516 - val_accuracy: 0.9394\n\nEpoch 300/1000                                         \n252/252 - 1s - loss: 0.5963 - accuracy: 0.8310 - val_loss: 0.5091 - val_accuracy: 0.9041\n\nEpoch 301/1000                                         \n252/252 - 1s - loss: 0.6270 - accuracy: 0.8247 - val_loss: 0.3244 - val_accuracy: 0.9652\n\nEpoch 302/1000                                         \n252/252 - 1s - loss: 0.5824 - accuracy: 0.8334 - val_loss: 0.2921 - val_accuracy: 0.9707\n\nEpoch 303/1000                                         \n252/252 - 1s - loss: 0.5893 - accuracy: 0.8279 - val_loss: 0.4440 - val_accuracy: 0.8852\n\nEpoch 304/1000                                         \n252/252 - 1s - loss: 0.6352 - accuracy: 0.8086 - val_loss: 0.2957 - val_accuracy: 0.9508\n\nEpoch 305/1000                                         \n252/252 - 1s - loss: 0.6049 - accuracy: 0.8188 - val_loss: 0.3692 - val_accuracy: 0.9200\n\nEpoch 306/1000                                         \n252/252 - 1s - loss: 0.6273 - accuracy: 0.8193 - val_loss: 0.3326 - val_accuracy: 0.9528\n\nEpoch 307/1000                                         \n252/252 - 1s - loss: 0.6192 - accuracy: 0.8081 - val_loss: 0.4596 - val_accuracy: 0.8484\n\nEpoch 308/1000                                         \n252/252 - 1s - loss: 0.6027 - accuracy: 0.8202 - val_loss: 0.3798 - val_accuracy: 0.9105\n\nEpoch 309/1000                                         \n252/252 - 1s - loss: 0.6260 - accuracy: 0.8138 - val_loss: 0.3720 - val_accuracy: 0.9240\n\nEpoch 310/1000                                         \n252/252 - 1s - loss: 0.6365 - accuracy: 0.8142 - val_loss: 0.2889 - val_accuracy: 0.9573\n\nEpoch 311/1000                                         \n252/252 - 1s - loss: 0.6200 - accuracy: 0.8222 - val_loss: 0.3149 - val_accuracy: 0.9453\n\nEpoch 312/1000                                         \n252/252 - 1s - loss: 0.6057 - accuracy: 0.8255 - val_loss: 0.3038 - val_accuracy: 0.9438\n\nEpoch 313/1000                                         \n252/252 - 1s - loss: 0.6048 - accuracy: 0.8248 - val_loss: 0.3241 - val_accuracy: 0.9309\n\nEpoch 314/1000                                         \n252/252 - 1s - loss: 0.5757 - accuracy: 0.8299 - val_loss: 0.8464 - val_accuracy: 0.6610\n\nEpoch 315/1000                                         \n252/252 - 1s - loss: 0.6199 - accuracy: 0.8281 - val_loss: 0.3267 - val_accuracy: 0.9319\n\nEpoch 316/1000                                         \n252/252 - 1s - loss: 0.5994 - accuracy: 0.8252 - val_loss: 0.3012 - val_accuracy: 0.9687\n\nEpoch 317/1000                                         \n252/252 - 1s - loss: 0.5483 - accuracy: 0.8358 - val_loss: 0.2705 - val_accuracy: 0.9622\n\nEpoch 318/1000                                         \n252/252 - 1s - loss: 0.6023 - accuracy: 0.8317 - val_loss: 0.3493 - val_accuracy: 0.9279\n\nEpoch 319/1000                                         \n252/252 - 1s - loss: 0.5594 - accuracy: 0.8381 - val_loss: 0.3319 - val_accuracy: 0.9423\n\nEpoch 320/1000                                         \n252/252 - 1s - loss: 0.5686 - accuracy: 0.8330 - val_loss: 0.3068 - val_accuracy: 0.9518\n\nEpoch 321/1000                                         \n252/252 - 1s - loss: 0.5955 - accuracy: 0.8205 - val_loss: 0.3681 - val_accuracy: 0.9031\n\nEpoch 322/1000                                         \n252/252 - 1s - loss: 0.6268 - accuracy: 0.8054 - val_loss: 0.5086 - val_accuracy: 0.8380\n\nEpoch 323/1000                                         \n252/252 - 1s - loss: 0.6439 - accuracy: 0.8044 - val_loss: 0.3884 - val_accuracy: 0.9125\n\nEpoch 324/1000                                         \n252/252 - 1s - loss: 0.5846 - accuracy: 0.8157 - val_loss: 0.3298 - val_accuracy: 0.9409\n\nEpoch 325/1000                                         \n252/252 - 1s - loss: 0.6197 - accuracy: 0.8173 - val_loss: 1.8167 - val_accuracy: 0.5701\n\nEpoch 326/1000                                         \n252/252 - 1s - loss: 0.6164 - accuracy: 0.8197 - val_loss: 0.4357 - val_accuracy: 0.9269\n\nEpoch 327/1000                                         \n252/252 - 1s - loss: 0.5982 - accuracy: 0.8179 - val_loss: 0.2696 - val_accuracy: 0.9538\n\nEpoch 328/1000                                         \n252/252 - 1s - loss: 0.5897 - accuracy: 0.8224 - val_loss: 0.3510 - val_accuracy: 0.9085\n\nEpoch 329/1000                                         \n252/252 - 1s - loss: 0.5970 - accuracy: 0.8186 - val_loss: 0.2671 - val_accuracy: 0.9503\n\nEpoch 330/1000                                         \n252/252 - 1s - loss: 0.6141 - accuracy: 0.8098 - val_loss: 0.2699 - val_accuracy: 0.9458\n\nEpoch 331/1000                                         \n252/252 - 1s - loss: 0.5996 - accuracy: 0.8118 - val_loss: 0.4196 - val_accuracy: 0.8792\n\nEpoch 332/1000                                         \n252/252 - 1s - loss: 0.5920 - accuracy: 0.8183 - val_loss: 0.2690 - val_accuracy: 0.9587\n\nEpoch 333/1000                                         \n252/252 - 1s - loss: 0.5964 - accuracy: 0.8201 - val_loss: 0.3034 - val_accuracy: 0.9737\n\nEpoch 334/1000                                         \n252/252 - 1s - loss: 0.5930 - accuracy: 0.8185 - val_loss: 0.3455 - val_accuracy: 0.9180\n\nEpoch 335/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6124 - accuracy: 0.8165 - val_loss: 0.2600 - val_accuracy: 0.9652\n\nEpoch 336/1000                                         \n252/252 - 1s - loss: 0.5880 - accuracy: 0.8219 - val_loss: 0.4801 - val_accuracy: 0.8489\n\nEpoch 337/1000                                         \n252/252 - 1s - loss: 0.5915 - accuracy: 0.8224 - val_loss: 0.2822 - val_accuracy: 0.9473\n\nEpoch 338/1000                                         \n252/252 - 1s - loss: 0.6192 - accuracy: 0.8193 - val_loss: 0.3124 - val_accuracy: 0.9389\n\nEpoch 339/1000                                         \n252/252 - 1s - loss: 0.6186 - accuracy: 0.8178 - val_loss: 0.3193 - val_accuracy: 0.9205\n\nEpoch 340/1000                                         \n252/252 - 1s - loss: 0.5939 - accuracy: 0.8173 - val_loss: 0.3551 - val_accuracy: 0.9225\n\nEpoch 341/1000                                         \n252/252 - 1s - loss: 0.6230 - accuracy: 0.8054 - val_loss: 0.2222 - val_accuracy: 0.9712\n\nEpoch 342/1000                                         \n252/252 - 1s - loss: 0.5724 - accuracy: 0.8197 - val_loss: 0.3154 - val_accuracy: 0.9145\n\nEpoch 343/1000                                         \n252/252 - 1s - loss: 0.6180 - accuracy: 0.8133 - val_loss: 0.2300 - val_accuracy: 0.9573\n\nEpoch 344/1000                                         \n252/252 - 1s - loss: 0.5987 - accuracy: 0.8128 - val_loss: 0.2545 - val_accuracy: 0.9548\n\nEpoch 345/1000                                         \n252/252 - 1s - loss: 0.6270 - accuracy: 0.7977 - val_loss: 1.5578 - val_accuracy: 0.6267\n\nEpoch 346/1000                                         \n252/252 - 1s - loss: 0.6494 - accuracy: 0.8009 - val_loss: 0.2847 - val_accuracy: 0.9652\n\nEpoch 347/1000                                         \n252/252 - 1s - loss: 0.6622 - accuracy: 0.7932 - val_loss: 0.2189 - val_accuracy: 0.9682\n\nEpoch 348/1000                                         \n252/252 - 1s - loss: 0.6295 - accuracy: 0.7941 - val_loss: 0.2418 - val_accuracy: 0.9667\n\nEpoch 349/1000                                         \n252/252 - 1s - loss: 0.6496 - accuracy: 0.7887 - val_loss: 0.5431 - val_accuracy: 0.8231\n\nEpoch 350/1000                                         \n252/252 - 1s - loss: 0.6322 - accuracy: 0.7963 - val_loss: 0.2618 - val_accuracy: 0.9602\n\nEpoch 351/1000                                         \n252/252 - 1s - loss: 0.6554 - accuracy: 0.7944 - val_loss: 0.3832 - val_accuracy: 0.8961\n\nEpoch 352/1000                                         \n252/252 - 1s - loss: 0.6745 - accuracy: 0.7916 - val_loss: 0.3377 - val_accuracy: 0.9309\n\nEpoch 353/1000                                         \n252/252 - 1s - loss: 0.6149 - accuracy: 0.7994 - val_loss: 0.5836 - val_accuracy: 0.8042\n\nEpoch 354/1000                                         \n252/252 - 1s - loss: 0.6221 - accuracy: 0.7963 - val_loss: 0.2263 - val_accuracy: 0.9712\n\nEpoch 355/1000                                         \n252/252 - 1s - loss: 0.6840 - accuracy: 0.7845 - val_loss: 0.3885 - val_accuracy: 0.8971\n\nEpoch 356/1000                                         \n252/252 - 1s - loss: 0.7089 - accuracy: 0.7645 - val_loss: 0.2826 - val_accuracy: 0.9677\n\nEpoch 357/1000                                         \n252/252 - 1s - loss: 0.7155 - accuracy: 0.7685 - val_loss: 0.8974 - val_accuracy: 0.7182\n\nEpoch 358/1000                                         \n252/252 - 1s - loss: 0.6924 - accuracy: 0.7786 - val_loss: 0.3217 - val_accuracy: 0.9254\n\nEpoch 359/1000                                         \n252/252 - 1s - loss: 0.6882 - accuracy: 0.7726 - val_loss: 0.4105 - val_accuracy: 0.8802\n\nEpoch 360/1000                                         \n252/252 - 1s - loss: 0.6518 - accuracy: 0.7884 - val_loss: 0.4096 - val_accuracy: 0.8623\n\nEpoch 361/1000                                         \n252/252 - 1s - loss: 0.6422 - accuracy: 0.7908 - val_loss: 0.4014 - val_accuracy: 0.8832\n\nEpoch 362/1000                                         \n252/252 - 1s - loss: 0.6393 - accuracy: 0.7925 - val_loss: 0.4197 - val_accuracy: 0.9021\n\nEpoch 363/1000                                         \n252/252 - 1s - loss: 0.6379 - accuracy: 0.7932 - val_loss: 0.3485 - val_accuracy: 0.9160\n\nEpoch 364/1000                                         \n252/252 - 1s - loss: 0.6907 - accuracy: 0.7871 - val_loss: 0.2634 - val_accuracy: 0.9612\n\nEpoch 365/1000                                         \n252/252 - 1s - loss: 0.6417 - accuracy: 0.7913 - val_loss: 1.0973 - val_accuracy: 0.6610\n\nEpoch 366/1000                                         \n252/252 - 1s - loss: 0.6454 - accuracy: 0.7947 - val_loss: 0.5140 - val_accuracy: 0.8211\n\nEpoch 367/1000                                         \n252/252 - 1s - loss: 0.6504 - accuracy: 0.7964 - val_loss: 0.2866 - val_accuracy: 0.9652\n\nEpoch 368/1000                                         \n252/252 - 1s - loss: 0.6959 - accuracy: 0.7804 - val_loss: 0.3460 - val_accuracy: 0.9115\n\nEpoch 369/1000                                         \n252/252 - 1s - loss: 0.6279 - accuracy: 0.7941 - val_loss: 0.2601 - val_accuracy: 0.9503\n\nEpoch 370/1000                                         \n252/252 - 1s - loss: 0.6246 - accuracy: 0.7984 - val_loss: 0.5477 - val_accuracy: 0.7903\n\nEpoch 371/1000                                         \n252/252 - 1s - loss: 0.7059 - accuracy: 0.7596 - val_loss: 0.2489 - val_accuracy: 0.9563\n\nEpoch 372/1000                                         \n252/252 - 1s - loss: 0.6937 - accuracy: 0.7603 - val_loss: 0.2934 - val_accuracy: 0.9508\n\nEpoch 373/1000                                         \n252/252 - 1s - loss: 0.7113 - accuracy: 0.7667 - val_loss: 0.2468 - val_accuracy: 0.9682\n\nEpoch 374/1000                                         \n252/252 - 1s - loss: 0.6709 - accuracy: 0.7725 - val_loss: 0.3902 - val_accuracy: 0.8981\n\nEpoch 375/1000                                         \n252/252 - 1s - loss: 0.6951 - accuracy: 0.7676 - val_loss: 0.4040 - val_accuracy: 0.8718\n\nEpoch 376/1000                                         \n252/252 - 1s - loss: 0.6962 - accuracy: 0.7761 - val_loss: 0.4053 - val_accuracy: 0.8668\n\nEpoch 377/1000                                         \n252/252 - 1s - loss: 0.6800 - accuracy: 0.7892 - val_loss: 0.3818 - val_accuracy: 0.8991\n\nEpoch 378/1000                                         \n252/252 - 1s - loss: 0.6879 - accuracy: 0.7850 - val_loss: 0.2575 - val_accuracy: 0.9781\n\nEpoch 379/1000                                         \n252/252 - 1s - loss: 0.6903 - accuracy: 0.7874 - val_loss: 0.4040 - val_accuracy: 0.8971\n\nEpoch 380/1000                                         \n252/252 - 1s - loss: 0.6299 - accuracy: 0.8072 - val_loss: 0.5447 - val_accuracy: 0.8419\n\nEpoch 381/1000                                         \n252/252 - 1s - loss: 0.6175 - accuracy: 0.8121 - val_loss: 0.6369 - val_accuracy: 0.8111\n\nEpoch 382/1000                                         \n252/252 - 1s - loss: 0.6019 - accuracy: 0.8180 - val_loss: 0.2718 - val_accuracy: 0.9652\n\nEpoch 383/1000                                         \n252/252 - 1s - loss: 0.6177 - accuracy: 0.8183 - val_loss: 0.6918 - val_accuracy: 0.7525\n\nEpoch 384/1000                                         \n252/252 - 1s - loss: 0.5861 - accuracy: 0.8236 - val_loss: 0.4425 - val_accuracy: 0.8613\n\nEpoch 385/1000                                         \n252/252 - 1s - loss: 0.5991 - accuracy: 0.8215 - val_loss: 0.5739 - val_accuracy: 0.8325\n\nEpoch 386/1000                                         \n252/252 - 1s - loss: 0.6073 - accuracy: 0.8127 - val_loss: 0.3941 - val_accuracy: 0.8941\n\nEpoch 387/1000                                         \n252/252 - 1s - loss: 0.5924 - accuracy: 0.8231 - val_loss: 0.8846 - val_accuracy: 0.7346\n\nEpoch 388/1000                                         \n252/252 - 1s - loss: 0.6201 - accuracy: 0.8194 - val_loss: 0.3895 - val_accuracy: 0.9245\n\nEpoch 389/1000                                         \n252/252 - 1s - loss: 0.6390 - accuracy: 0.8031 - val_loss: 0.6130 - val_accuracy: 0.7957\n\nEpoch 390/1000                                         \n252/252 - 1s - loss: 0.6669 - accuracy: 0.7951 - val_loss: 0.4302 - val_accuracy: 0.8807\n\nEpoch 391/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6613 - accuracy: 0.8024 - val_loss: 0.3074 - val_accuracy: 0.9697\n\nEpoch 392/1000                                         \n252/252 - 1s - loss: 0.6991 - accuracy: 0.7753 - val_loss: 0.3393 - val_accuracy: 0.9409\n\nEpoch 393/1000                                         \n252/252 - 1s - loss: 0.7026 - accuracy: 0.7727 - val_loss: 0.3778 - val_accuracy: 0.9384\n\nEpoch 394/1000                                         \n252/252 - 1s - loss: 0.7209 - accuracy: 0.7774 - val_loss: 0.4358 - val_accuracy: 0.8912\n\nEpoch 395/1000                                         \n252/252 - 1s - loss: 0.7069 - accuracy: 0.7812 - val_loss: 0.4392 - val_accuracy: 0.9085\n\nEpoch 396/1000                                         \n252/252 - 1s - loss: 0.6938 - accuracy: 0.7810 - val_loss: 0.3594 - val_accuracy: 0.9404\n\nEpoch 397/1000                                         \n252/252 - 1s - loss: 0.6590 - accuracy: 0.7954 - val_loss: 0.3188 - val_accuracy: 0.9319\n\nEpoch 398/1000                                         \n252/252 - 1s - loss: 0.6770 - accuracy: 0.7861 - val_loss: 0.4122 - val_accuracy: 0.9230\n\nEpoch 399/1000                                         \n252/252 - 1s - loss: 0.6748 - accuracy: 0.7953 - val_loss: 0.3973 - val_accuracy: 0.9294\n\nEpoch 400/1000                                         \n252/252 - 1s - loss: 0.6538 - accuracy: 0.7954 - val_loss: 0.2786 - val_accuracy: 0.9781\n\nEpoch 401/1000                                         \n252/252 - 1s - loss: 0.6626 - accuracy: 0.7972 - val_loss: 1.1501 - val_accuracy: 0.5860\n\nEpoch 402/1000                                         \n252/252 - 1s - loss: 0.6652 - accuracy: 0.8004 - val_loss: 0.3932 - val_accuracy: 0.8807\n\nEpoch 403/1000                                         \n252/252 - 1s - loss: 0.6449 - accuracy: 0.8005 - val_loss: 0.3698 - val_accuracy: 0.9006\n\nEpoch 404/1000                                         \n252/252 - 1s - loss: 0.6499 - accuracy: 0.8041 - val_loss: 0.2777 - val_accuracy: 0.9503\n\nEpoch 405/1000                                         \n252/252 - 1s - loss: 0.6432 - accuracy: 0.8015 - val_loss: 0.3310 - val_accuracy: 0.9438\n\nEpoch 406/1000                                         \n252/252 - 1s - loss: 0.6401 - accuracy: 0.8015 - val_loss: 0.3811 - val_accuracy: 0.9175\n\nEpoch 407/1000                                         \n252/252 - 1s - loss: 0.6475 - accuracy: 0.8098 - val_loss: 0.3890 - val_accuracy: 0.9130\n\nEpoch 408/1000                                         \n252/252 - 1s - loss: 0.6505 - accuracy: 0.8042 - val_loss: 0.4415 - val_accuracy: 0.8718\n\nEpoch 409/1000                                         \n252/252 - 1s - loss: 0.6742 - accuracy: 0.7957 - val_loss: 0.7523 - val_accuracy: 0.7629\n\nEpoch 410/1000                                         \n252/252 - 1s - loss: 0.6151 - accuracy: 0.8119 - val_loss: 0.4326 - val_accuracy: 0.8658\n\nEpoch 411/1000                                         \n252/252 - 1s - loss: 0.6498 - accuracy: 0.7999 - val_loss: 0.3111 - val_accuracy: 0.9543\n\nEpoch 412/1000                                         \n252/252 - 1s - loss: 0.6400 - accuracy: 0.8040 - val_loss: 0.4709 - val_accuracy: 0.8390\n\nEpoch 413/1000                                         \n252/252 - 1s - loss: 0.6307 - accuracy: 0.8029 - val_loss: 0.2514 - val_accuracy: 0.9607\n\nEpoch 414/1000                                         \n252/252 - 1s - loss: 0.6998 - accuracy: 0.7823 - val_loss: 0.3232 - val_accuracy: 0.9180\n\nEpoch 415/1000                                         \n252/252 - 1s - loss: 0.7394 - accuracy: 0.7631 - val_loss: 0.2394 - val_accuracy: 0.9747\n\nEpoch 416/1000                                         \n252/252 - 1s - loss: 0.7368 - accuracy: 0.7740 - val_loss: 0.3023 - val_accuracy: 0.9488\n\nEpoch 417/1000                                         \n252/252 - 1s - loss: 0.7039 - accuracy: 0.7792 - val_loss: 1.0307 - val_accuracy: 0.7247\n\nEpoch 418/1000                                         \n252/252 - 1s - loss: 0.6717 - accuracy: 0.7912 - val_loss: 0.2884 - val_accuracy: 0.9697\n\nEpoch 419/1000                                         \n252/252 - 1s - loss: 0.6569 - accuracy: 0.8020 - val_loss: 0.3362 - val_accuracy: 0.9518\n\nEpoch 420/1000                                         \n252/252 - 1s - loss: 0.6836 - accuracy: 0.8000 - val_loss: 0.2827 - val_accuracy: 0.9717\n\nEpoch 421/1000                                         \n252/252 - 1s - loss: 0.6370 - accuracy: 0.8091 - val_loss: 0.4501 - val_accuracy: 0.8683\n\nEpoch 422/1000                                         \n252/252 - 1s - loss: 0.6158 - accuracy: 0.8149 - val_loss: 0.3651 - val_accuracy: 0.9279\n\nEpoch 423/1000                                         \n252/252 - 1s - loss: 0.6578 - accuracy: 0.8102 - val_loss: 0.4327 - val_accuracy: 0.8728\n\nEpoch 424/1000                                         \n252/252 - 1s - loss: 0.6612 - accuracy: 0.8158 - val_loss: 0.4457 - val_accuracy: 0.8762\n\nEpoch 425/1000                                         \n252/252 - 1s - loss: 0.6067 - accuracy: 0.8276 - val_loss: 0.3896 - val_accuracy: 0.8832\n\nEpoch 426/1000                                         \n252/252 - 1s - loss: 0.6133 - accuracy: 0.8231 - val_loss: 0.3749 - val_accuracy: 0.9215\n\nEpoch 427/1000                                         \n252/252 - 1s - loss: 0.6685 - accuracy: 0.8023 - val_loss: 0.3837 - val_accuracy: 0.9264\n\nEpoch 428/1000                                         \n252/252 - 1s - loss: 0.6127 - accuracy: 0.8181 - val_loss: 0.3381 - val_accuracy: 0.9418\n\nEpoch 429/1000                                         \n252/252 - 1s - loss: 0.6548 - accuracy: 0.8064 - val_loss: 0.5084 - val_accuracy: 0.8444\n\nEpoch 430/1000                                         \n252/252 - 1s - loss: 0.6370 - accuracy: 0.8088 - val_loss: 0.2638 - val_accuracy: 0.9707\n\nEpoch 431/1000                                         \n252/252 - 1s - loss: 0.6069 - accuracy: 0.8153 - val_loss: 0.3575 - val_accuracy: 0.9339\n\nEpoch 432/1000                                         \n252/252 - 1s - loss: 0.6426 - accuracy: 0.8008 - val_loss: 0.2877 - val_accuracy: 0.9563\n\nEpoch 433/1000                                         \n252/252 - 1s - loss: 0.6576 - accuracy: 0.7930 - val_loss: 0.3106 - val_accuracy: 0.9513\n\nEpoch 434/1000                                         \n252/252 - 1s - loss: 0.6295 - accuracy: 0.8034 - val_loss: 0.2996 - val_accuracy: 0.9438\n\nEpoch 435/1000                                         \n252/252 - 1s - loss: 0.6770 - accuracy: 0.7964 - val_loss: 0.2795 - val_accuracy: 0.9722\n\nEpoch 436/1000                                         \n252/252 - 1s - loss: 0.6165 - accuracy: 0.8036 - val_loss: 0.4135 - val_accuracy: 0.8877\n\nEpoch 437/1000                                         \n252/252 - 1s - loss: 0.6478 - accuracy: 0.8025 - val_loss: 0.2543 - val_accuracy: 0.9533\n\nEpoch 438/1000                                         \n252/252 - 1s - loss: 0.6452 - accuracy: 0.8056 - val_loss: 0.2837 - val_accuracy: 0.9568\n\nEpoch 439/1000                                         \n252/252 - 1s - loss: 0.6249 - accuracy: 0.8047 - val_loss: 0.2572 - val_accuracy: 0.9702\n\nEpoch 440/1000                                         \n252/252 - 1s - loss: 0.6813 - accuracy: 0.7952 - val_loss: 0.3254 - val_accuracy: 0.9339\n\nEpoch 441/1000                                         \n252/252 - 1s - loss: 0.6670 - accuracy: 0.7984 - val_loss: 0.2804 - val_accuracy: 0.9573\n\nEpoch 442/1000                                         \n252/252 - 1s - loss: 0.6655 - accuracy: 0.7967 - val_loss: 0.4822 - val_accuracy: 0.8633\n\nEpoch 443/1000                                         \n252/252 - 1s - loss: 0.6571 - accuracy: 0.7957 - val_loss: 0.7802 - val_accuracy: 0.7401\n\nEpoch 444/1000                                         \n252/252 - 1s - loss: 0.6184 - accuracy: 0.8067 - val_loss: 0.3916 - val_accuracy: 0.8857\n\nEpoch 445/1000                                         \n252/252 - 1s - loss: 0.6457 - accuracy: 0.8025 - val_loss: 0.5398 - val_accuracy: 0.8191\n\nEpoch 446/1000                                         \n252/252 - 1s - loss: 0.6258 - accuracy: 0.8021 - val_loss: 0.3156 - val_accuracy: 0.9513\n\nEpoch 447/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6175 - accuracy: 0.8052 - val_loss: 0.3229 - val_accuracy: 0.9369\n\nEpoch 448/1000                                         \n252/252 - 1s - loss: 0.6178 - accuracy: 0.8138 - val_loss: 0.2648 - val_accuracy: 0.9717\n\nEpoch 449/1000                                         \n252/252 - 1s - loss: 0.6183 - accuracy: 0.8131 - val_loss: 0.3193 - val_accuracy: 0.9394\n\nEpoch 450/1000                                         \n252/252 - 1s - loss: 0.6267 - accuracy: 0.8122 - val_loss: 0.2696 - val_accuracy: 0.9697\n\nEpoch 451/1000                                         \n252/252 - 1s - loss: 0.5936 - accuracy: 0.8160 - val_loss: 0.4804 - val_accuracy: 0.8246\n\nEpoch 452/1000                                         \n252/252 - 1s - loss: 0.6404 - accuracy: 0.8039 - val_loss: 0.2855 - val_accuracy: 0.9428\n\nEpoch 453/1000                                         \n252/252 - 1s - loss: 0.5913 - accuracy: 0.8132 - val_loss: 0.2575 - val_accuracy: 0.9533\n\nEpoch 454/1000                                         \n252/252 - 1s - loss: 0.5906 - accuracy: 0.8178 - val_loss: 0.9764 - val_accuracy: 0.7247\n\nEpoch 455/1000                                         \n252/252 - 1s - loss: 0.6015 - accuracy: 0.8147 - val_loss: 0.6715 - val_accuracy: 0.7833\n\nEpoch 456/1000                                         \n252/252 - 1s - loss: 0.6274 - accuracy: 0.8088 - val_loss: 0.2364 - val_accuracy: 0.9667\n\nEpoch 457/1000                                         \n252/252 - 1s - loss: 0.6566 - accuracy: 0.7972 - val_loss: 0.2880 - val_accuracy: 0.9409\n\nEpoch 458/1000                                         \n252/252 - 1s - loss: 0.6113 - accuracy: 0.8096 - val_loss: 0.2126 - val_accuracy: 0.9722\n\nEpoch 459/1000                                         \n252/252 - 1s - loss: 0.6479 - accuracy: 0.7990 - val_loss: 0.3951 - val_accuracy: 0.8996\n\nEpoch 460/1000                                         \n252/252 - 1s - loss: 0.7178 - accuracy: 0.7778 - val_loss: 0.6792 - val_accuracy: 0.7584\n\nEpoch 461/1000                                         \n252/252 - 1s - loss: 0.6974 - accuracy: 0.7871 - val_loss: 0.3013 - val_accuracy: 0.9493\n\nEpoch 462/1000                                         \n252/252 - 1s - loss: 0.7131 - accuracy: 0.7720 - val_loss: 0.7079 - val_accuracy: 0.7570\n\nEpoch 463/1000                                         \n252/252 - 1s - loss: 0.7130 - accuracy: 0.7762 - val_loss: 0.3294 - val_accuracy: 0.9349\n\nEpoch 464/1000                                         \n252/252 - 1s - loss: 0.6988 - accuracy: 0.7748 - val_loss: 0.4205 - val_accuracy: 0.8757\n\nEpoch 465/1000                                         \n252/252 - 1s - loss: 0.6595 - accuracy: 0.7901 - val_loss: 1.1408 - val_accuracy: 0.6233\n\nEpoch 466/1000                                         \n252/252 - 1s - loss: 0.6494 - accuracy: 0.7969 - val_loss: 0.6409 - val_accuracy: 0.7649\n\nEpoch 467/1000                                         \n252/252 - 1s - loss: 0.6953 - accuracy: 0.7850 - val_loss: 0.6443 - val_accuracy: 0.7495\n\nEpoch 468/1000                                         \n252/252 - 1s - loss: 0.6813 - accuracy: 0.7934 - val_loss: 0.5109 - val_accuracy: 0.8330\n\nEpoch 469/1000                                         \n252/252 - 1s - loss: 0.7536 - accuracy: 0.7792 - val_loss: 1.0366 - val_accuracy: 0.6834\n\nEpoch 470/1000                                         \n252/252 - 1s - loss: 0.7160 - accuracy: 0.7876 - val_loss: 0.4177 - val_accuracy: 0.9071\n\nEpoch 471/1000                                         \n252/252 - 1s - loss: 0.7255 - accuracy: 0.7907 - val_loss: 0.3560 - val_accuracy: 0.9259\n\nEpoch 472/1000                                         \n252/252 - 1s - loss: 0.6597 - accuracy: 0.8046 - val_loss: 0.3866 - val_accuracy: 0.9254\n\nEpoch 473/1000                                         \n252/252 - 1s - loss: 0.6718 - accuracy: 0.8018 - val_loss: 0.3371 - val_accuracy: 0.9513\n\nEpoch 474/1000                                         \n252/252 - 1s - loss: 0.7429 - accuracy: 0.7820 - val_loss: 0.5337 - val_accuracy: 0.8539\n\nEpoch 475/1000                                         \n252/252 - 1s - loss: 0.7013 - accuracy: 0.7958 - val_loss: 0.3473 - val_accuracy: 0.9319\n\nEpoch 476/1000                                         \n252/252 - 1s - loss: 0.7113 - accuracy: 0.7925 - val_loss: 0.6396 - val_accuracy: 0.7753\n\nEpoch 477/1000                                         \n252/252 - 1s - loss: 0.6571 - accuracy: 0.8085 - val_loss: 0.4086 - val_accuracy: 0.8882\n\nEpoch 478/1000                                         \n252/252 - 1s - loss: 0.6475 - accuracy: 0.8123 - val_loss: 0.3658 - val_accuracy: 0.9394\n\nEpoch 479/1000                                         \n252/252 - 1s - loss: 0.6159 - accuracy: 0.8207 - val_loss: 0.5006 - val_accuracy: 0.8454\n\nEpoch 480/1000                                         \n252/252 - 1s - loss: 0.6598 - accuracy: 0.8081 - val_loss: 0.4553 - val_accuracy: 0.8777\n\nEpoch 481/1000                                         \n252/252 - 1s - loss: 0.6537 - accuracy: 0.8101 - val_loss: 0.3884 - val_accuracy: 0.9364\n\nEpoch 482/1000                                         \n252/252 - 1s - loss: 0.6857 - accuracy: 0.7972 - val_loss: 0.4156 - val_accuracy: 0.8986\n\nEpoch 483/1000                                         \n252/252 - 1s - loss: 0.6523 - accuracy: 0.8180 - val_loss: 0.4862 - val_accuracy: 0.9095\n\nEpoch 484/1000                                         \n252/252 - 1s - loss: 0.6728 - accuracy: 0.8132 - val_loss: 0.4889 - val_accuracy: 0.8574\n\nEpoch 485/1000                                         \n252/252 - 1s - loss: 0.6230 - accuracy: 0.8237 - val_loss: 0.3794 - val_accuracy: 0.9329\n\nEpoch 486/1000                                         \n252/252 - 1s - loss: 0.6369 - accuracy: 0.8324 - val_loss: 0.4868 - val_accuracy: 0.8509\n\nEpoch 487/1000                                         \n252/252 - 1s - loss: 0.6257 - accuracy: 0.8272 - val_loss: 0.4075 - val_accuracy: 0.8713\n\nEpoch 488/1000                                         \n252/252 - 1s - loss: 0.6999 - accuracy: 0.8003 - val_loss: 0.4574 - val_accuracy: 0.8643\n\nEpoch 489/1000                                         \n252/252 - 1s - loss: 0.6882 - accuracy: 0.8113 - val_loss: 0.5208 - val_accuracy: 0.8688\n\nEpoch 490/1000                                         \n252/252 - 1s - loss: 0.6819 - accuracy: 0.8152 - val_loss: 0.3576 - val_accuracy: 0.9354\n\nEpoch 491/1000                                         \n252/252 - 1s - loss: 0.6274 - accuracy: 0.8215 - val_loss: 1.2418 - val_accuracy: 0.6218\n\nEpoch 492/1000                                         \n252/252 - 1s - loss: 0.6077 - accuracy: 0.8329 - val_loss: 0.4247 - val_accuracy: 0.8832\n\nEpoch 493/1000                                         \n252/252 - 1s - loss: 0.5831 - accuracy: 0.8395 - val_loss: 0.3733 - val_accuracy: 0.9210\n\nEpoch 494/1000                                         \n252/252 - 1s - loss: 0.6001 - accuracy: 0.8340 - val_loss: 0.3243 - val_accuracy: 0.9583\n\nEpoch 495/1000                                         \n252/252 - 1s - loss: 0.6348 - accuracy: 0.8314 - val_loss: 0.4878 - val_accuracy: 0.9041\n\nEpoch 496/1000                                         \n252/252 - 1s - loss: 0.6115 - accuracy: 0.8380 - val_loss: 0.2878 - val_accuracy: 0.9587\n\nEpoch 497/1000                                         \n252/252 - 1s - loss: 0.5841 - accuracy: 0.8389 - val_loss: 0.3174 - val_accuracy: 0.9240\n\nEpoch 498/1000                                         \n252/252 - 1s - loss: 0.6048 - accuracy: 0.8379 - val_loss: 0.3915 - val_accuracy: 0.9518\n\nEpoch 499/1000                                         \n252/252 - 1s - loss: 0.6008 - accuracy: 0.8379 - val_loss: 0.6131 - val_accuracy: 0.7987\n\nEpoch 500/1000                                         \n252/252 - 1s - loss: 0.5570 - accuracy: 0.8456 - val_loss: 2.7412 - val_accuracy: 0.2868\n\nEpoch 501/1000                                         \n252/252 - 1s - loss: 0.6237 - accuracy: 0.8252 - val_loss: 0.3135 - val_accuracy: 0.9503\n\nEpoch 502/1000                                         \n252/252 - 1s - loss: 0.6373 - accuracy: 0.8087 - val_loss: 0.6132 - val_accuracy: 0.7903\n\nEpoch 503/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6914 - accuracy: 0.7988 - val_loss: 0.3601 - val_accuracy: 0.9488\n\nEpoch 504/1000                                         \n252/252 - 1s - loss: 0.6783 - accuracy: 0.8081 - val_loss: 0.3922 - val_accuracy: 0.9463\n\nEpoch 505/1000                                         \n252/252 - 1s - loss: 0.6503 - accuracy: 0.8051 - val_loss: 1.1038 - val_accuracy: 0.5522\n\nEpoch 506/1000                                         \n252/252 - 1s - loss: 0.6249 - accuracy: 0.8173 - val_loss: 0.4443 - val_accuracy: 0.9259\n\nEpoch 507/1000                                         \n252/252 - 1s - loss: 0.6294 - accuracy: 0.8205 - val_loss: 0.6600 - val_accuracy: 0.7594\n\nEpoch 508/1000                                         \n252/252 - 1s - loss: 0.6038 - accuracy: 0.8292 - val_loss: 0.4961 - val_accuracy: 0.8390\n\nEpoch 509/1000                                         \n252/252 - 1s - loss: 0.6178 - accuracy: 0.8215 - val_loss: 0.3236 - val_accuracy: 0.9602\n\nEpoch 510/1000                                         \n252/252 - 1s - loss: 0.5907 - accuracy: 0.8308 - val_loss: 0.5814 - val_accuracy: 0.8151\n\nEpoch 511/1000                                         \n252/252 - 1s - loss: 0.6410 - accuracy: 0.8215 - val_loss: 0.3931 - val_accuracy: 0.8966\n\nEpoch 512/1000                                         \n252/252 - 1s - loss: 0.5986 - accuracy: 0.8315 - val_loss: 0.2663 - val_accuracy: 0.9791\n\nEpoch 513/1000                                         \n252/252 - 1s - loss: 0.6147 - accuracy: 0.8264 - val_loss: 0.2483 - val_accuracy: 0.9672\n\nEpoch 514/1000                                         \n252/252 - 1s - loss: 0.5966 - accuracy: 0.8355 - val_loss: 0.3161 - val_accuracy: 0.9289\n\nEpoch 515/1000                                         \n252/252 - 1s - loss: 0.6279 - accuracy: 0.8196 - val_loss: 3.5672 - val_accuracy: 0.1914\n\nEpoch 516/1000                                         \n252/252 - 1s - loss: 0.5999 - accuracy: 0.8303 - val_loss: 0.2937 - val_accuracy: 0.9369\n\nEpoch 517/1000                                         \n252/252 - 1s - loss: 0.5985 - accuracy: 0.8318 - val_loss: 0.2985 - val_accuracy: 0.9359\n\nEpoch 518/1000                                         \n252/252 - 1s - loss: 0.5977 - accuracy: 0.8318 - val_loss: 0.2908 - val_accuracy: 0.9717\n\nEpoch 519/1000                                         \n252/252 - 1s - loss: 0.5863 - accuracy: 0.8341 - val_loss: 0.2946 - val_accuracy: 0.9622\n\nEpoch 520/1000                                         \n252/252 - 1s - loss: 0.6082 - accuracy: 0.8139 - val_loss: 1.0005 - val_accuracy: 0.5626\n\nEpoch 521/1000                                         \n252/252 - 1s - loss: 0.6163 - accuracy: 0.8142 - val_loss: 0.3767 - val_accuracy: 0.9011\n\nEpoch 522/1000                                         \n252/252 - 1s - loss: 0.6099 - accuracy: 0.8150 - val_loss: 0.4455 - val_accuracy: 0.8767\n\nEpoch 523/1000                                         \n252/252 - 1s - loss: 0.6020 - accuracy: 0.8189 - val_loss: 0.4626 - val_accuracy: 0.8748\n\nEpoch 524/1000                                         \n252/252 - 1s - loss: 0.6540 - accuracy: 0.8064 - val_loss: 0.2951 - val_accuracy: 0.9374\n\nEpoch 525/1000                                         \n252/252 - 1s - loss: 0.6598 - accuracy: 0.7938 - val_loss: 0.5486 - val_accuracy: 0.9031\n\nEpoch 526/1000                                         \n252/252 - 1s - loss: 0.6842 - accuracy: 0.7618 - val_loss: 0.3337 - val_accuracy: 0.9781\n\nEpoch 527/1000                                         \n252/252 - 1s - loss: 0.7103 - accuracy: 0.7574 - val_loss: 0.5807 - val_accuracy: 0.7480\n\nEpoch 528/1000                                         \n252/252 - 1s - loss: 0.7151 - accuracy: 0.7681 - val_loss: 0.6516 - val_accuracy: 0.7878\n\nEpoch 529/1000                                         \n252/252 - 1s - loss: 0.6986 - accuracy: 0.7698 - val_loss: 0.5063 - val_accuracy: 0.8857\n\nEpoch 530/1000                                         \n252/252 - 1s - loss: 0.6746 - accuracy: 0.7836 - val_loss: 0.3800 - val_accuracy: 0.9026\n\nEpoch 531/1000                                         \n252/252 - 1s - loss: 0.6436 - accuracy: 0.7912 - val_loss: 0.5416 - val_accuracy: 0.7997\n\nEpoch 532/1000                                         \n252/252 - 1s - loss: 0.6741 - accuracy: 0.7892 - val_loss: 0.2971 - val_accuracy: 0.9578\n\nEpoch 533/1000                                         \n252/252 - 1s - loss: 0.6268 - accuracy: 0.7997 - val_loss: 0.3681 - val_accuracy: 0.9578\n\nEpoch 534/1000                                         \n252/252 - 1s - loss: 0.6859 - accuracy: 0.7735 - val_loss: 0.4340 - val_accuracy: 0.8812\n\nEpoch 535/1000                                         \n252/252 - 1s - loss: 0.6985 - accuracy: 0.7798 - val_loss: 0.4327 - val_accuracy: 0.9041\n\nEpoch 536/1000                                         \n252/252 - 1s - loss: 0.7056 - accuracy: 0.7763 - val_loss: 0.2704 - val_accuracy: 0.9602\n\nEpoch 537/1000                                         \n252/252 - 1s - loss: 0.7018 - accuracy: 0.7812 - val_loss: 0.3682 - val_accuracy: 0.9081\n\nEpoch 538/1000                                         \n252/252 - 1s - loss: 0.6880 - accuracy: 0.7913 - val_loss: 0.5230 - val_accuracy: 0.8206\n\nEpoch 539/1000                                         \n252/252 - 1s - loss: 0.7276 - accuracy: 0.7768 - val_loss: 0.3277 - val_accuracy: 0.9294\n\nEpoch 540/1000                                         \n252/252 - 1s - loss: 0.7319 - accuracy: 0.7724 - val_loss: 0.3298 - val_accuracy: 0.9274\n\nEpoch 541/1000                                         \n252/252 - 1s - loss: 0.7092 - accuracy: 0.7798 - val_loss: 0.3360 - val_accuracy: 0.9264\n\nEpoch 542/1000                                         \n252/252 - 1s - loss: 0.6968 - accuracy: 0.7916 - val_loss: 0.3223 - val_accuracy: 0.9662\n\nEpoch 543/1000                                         \n252/252 - 1s - loss: 0.6933 - accuracy: 0.7973 - val_loss: 0.3824 - val_accuracy: 0.9031\n\nEpoch 544/1000                                         \n252/252 - 1s - loss: 0.6677 - accuracy: 0.8024 - val_loss: 0.5000 - val_accuracy: 0.8380\n\nEpoch 545/1000                                         \n252/252 - 1s - loss: 0.6436 - accuracy: 0.8088 - val_loss: 0.3293 - val_accuracy: 0.9453\n\nEpoch 546/1000                                         \n252/252 - 1s - loss: 0.6592 - accuracy: 0.8085 - val_loss: 0.2697 - val_accuracy: 0.9587\n\nEpoch 547/1000                                         \n252/252 - 1s - loss: 0.6250 - accuracy: 0.8199 - val_loss: 0.2864 - val_accuracy: 0.9558\n\nEpoch 548/1000                                         \n252/252 - 1s - loss: 0.6181 - accuracy: 0.8174 - val_loss: 1.1062 - val_accuracy: 0.6064\n\nEpoch 549/1000                                         \n252/252 - 1s - loss: 0.6025 - accuracy: 0.8282 - val_loss: 0.4142 - val_accuracy: 0.8862\n\nEpoch 550/1000                                         \n252/252 - 1s - loss: 0.6058 - accuracy: 0.8272 - val_loss: 0.5490 - val_accuracy: 0.8270\n\nEpoch 551/1000                                         \n252/252 - 1s - loss: 0.5956 - accuracy: 0.8258 - val_loss: 0.3154 - val_accuracy: 0.9364\n\nEpoch 552/1000                                         \n252/252 - 1s - loss: 0.5691 - accuracy: 0.8391 - val_loss: 0.4723 - val_accuracy: 0.8638\n\nEpoch 553/1000                                         \n252/252 - 1s - loss: 0.5784 - accuracy: 0.8401 - val_loss: 0.2804 - val_accuracy: 0.9543\n\nEpoch 554/1000                                         \n252/252 - 1s - loss: 0.5963 - accuracy: 0.8246 - val_loss: 0.4640 - val_accuracy: 0.8752\n\nEpoch 555/1000                                         \n252/252 - 1s - loss: 0.6409 - accuracy: 0.7994 - val_loss: 0.2924 - val_accuracy: 0.9409\n\nEpoch 556/1000                                         \n252/252 - 1s - loss: 0.5955 - accuracy: 0.8108 - val_loss: 0.4259 - val_accuracy: 0.8638\n\nEpoch 557/1000                                         \n252/252 - 1s - loss: 0.6577 - accuracy: 0.8020 - val_loss: 0.2757 - val_accuracy: 0.9612\n\nEpoch 558/1000                                         \n252/252 - 1s - loss: 0.6151 - accuracy: 0.8054 - val_loss: 0.3391 - val_accuracy: 0.9289\n\nEpoch 559/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6065 - accuracy: 0.8132 - val_loss: 0.3292 - val_accuracy: 0.9259\n\nEpoch 560/1000                                         \n252/252 - 1s - loss: 0.6178 - accuracy: 0.8098 - val_loss: 0.3470 - val_accuracy: 0.9438\n\nEpoch 561/1000                                         \n252/252 - 1s - loss: 0.6066 - accuracy: 0.8174 - val_loss: 0.3844 - val_accuracy: 0.9533\n\nEpoch 562/1000                                         \n252/252 - 1s - loss: 0.6313 - accuracy: 0.8057 - val_loss: 0.3356 - val_accuracy: 0.9195\n\nEpoch 563/1000                                         \n252/252 - 1s - loss: 0.6124 - accuracy: 0.8195 - val_loss: 0.2616 - val_accuracy: 0.9642\n\nEpoch 564/1000                                         \n252/252 - 1s - loss: 0.6005 - accuracy: 0.8148 - val_loss: 0.3167 - val_accuracy: 0.9443\n\nEpoch 565/1000                                         \n252/252 - 1s - loss: 0.6166 - accuracy: 0.8145 - val_loss: 0.9399 - val_accuracy: 0.7222\n\nEpoch 566/1000                                         \n252/252 - 1s - loss: 0.6075 - accuracy: 0.8183 - val_loss: 0.6558 - val_accuracy: 0.7947\n\nEpoch 567/1000                                         \n252/252 - 1s - loss: 0.6251 - accuracy: 0.8113 - val_loss: 1.1049 - val_accuracy: 0.6228\n\nEpoch 568/1000                                         \n252/252 - 1s - loss: 0.6091 - accuracy: 0.8142 - val_loss: 0.6623 - val_accuracy: 0.7788\n\nEpoch 569/1000                                         \n252/252 - 1s - loss: 0.6223 - accuracy: 0.8102 - val_loss: 0.3241 - val_accuracy: 0.9334\n\nEpoch 570/1000                                         \n252/252 - 1s - loss: 0.5902 - accuracy: 0.8173 - val_loss: 0.6423 - val_accuracy: 0.7942\n\nEpoch 571/1000                                         \n252/252 - 1s - loss: 0.6429 - accuracy: 0.8049 - val_loss: 0.3292 - val_accuracy: 0.9125\n\nEpoch 572/1000                                         \n252/252 - 1s - loss: 0.6068 - accuracy: 0.8163 - val_loss: 0.5067 - val_accuracy: 0.8370\n\nEpoch 573/1000                                         \n252/252 - 1s - loss: 0.6224 - accuracy: 0.8083 - val_loss: 0.5276 - val_accuracy: 0.8375\n\nEpoch 574/1000                                         \n252/252 - 1s - loss: 0.5843 - accuracy: 0.8188 - val_loss: 0.2884 - val_accuracy: 0.9339\n\nEpoch 575/1000                                         \n252/252 - 1s - loss: 0.6499 - accuracy: 0.8087 - val_loss: 0.3025 - val_accuracy: 0.9483\n\nEpoch 576/1000                                         \n252/252 - 1s - loss: 0.6243 - accuracy: 0.8129 - val_loss: 0.2536 - val_accuracy: 0.9468\n\nEpoch 577/1000                                         \n252/252 - 1s - loss: 0.6119 - accuracy: 0.8143 - val_loss: 0.2644 - val_accuracy: 0.9761\n\nEpoch 578/1000                                         \n252/252 - 1s - loss: 0.6585 - accuracy: 0.7957 - val_loss: 0.3391 - val_accuracy: 0.9120\n\nEpoch 579/1000                                         \n252/252 - 1s - loss: 0.6423 - accuracy: 0.7973 - val_loss: 0.2789 - val_accuracy: 0.9657\n\nEpoch 580/1000                                         \n252/252 - 1s - loss: 0.6514 - accuracy: 0.7874 - val_loss: 0.3336 - val_accuracy: 0.9185\n\nEpoch 581/1000                                         \n252/252 - 1s - loss: 0.6579 - accuracy: 0.7849 - val_loss: 0.3196 - val_accuracy: 0.9334\n\nEpoch 582/1000                                         \n252/252 - 1s - loss: 0.6748 - accuracy: 0.7876 - val_loss: 0.8011 - val_accuracy: 0.7411\n\nEpoch 583/1000                                         \n252/252 - 1s - loss: 0.7183 - accuracy: 0.7770 - val_loss: 0.5729 - val_accuracy: 0.7848\n\nEpoch 584/1000                                         \n252/252 - 1s - loss: 0.6560 - accuracy: 0.7954 - val_loss: 0.2503 - val_accuracy: 0.9553\n\nEpoch 585/1000                                         \n252/252 - 1s - loss: 0.6820 - accuracy: 0.7869 - val_loss: 0.2531 - val_accuracy: 0.9692\n\nEpoch 586/1000                                         \n252/252 - 1s - loss: 0.6642 - accuracy: 0.7936 - val_loss: 0.4266 - val_accuracy: 0.8912\n\nEpoch 587/1000                                         \n252/252 - 1s - loss: 0.6275 - accuracy: 0.8081 - val_loss: 0.5710 - val_accuracy: 0.7917\n\nEpoch 588/1000                                         \n252/252 - 1s - loss: 0.6329 - accuracy: 0.8082 - val_loss: 0.2358 - val_accuracy: 0.9692\n\nEpoch 589/1000                                         \n252/252 - 1s - loss: 0.6461 - accuracy: 0.8003 - val_loss: 0.2251 - val_accuracy: 0.9682\n\nEpoch 590/1000                                         \n252/252 - 1s - loss: 0.6856 - accuracy: 0.7983 - val_loss: 0.3191 - val_accuracy: 0.9160\n\nEpoch 591/1000                                         \n252/252 - 1s - loss: 0.6297 - accuracy: 0.8103 - val_loss: 0.4225 - val_accuracy: 0.8921\n\nEpoch 592/1000                                         \n252/252 - 1s - loss: 0.6397 - accuracy: 0.8005 - val_loss: 0.2467 - val_accuracy: 0.9573\n\nEpoch 593/1000                                         \n252/252 - 1s - loss: 0.6719 - accuracy: 0.7819 - val_loss: 0.2781 - val_accuracy: 0.9583\n\nEpoch 594/1000                                         \n252/252 - 1s - loss: 0.6647 - accuracy: 0.7750 - val_loss: 0.7704 - val_accuracy: 0.6769\n\nEpoch 595/1000                                         \n252/252 - 1s - loss: 0.6767 - accuracy: 0.7683 - val_loss: 0.3843 - val_accuracy: 0.9095\n\nEpoch 596/1000                                         \n252/252 - 1s - loss: 0.6766 - accuracy: 0.7726 - val_loss: 0.2568 - val_accuracy: 0.9513\n\nEpoch 597/1000                                         \n252/252 - 1s - loss: 0.7002 - accuracy: 0.7678 - val_loss: 0.6908 - val_accuracy: 0.7749\n\nEpoch 598/1000                                         \n252/252 - 1s - loss: 0.6771 - accuracy: 0.7792 - val_loss: 0.4744 - val_accuracy: 0.8703\n\nEpoch 599/1000                                         \n252/252 - 1s - loss: 0.7010 - accuracy: 0.7674 - val_loss: 0.3686 - val_accuracy: 0.9041\n\nEpoch 600/1000                                         \n252/252 - 1s - loss: 0.6959 - accuracy: 0.7835 - val_loss: 0.3394 - val_accuracy: 0.9433\n\nEpoch 601/1000                                         \n252/252 - 1s - loss: 0.7143 - accuracy: 0.7812 - val_loss: 0.2668 - val_accuracy: 0.9583\n\nEpoch 602/1000                                         \n252/252 - 1s - loss: 0.6704 - accuracy: 0.7902 - val_loss: 0.4364 - val_accuracy: 0.9105\n\nEpoch 603/1000                                         \n252/252 - 1s - loss: 0.7422 - accuracy: 0.7605 - val_loss: 0.4573 - val_accuracy: 0.9090\n\nEpoch 604/1000                                         \n252/252 - 1s - loss: 0.7185 - accuracy: 0.7619 - val_loss: 0.2674 - val_accuracy: 0.9781\n\nEpoch 605/1000                                         \n252/252 - 1s - loss: 0.7251 - accuracy: 0.7738 - val_loss: 0.2986 - val_accuracy: 0.9682\n\nEpoch 606/1000                                         \n252/252 - 1s - loss: 0.7219 - accuracy: 0.7731 - val_loss: 0.3113 - val_accuracy: 0.9354\n\nEpoch 607/1000                                         \n252/252 - 1s - loss: 0.7053 - accuracy: 0.7752 - val_loss: 0.3534 - val_accuracy: 0.9404\n\nEpoch 608/1000                                         \n252/252 - 1s - loss: 0.7459 - accuracy: 0.7715 - val_loss: 0.2836 - val_accuracy: 0.9563\n\nEpoch 609/1000                                         \n252/252 - 1s - loss: 0.7104 - accuracy: 0.7730 - val_loss: 0.3790 - val_accuracy: 0.9264\n\nEpoch 610/1000                                         \n252/252 - 1s - loss: 0.7041 - accuracy: 0.7808 - val_loss: 0.9242 - val_accuracy: 0.7386\n\nEpoch 611/1000                                         \n252/252 - 1s - loss: 0.7018 - accuracy: 0.7761 - val_loss: 0.3445 - val_accuracy: 0.9289\n\nEpoch 612/1000                                         \n252/252 - 1s - loss: 0.6714 - accuracy: 0.7822 - val_loss: 0.2473 - val_accuracy: 0.9642\n\nEpoch 613/1000                                         \n252/252 - 1s - loss: 0.6710 - accuracy: 0.7825 - val_loss: 0.3020 - val_accuracy: 0.9652\n\nEpoch 614/1000                                         \n252/252 - 1s - loss: 0.6711 - accuracy: 0.7880 - val_loss: 0.9522 - val_accuracy: 0.7480\n\nEpoch 615/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6769 - accuracy: 0.7886 - val_loss: 0.2330 - val_accuracy: 0.9747\n\nEpoch 616/1000                                         \n252/252 - 1s - loss: 0.6459 - accuracy: 0.7951 - val_loss: 0.3186 - val_accuracy: 0.9473\n\nEpoch 617/1000                                         \n252/252 - 1s - loss: 0.6747 - accuracy: 0.7871 - val_loss: 0.2928 - val_accuracy: 0.9692\n\nEpoch 618/1000                                         \n252/252 - 1s - loss: 0.6423 - accuracy: 0.7968 - val_loss: 0.4204 - val_accuracy: 0.8807\n\nEpoch 619/1000                                         \n252/252 - 1s - loss: 0.6890 - accuracy: 0.7861 - val_loss: 0.2588 - val_accuracy: 0.9687\n\nEpoch 620/1000                                         \n252/252 - 1s - loss: 0.6829 - accuracy: 0.7804 - val_loss: 0.3953 - val_accuracy: 0.8882\n\nEpoch 621/1000                                         \n252/252 - 1s - loss: 0.6365 - accuracy: 0.7967 - val_loss: 0.6133 - val_accuracy: 0.7873\n\nEpoch 622/1000                                         \n252/252 - 1s - loss: 0.6500 - accuracy: 0.7978 - val_loss: 0.8739 - val_accuracy: 0.6730\n\nEpoch 623/1000                                         \n252/252 - 1s - loss: 0.6908 - accuracy: 0.7833 - val_loss: 0.3065 - val_accuracy: 0.9458\n\nEpoch 624/1000                                         \n252/252 - 1s - loss: 0.6810 - accuracy: 0.7861 - val_loss: 0.2706 - val_accuracy: 0.9632\n\nEpoch 625/1000                                         \n252/252 - 1s - loss: 0.6591 - accuracy: 0.7858 - val_loss: 0.4705 - val_accuracy: 0.8852\n\nEpoch 626/1000                                         \n252/252 - 1s - loss: 0.6156 - accuracy: 0.7983 - val_loss: 0.2974 - val_accuracy: 0.9215\n\nEpoch 627/1000                                         \n252/252 - 1s - loss: 0.6462 - accuracy: 0.7957 - val_loss: 0.3314 - val_accuracy: 0.9090\n\nEpoch 628/1000                                         \n252/252 - 1s - loss: 0.6669 - accuracy: 0.7897 - val_loss: 0.2351 - val_accuracy: 0.9612\n\nEpoch 629/1000                                         \n252/252 - 1s - loss: 0.6508 - accuracy: 0.8008 - val_loss: 0.3762 - val_accuracy: 0.9155\n\nEpoch 630/1000                                         \n252/252 - 1s - loss: 0.6445 - accuracy: 0.7962 - val_loss: 0.2329 - val_accuracy: 0.9597\n\nEpoch 631/1000                                         \n252/252 - 1s - loss: 0.6318 - accuracy: 0.7993 - val_loss: 0.3494 - val_accuracy: 0.9573\n\nEpoch 632/1000                                         \n252/252 - 1s - loss: 0.7148 - accuracy: 0.7768 - val_loss: 0.4411 - val_accuracy: 0.8807\n\nEpoch 633/1000                                         \n252/252 - 1s - loss: 0.7532 - accuracy: 0.7672 - val_loss: 0.4306 - val_accuracy: 0.8817\n\nEpoch 634/1000                                         \n252/252 - 1s - loss: 0.7332 - accuracy: 0.7720 - val_loss: 0.3374 - val_accuracy: 0.9498\n\nEpoch 635/1000                                         \n252/252 - 1s - loss: 0.7306 - accuracy: 0.7833 - val_loss: 0.7324 - val_accuracy: 0.7013\n\nEpoch 636/1000                                         \n252/252 - 1s - loss: 0.7365 - accuracy: 0.7805 - val_loss: 0.2853 - val_accuracy: 0.9647\n\nEpoch 637/1000                                         \n252/252 - 1s - loss: 0.6606 - accuracy: 0.8019 - val_loss: 0.3694 - val_accuracy: 0.9240\n\nEpoch 638/1000                                         \n252/252 - 1s - loss: 0.6586 - accuracy: 0.7927 - val_loss: 0.4019 - val_accuracy: 0.9105\n\nEpoch 639/1000                                         \n252/252 - 1s - loss: 0.6875 - accuracy: 0.7903 - val_loss: 0.4123 - val_accuracy: 0.9105\n\nEpoch 640/1000                                         \n252/252 - 1s - loss: 0.6542 - accuracy: 0.8029 - val_loss: 0.8855 - val_accuracy: 0.7212\n\nEpoch 641/1000                                         \n252/252 - 1s - loss: 0.6361 - accuracy: 0.8086 - val_loss: 0.4064 - val_accuracy: 0.8822\n\nEpoch 642/1000                                         \n252/252 - 1s - loss: 0.6253 - accuracy: 0.8103 - val_loss: 0.2751 - val_accuracy: 0.9717\n\nEpoch 643/1000                                         \n252/252 - 1s - loss: 0.6758 - accuracy: 0.7932 - val_loss: 0.3517 - val_accuracy: 0.9389\n\nEpoch 644/1000                                         \n252/252 - 1s - loss: 0.6979 - accuracy: 0.7703 - val_loss: 0.3035 - val_accuracy: 0.9742\n\nEpoch 645/1000                                         \n252/252 - 1s - loss: 0.6899 - accuracy: 0.7808 - val_loss: 0.5135 - val_accuracy: 0.8424\n\nEpoch 646/1000                                         \n252/252 - 1s - loss: 0.7107 - accuracy: 0.7760 - val_loss: 0.3653 - val_accuracy: 0.9379\n\nEpoch 647/1000                                         \n252/252 - 1s - loss: 0.7045 - accuracy: 0.7799 - val_loss: 0.5318 - val_accuracy: 0.8474\n\nEpoch 648/1000                                         \n252/252 - 1s - loss: 0.6674 - accuracy: 0.7854 - val_loss: 0.5283 - val_accuracy: 0.8131\n\nEpoch 649/1000                                         \n252/252 - 1s - loss: 0.7170 - accuracy: 0.7804 - val_loss: 0.4275 - val_accuracy: 0.9140\n\nEpoch 650/1000                                         \n252/252 - 1s - loss: 0.6774 - accuracy: 0.7889 - val_loss: 0.3870 - val_accuracy: 0.9066\n\nEpoch 651/1000                                         \n252/252 - 1s - loss: 0.6523 - accuracy: 0.7982 - val_loss: 0.4953 - val_accuracy: 0.8290\n\nEpoch 652/1000                                         \n252/252 - 1s - loss: 0.6722 - accuracy: 0.7908 - val_loss: 0.3746 - val_accuracy: 0.8872\n\nEpoch 653/1000                                         \n252/252 - 1s - loss: 0.6568 - accuracy: 0.7992 - val_loss: 0.4332 - val_accuracy: 0.8608\n\nEpoch 654/1000                                         \n252/252 - 1s - loss: 0.6328 - accuracy: 0.8066 - val_loss: 0.4204 - val_accuracy: 0.8673\n\nEpoch 655/1000                                         \n252/252 - 1s - loss: 0.6153 - accuracy: 0.8189 - val_loss: 0.5139 - val_accuracy: 0.8698\n\nEpoch 656/1000                                         \n252/252 - 1s - loss: 0.6444 - accuracy: 0.8126 - val_loss: 0.2916 - val_accuracy: 0.9612\n\nEpoch 657/1000                                         \n252/252 - 1s - loss: 0.6142 - accuracy: 0.8197 - val_loss: 0.3617 - val_accuracy: 0.9140\n\nEpoch 658/1000                                         \n252/252 - 1s - loss: 0.6088 - accuracy: 0.8170 - val_loss: 0.3575 - val_accuracy: 0.9240\n\nEpoch 659/1000                                         \n252/252 - 1s - loss: 0.6102 - accuracy: 0.8164 - val_loss: 0.3953 - val_accuracy: 0.9051\n\nEpoch 660/1000                                         \n252/252 - 1s - loss: 0.6535 - accuracy: 0.8085 - val_loss: 0.3335 - val_accuracy: 0.9597\n\nEpoch 661/1000                                         \n252/252 - 1s - loss: 0.6096 - accuracy: 0.8230 - val_loss: 0.6361 - val_accuracy: 0.7450\n\nEpoch 662/1000                                         \n252/252 - 1s - loss: 0.6372 - accuracy: 0.8153 - val_loss: 0.8823 - val_accuracy: 0.7266\n\nEpoch 663/1000                                         \n252/252 - 1s - loss: 0.6034 - accuracy: 0.8278 - val_loss: 0.2641 - val_accuracy: 0.9737\n\nEpoch 664/1000                                         \n252/252 - 1s - loss: 0.6160 - accuracy: 0.8185 - val_loss: 0.3673 - val_accuracy: 0.8991\n\nEpoch 665/1000                                         \n252/252 - 1s - loss: 0.6838 - accuracy: 0.7910 - val_loss: 0.4394 - val_accuracy: 0.8847\n\nEpoch 666/1000                                         \n252/252 - 1s - loss: 0.6799 - accuracy: 0.7903 - val_loss: 0.3299 - val_accuracy: 0.9354\n\nEpoch 667/1000                                         \n252/252 - 1s - loss: 0.6436 - accuracy: 0.8035 - val_loss: 0.6506 - val_accuracy: 0.7808\n\nEpoch 668/1000                                         \n252/252 - 1s - loss: 0.6234 - accuracy: 0.8149 - val_loss: 0.2758 - val_accuracy: 0.9538\n\nEpoch 669/1000                                         \n252/252 - 1s - loss: 0.6828 - accuracy: 0.7841 - val_loss: 0.3479 - val_accuracy: 0.9150\n\nEpoch 670/1000                                         \n252/252 - 1s - loss: 0.6389 - accuracy: 0.7988 - val_loss: 0.5434 - val_accuracy: 0.7873\n\nEpoch 671/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6624 - accuracy: 0.7993 - val_loss: 0.3312 - val_accuracy: 0.9279\n\nEpoch 672/1000                                         \n252/252 - 1s - loss: 0.6393 - accuracy: 0.8031 - val_loss: 0.2630 - val_accuracy: 0.9672\n\nEpoch 673/1000                                         \n252/252 - 1s - loss: 0.6285 - accuracy: 0.8049 - val_loss: 0.2894 - val_accuracy: 0.9493\n\nEpoch 674/1000                                         \n252/252 - 1s - loss: 0.6591 - accuracy: 0.7949 - val_loss: 0.4266 - val_accuracy: 0.8718\n\nEpoch 675/1000                                         \n252/252 - 1s - loss: 0.6281 - accuracy: 0.8003 - val_loss: 0.2730 - val_accuracy: 0.9662\n\nEpoch 676/1000                                         \n252/252 - 1s - loss: 0.6081 - accuracy: 0.8068 - val_loss: 0.3536 - val_accuracy: 0.9423\n\nEpoch 677/1000                                         \n252/252 - 1s - loss: 0.6653 - accuracy: 0.7995 - val_loss: 0.3915 - val_accuracy: 0.8976\n\nEpoch 678/1000                                         \n252/252 - 1s - loss: 0.7325 - accuracy: 0.7820 - val_loss: 0.4291 - val_accuracy: 0.9115\n\nEpoch 679/1000                                         \n252/252 - 1s - loss: 0.6948 - accuracy: 0.7912 - val_loss: 0.4679 - val_accuracy: 0.9085\n\nEpoch 680/1000                                         \n252/252 - 1s - loss: 0.6769 - accuracy: 0.7894 - val_loss: 0.3830 - val_accuracy: 0.8941\n\nEpoch 681/1000                                         \n252/252 - 1s - loss: 0.7289 - accuracy: 0.7879 - val_loss: 0.3421 - val_accuracy: 0.9573\n\nEpoch 682/1000                                         \n252/252 - 1s - loss: 0.6590 - accuracy: 0.8005 - val_loss: 0.3557 - val_accuracy: 0.9225\n\nEpoch 683/1000                                         \n252/252 - 1s - loss: 0.6718 - accuracy: 0.8000 - val_loss: 0.2807 - val_accuracy: 0.9672\n\nEpoch 684/1000                                         \n252/252 - 1s - loss: 0.6477 - accuracy: 0.8060 - val_loss: 0.3858 - val_accuracy: 0.9314\n\nEpoch 685/1000                                         \n252/252 - 1s - loss: 0.6446 - accuracy: 0.8026 - val_loss: 0.5236 - val_accuracy: 0.8360\n\nEpoch 686/1000                                         \n252/252 - 1s - loss: 0.6575 - accuracy: 0.8106 - val_loss: 0.4434 - val_accuracy: 0.8549\n\nEpoch 687/1000                                         \n252/252 - 1s - loss: 0.6857 - accuracy: 0.7925 - val_loss: 0.7682 - val_accuracy: 0.7013\n\nEpoch 688/1000                                         \n252/252 - 1s - loss: 0.6931 - accuracy: 0.7937 - val_loss: 0.2609 - val_accuracy: 0.9662\n\nEpoch 689/1000                                         \n252/252 - 1s - loss: 0.6540 - accuracy: 0.7985 - val_loss: 0.3885 - val_accuracy: 0.9190\n\nEpoch 690/1000                                         \n252/252 - 1s - loss: 0.6505 - accuracy: 0.8010 - val_loss: 0.3555 - val_accuracy: 0.9115\n\nEpoch 691/1000                                         \n252/252 - 1s - loss: 0.6425 - accuracy: 0.8031 - val_loss: 0.3912 - val_accuracy: 0.9195\n\nEpoch 692/1000                                         \n252/252 - 1s - loss: 0.6109 - accuracy: 0.8118 - val_loss: 0.2919 - val_accuracy: 0.9612\n\nEpoch 693/1000                                         \n252/252 - 1s - loss: 0.6970 - accuracy: 0.7864 - val_loss: 0.2892 - val_accuracy: 0.9607\n\nEpoch 694/1000                                         \n252/252 - 1s - loss: 0.6604 - accuracy: 0.7973 - val_loss: 0.6509 - val_accuracy: 0.7739\n\nEpoch 695/1000                                         \n252/252 - 1s - loss: 0.6426 - accuracy: 0.8051 - val_loss: 0.3668 - val_accuracy: 0.9145\n\nEpoch 696/1000                                         \n252/252 - 1s - loss: 0.6520 - accuracy: 0.8067 - val_loss: 0.3438 - val_accuracy: 0.9364\n\nEpoch 697/1000                                         \n252/252 - 1s - loss: 0.6437 - accuracy: 0.8095 - val_loss: 0.3141 - val_accuracy: 0.9428\n\nEpoch 698/1000                                         \n252/252 - 1s - loss: 0.6413 - accuracy: 0.8066 - val_loss: 1.6525 - val_accuracy: 0.5169\n\nEpoch 699/1000                                         \n252/252 - 1s - loss: 0.6374 - accuracy: 0.8083 - val_loss: 0.4027 - val_accuracy: 0.8991\n\nEpoch 700/1000                                         \n252/252 - 1s - loss: 0.6251 - accuracy: 0.8112 - val_loss: 0.4708 - val_accuracy: 0.8966\n\nEpoch 701/1000                                         \n252/252 - 1s - loss: 0.6170 - accuracy: 0.8097 - val_loss: 0.2854 - val_accuracy: 0.9697\n\nEpoch 702/1000                                         \n252/252 - 1s - loss: 0.6386 - accuracy: 0.8067 - val_loss: 0.3333 - val_accuracy: 0.9205\n\nEpoch 703/1000                                         \n252/252 - 1s - loss: 0.6289 - accuracy: 0.8051 - val_loss: 0.2659 - val_accuracy: 0.9597\n\nEpoch 704/1000                                         \n252/252 - 1s - loss: 0.6170 - accuracy: 0.8101 - val_loss: 0.3253 - val_accuracy: 0.9359\n\nEpoch 705/1000                                         \n252/252 - 1s - loss: 0.6360 - accuracy: 0.8082 - val_loss: 0.5845 - val_accuracy: 0.8340\n\nEpoch 706/1000                                         \n252/252 - 1s - loss: 0.6252 - accuracy: 0.8165 - val_loss: 0.5324 - val_accuracy: 0.8479\n\nEpoch 707/1000                                         \n252/252 - 1s - loss: 0.6098 - accuracy: 0.8148 - val_loss: 0.7587 - val_accuracy: 0.7580\n\nEpoch 708/1000                                         \n252/252 - 1s - loss: 0.5957 - accuracy: 0.8225 - val_loss: 0.2778 - val_accuracy: 0.9662\n\nEpoch 709/1000                                         \n252/252 - 1s - loss: 0.6155 - accuracy: 0.8111 - val_loss: 0.2840 - val_accuracy: 0.9637\n\nEpoch 710/1000                                         \n252/252 - 1s - loss: 0.6348 - accuracy: 0.8042 - val_loss: 0.3201 - val_accuracy: 0.9478\n\nEpoch 711/1000                                         \n252/252 - 1s - loss: 0.6452 - accuracy: 0.8051 - val_loss: 0.2859 - val_accuracy: 0.9438\n\nEpoch 712/1000                                         \n252/252 - 1s - loss: 0.6270 - accuracy: 0.8062 - val_loss: 0.4395 - val_accuracy: 0.8653\n\nEpoch 713/1000                                         \n252/252 - 1s - loss: 0.6251 - accuracy: 0.8140 - val_loss: 0.2847 - val_accuracy: 0.9602\n\nEpoch 714/1000                                         \n252/252 - 1s - loss: 0.7091 - accuracy: 0.7715 - val_loss: 0.3525 - val_accuracy: 0.9140\n\nEpoch 715/1000                                         \n252/252 - 1s - loss: 0.6986 - accuracy: 0.7706 - val_loss: 0.2620 - val_accuracy: 0.9692\n\nEpoch 716/1000                                         \n252/252 - 1s - loss: 0.6922 - accuracy: 0.7787 - val_loss: 0.4967 - val_accuracy: 0.8479\n\nEpoch 717/1000                                         \n252/252 - 1s - loss: 0.6703 - accuracy: 0.7876 - val_loss: 0.3960 - val_accuracy: 0.9170\n\nEpoch 718/1000                                         \n252/252 - 1s - loss: 0.6481 - accuracy: 0.7973 - val_loss: 0.3312 - val_accuracy: 0.9349\n\nEpoch 719/1000                                         \n252/252 - 1s - loss: 0.6208 - accuracy: 0.8104 - val_loss: 0.4126 - val_accuracy: 0.9210\n\nEpoch 720/1000                                         \n252/252 - 1s - loss: 0.6361 - accuracy: 0.8023 - val_loss: 0.4527 - val_accuracy: 0.9185\n\nEpoch 721/1000                                         \n252/252 - 1s - loss: 0.6052 - accuracy: 0.8163 - val_loss: 0.3782 - val_accuracy: 0.9140\n\nEpoch 722/1000                                         \n252/252 - 1s - loss: 0.6073 - accuracy: 0.8215 - val_loss: 0.5068 - val_accuracy: 0.8415\n\nEpoch 723/1000                                         \n252/252 - 1s - loss: 0.5855 - accuracy: 0.8314 - val_loss: 0.3232 - val_accuracy: 0.9602\n\nEpoch 724/1000                                         \n252/252 - 1s - loss: 0.5706 - accuracy: 0.8315 - val_loss: 0.2952 - val_accuracy: 0.9781\n\nEpoch 725/1000                                         \n252/252 - 1s - loss: 0.5458 - accuracy: 0.8385 - val_loss: 0.2658 - val_accuracy: 0.9761\n\nEpoch 726/1000                                         \n252/252 - 1s - loss: 0.5999 - accuracy: 0.8240 - val_loss: 0.3723 - val_accuracy: 0.9274\n\nEpoch 727/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6090 - accuracy: 0.8096 - val_loss: 0.3355 - val_accuracy: 0.9612\n\nEpoch 728/1000                                         \n252/252 - 1s - loss: 0.5827 - accuracy: 0.8179 - val_loss: 0.4623 - val_accuracy: 0.8449\n\nEpoch 729/1000                                         \n252/252 - 1s - loss: 0.5829 - accuracy: 0.8238 - val_loss: 0.2985 - val_accuracy: 0.9612\n\nEpoch 730/1000                                         \n252/252 - 1s - loss: 0.6128 - accuracy: 0.8129 - val_loss: 0.2345 - val_accuracy: 0.9786\n\nEpoch 731/1000                                         \n252/252 - 1s - loss: 0.5927 - accuracy: 0.8314 - val_loss: 0.8311 - val_accuracy: 0.7589\n\nEpoch 732/1000                                         \n252/252 - 1s - loss: 0.5561 - accuracy: 0.8336 - val_loss: 0.3706 - val_accuracy: 0.9011\n\nEpoch 733/1000                                         \n252/252 - 1s - loss: 0.5602 - accuracy: 0.8382 - val_loss: 0.2413 - val_accuracy: 0.9786\n\nEpoch 734/1000                                         \n252/252 - 1s - loss: 0.5854 - accuracy: 0.8221 - val_loss: 0.3768 - val_accuracy: 0.9001\n\nEpoch 735/1000                                         \n252/252 - 1s - loss: 0.5837 - accuracy: 0.8287 - val_loss: 0.2584 - val_accuracy: 0.9617\n\nEpoch 736/1000                                         \n252/252 - 1s - loss: 0.5835 - accuracy: 0.8281 - val_loss: 0.5400 - val_accuracy: 0.8410\n\nEpoch 737/1000                                         \n252/252 - 1s - loss: 0.5635 - accuracy: 0.8325 - val_loss: 0.2978 - val_accuracy: 0.9548\n\nEpoch 738/1000                                         \n252/252 - 1s - loss: 0.5963 - accuracy: 0.8287 - val_loss: 0.3750 - val_accuracy: 0.8976\n\nEpoch 739/1000                                         \n252/252 - 1s - loss: 0.5464 - accuracy: 0.8367 - val_loss: 0.2635 - val_accuracy: 0.9632\n\nEpoch 740/1000                                         \n252/252 - 1s - loss: 0.5434 - accuracy: 0.8390 - val_loss: 0.2727 - val_accuracy: 0.9592\n\nEpoch 741/1000                                         \n252/252 - 1s - loss: 0.5338 - accuracy: 0.8442 - val_loss: 0.5208 - val_accuracy: 0.8464\n\nEpoch 742/1000                                         \n252/252 - 1s - loss: 0.5510 - accuracy: 0.8353 - val_loss: 0.3357 - val_accuracy: 0.9513\n\nEpoch 743/1000                                         \n252/252 - 1s - loss: 0.5575 - accuracy: 0.8382 - val_loss: 0.2410 - val_accuracy: 0.9612\n\nEpoch 744/1000                                         \n252/252 - 1s - loss: 0.5466 - accuracy: 0.8294 - val_loss: 0.3747 - val_accuracy: 0.8926\n\nEpoch 745/1000                                         \n252/252 - 1s - loss: 0.5425 - accuracy: 0.8328 - val_loss: 0.2232 - val_accuracy: 0.9747\n\nEpoch 746/1000                                         \n252/252 - 1s - loss: 0.5646 - accuracy: 0.8343 - val_loss: 0.2242 - val_accuracy: 0.9742\n\nEpoch 747/1000                                         \n252/252 - 1s - loss: 0.5470 - accuracy: 0.8376 - val_loss: 0.2862 - val_accuracy: 0.9468\n\nEpoch 748/1000                                         \n252/252 - 1s - loss: 0.5836 - accuracy: 0.8305 - val_loss: 1.6664 - val_accuracy: 0.6019\n\nEpoch 749/1000                                         \n252/252 - 1s - loss: 0.5326 - accuracy: 0.8407 - val_loss: 0.3772 - val_accuracy: 0.9095\n\nEpoch 750/1000                                         \n252/252 - 1s - loss: 0.5594 - accuracy: 0.8338 - val_loss: 0.2413 - val_accuracy: 0.9682\n\nEpoch 751/1000                                         \n252/252 - 1s - loss: 0.5742 - accuracy: 0.8238 - val_loss: 0.3826 - val_accuracy: 0.8882\n\nEpoch 752/1000                                         \n252/252 - 1s - loss: 0.5829 - accuracy: 0.8251 - val_loss: 0.3748 - val_accuracy: 0.8912\n\nEpoch 753/1000                                         \n252/252 - 1s - loss: 0.5665 - accuracy: 0.8348 - val_loss: 0.3399 - val_accuracy: 0.9294\n\nEpoch 754/1000                                         \n252/252 - 1s - loss: 0.5937 - accuracy: 0.8284 - val_loss: 0.3984 - val_accuracy: 0.8827\n\nEpoch 755/1000                                         \n252/252 - 1s - loss: 0.6359 - accuracy: 0.8104 - val_loss: 0.2512 - val_accuracy: 0.9672\n\nEpoch 756/1000                                         \n252/252 - 1s - loss: 0.5430 - accuracy: 0.8397 - val_loss: 0.2374 - val_accuracy: 0.9578\n\nEpoch 757/1000                                         \n252/252 - 1s - loss: 0.5476 - accuracy: 0.8283 - val_loss: 0.3213 - val_accuracy: 0.9642\n\nEpoch 758/1000                                         \n252/252 - 1s - loss: 0.5755 - accuracy: 0.8267 - val_loss: 0.3796 - val_accuracy: 0.9145\n\nEpoch 759/1000                                         \n252/252 - 1s - loss: 0.5753 - accuracy: 0.8286 - val_loss: 0.3110 - val_accuracy: 0.9448\n\nEpoch 760/1000                                         \n252/252 - 1s - loss: 0.5749 - accuracy: 0.8272 - val_loss: 0.2182 - val_accuracy: 0.9682\n\nEpoch 761/1000                                         \n252/252 - 1s - loss: 0.6024 - accuracy: 0.8071 - val_loss: 0.2988 - val_accuracy: 0.9563\n\nEpoch 762/1000                                         \n252/252 - 1s - loss: 0.5778 - accuracy: 0.8112 - val_loss: 0.2627 - val_accuracy: 0.9583\n\nEpoch 763/1000                                         \n252/252 - 1s - loss: 0.6182 - accuracy: 0.8011 - val_loss: 0.2662 - val_accuracy: 0.9518\n\nEpoch 764/1000                                         \n252/252 - 1s - loss: 0.6569 - accuracy: 0.7891 - val_loss: 0.4818 - val_accuracy: 0.8802\n\nEpoch 765/1000                                         \n252/252 - 1s - loss: 0.6557 - accuracy: 0.7884 - val_loss: 0.3321 - val_accuracy: 0.9528\n\nEpoch 766/1000                                         \n252/252 - 1s - loss: 0.6725 - accuracy: 0.7835 - val_loss: 0.3659 - val_accuracy: 0.9240\n\nEpoch 767/1000                                         \n252/252 - 1s - loss: 0.6693 - accuracy: 0.7846 - val_loss: 0.3792 - val_accuracy: 0.9135\n\nEpoch 768/1000                                         \n252/252 - 1s - loss: 0.6357 - accuracy: 0.7916 - val_loss: 0.4672 - val_accuracy: 0.8678\n\nEpoch 769/1000                                         \n252/252 - 1s - loss: 0.6317 - accuracy: 0.7973 - val_loss: 0.9402 - val_accuracy: 0.7530\n\nEpoch 770/1000                                         \n252/252 - 1s - loss: 0.6741 - accuracy: 0.7808 - val_loss: 0.7522 - val_accuracy: 0.7425\n\nEpoch 771/1000                                         \n252/252 - 1s - loss: 0.6647 - accuracy: 0.7934 - val_loss: 0.4230 - val_accuracy: 0.8862\n\nEpoch 772/1000                                         \n252/252 - 1s - loss: 0.6606 - accuracy: 0.7912 - val_loss: 0.6342 - val_accuracy: 0.7967\n\nEpoch 773/1000                                         \n252/252 - 1s - loss: 0.6784 - accuracy: 0.7863 - val_loss: 0.3214 - val_accuracy: 0.9374\n\nEpoch 774/1000                                         \n252/252 - 1s - loss: 0.6389 - accuracy: 0.8029 - val_loss: 0.3306 - val_accuracy: 0.9145\n\nEpoch 775/1000                                         \n252/252 - 1s - loss: 0.6561 - accuracy: 0.7967 - val_loss: 0.3451 - val_accuracy: 0.9279\n\nEpoch 776/1000                                         \n252/252 - 1s - loss: 0.6739 - accuracy: 0.7964 - val_loss: 0.4058 - val_accuracy: 0.9165\n\nEpoch 777/1000                                         \n252/252 - 1s - loss: 0.6844 - accuracy: 0.7902 - val_loss: 0.8858 - val_accuracy: 0.6899\n\nEpoch 778/1000                                         \n252/252 - 1s - loss: 0.6203 - accuracy: 0.8036 - val_loss: 0.5359 - val_accuracy: 0.8042\n\nEpoch 779/1000                                         \n252/252 - 1s - loss: 0.6333 - accuracy: 0.8042 - val_loss: 0.4761 - val_accuracy: 0.8817\n\nEpoch 780/1000                                         \n252/252 - 1s - loss: 0.6123 - accuracy: 0.8097 - val_loss: 0.2539 - val_accuracy: 0.9642\n\nEpoch 781/1000                                         \n252/252 - 1s - loss: 0.6144 - accuracy: 0.8070 - val_loss: 2.4274 - val_accuracy: 0.5522\n\nEpoch 782/1000                                         \n252/252 - 1s - loss: 0.6206 - accuracy: 0.8065 - val_loss: 0.3014 - val_accuracy: 0.9389\n\nEpoch 783/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.5860 - accuracy: 0.8200 - val_loss: 0.2936 - val_accuracy: 0.9418\n\nEpoch 784/1000                                         \n252/252 - 1s - loss: 0.6114 - accuracy: 0.8111 - val_loss: 0.3483 - val_accuracy: 0.9329\n\nEpoch 785/1000                                         \n252/252 - 1s - loss: 0.5797 - accuracy: 0.8217 - val_loss: 0.2736 - val_accuracy: 0.9533\n\nEpoch 786/1000                                         \n252/252 - 1s - loss: 0.5808 - accuracy: 0.8248 - val_loss: 0.3296 - val_accuracy: 0.9140\n\nEpoch 787/1000                                         \n252/252 - 1s - loss: 0.5967 - accuracy: 0.8191 - val_loss: 0.7870 - val_accuracy: 0.6938\n\nEpoch 788/1000                                         \n252/252 - 1s - loss: 0.5813 - accuracy: 0.8264 - val_loss: 0.4694 - val_accuracy: 0.8613\n\nEpoch 789/1000                                         \n252/252 - 1s - loss: 0.5691 - accuracy: 0.8240 - val_loss: 0.5873 - val_accuracy: 0.7719\n\nEpoch 790/1000                                         \n252/252 - 1s - loss: 0.5758 - accuracy: 0.8267 - val_loss: 0.2870 - val_accuracy: 0.9553\n\nEpoch 791/1000                                         \n252/252 - 1s - loss: 0.6045 - accuracy: 0.8171 - val_loss: 0.3348 - val_accuracy: 0.9399\n\nEpoch 792/1000                                         \n252/252 - 1s - loss: 0.5785 - accuracy: 0.8225 - val_loss: 0.4779 - val_accuracy: 0.8514\n\nEpoch 793/1000                                         \n252/252 - 1s - loss: 0.6119 - accuracy: 0.8114 - val_loss: 0.2826 - val_accuracy: 0.9374\n\nEpoch 794/1000                                         \n252/252 - 1s - loss: 0.5778 - accuracy: 0.8164 - val_loss: 0.2526 - val_accuracy: 0.9722\n\nEpoch 795/1000                                         \n252/252 - 1s - loss: 0.5925 - accuracy: 0.8217 - val_loss: 0.3622 - val_accuracy: 0.9150\n\nEpoch 796/1000                                         \n252/252 - 1s - loss: 0.5802 - accuracy: 0.8160 - val_loss: 0.8590 - val_accuracy: 0.6839\n\nEpoch 797/1000                                         \n252/252 - 1s - loss: 0.6276 - accuracy: 0.8025 - val_loss: 1.4561 - val_accuracy: 0.5696\n\nEpoch 798/1000                                         \n252/252 - 1s - loss: 0.5923 - accuracy: 0.8113 - val_loss: 0.3313 - val_accuracy: 0.9110\n\nEpoch 799/1000                                         \n252/252 - 1s - loss: 0.6150 - accuracy: 0.8050 - val_loss: 0.2372 - val_accuracy: 0.9682\n\nEpoch 800/1000                                         \n252/252 - 1s - loss: 0.6179 - accuracy: 0.8097 - val_loss: 0.4288 - val_accuracy: 0.8668\n\nEpoch 801/1000                                         \n252/252 - 1s - loss: 0.5748 - accuracy: 0.8173 - val_loss: 0.4847 - val_accuracy: 0.8444\n\nEpoch 802/1000                                         \n252/252 - 1s - loss: 0.5932 - accuracy: 0.8122 - val_loss: 0.3021 - val_accuracy: 0.9433\n\nEpoch 803/1000                                         \n252/252 - 1s - loss: 0.5880 - accuracy: 0.8163 - val_loss: 0.4345 - val_accuracy: 0.8588\n\nEpoch 804/1000                                         \n252/252 - 1s - loss: 0.5896 - accuracy: 0.8230 - val_loss: 0.2969 - val_accuracy: 0.9587\n\nEpoch 805/1000                                         \n252/252 - 1s - loss: 0.5808 - accuracy: 0.8183 - val_loss: 0.4100 - val_accuracy: 0.8897\n\nEpoch 806/1000                                         \n252/252 - 1s - loss: 0.5820 - accuracy: 0.8163 - val_loss: 0.2830 - val_accuracy: 0.9359\n\nEpoch 807/1000                                         \n252/252 - 1s - loss: 0.6019 - accuracy: 0.8099 - val_loss: 0.3136 - val_accuracy: 0.9324\n\nEpoch 808/1000                                         \n252/252 - 1s - loss: 0.6414 - accuracy: 0.7911 - val_loss: 0.6203 - val_accuracy: 0.7749\n\nEpoch 809/1000                                         \n252/252 - 1s - loss: 0.6270 - accuracy: 0.8052 - val_loss: 0.3347 - val_accuracy: 0.9210\n\nEpoch 810/1000                                         \n252/252 - 1s - loss: 0.6315 - accuracy: 0.7922 - val_loss: 0.3191 - val_accuracy: 0.9433\n\nEpoch 811/1000                                         \n252/252 - 1s - loss: 0.6769 - accuracy: 0.7741 - val_loss: 0.5115 - val_accuracy: 0.8300\n\nEpoch 812/1000                                         \n252/252 - 1s - loss: 0.6946 - accuracy: 0.7742 - val_loss: 0.3137 - val_accuracy: 0.9349\n\nEpoch 813/1000                                         \n252/252 - 1s - loss: 0.6898 - accuracy: 0.7843 - val_loss: 0.3674 - val_accuracy: 0.9518\n\nEpoch 814/1000                                         \n252/252 - 1s - loss: 0.6673 - accuracy: 0.7870 - val_loss: 0.3868 - val_accuracy: 0.8907\n\nEpoch 815/1000                                         \n252/252 - 1s - loss: 0.7019 - accuracy: 0.7799 - val_loss: 0.2528 - val_accuracy: 0.9587\n\nEpoch 816/1000                                         \n252/252 - 1s - loss: 0.6760 - accuracy: 0.7791 - val_loss: 0.4286 - val_accuracy: 0.8728\n\nEpoch 817/1000                                         \n252/252 - 1s - loss: 0.6684 - accuracy: 0.7802 - val_loss: 0.3362 - val_accuracy: 0.9483\n\nEpoch 818/1000                                         \n252/252 - 1s - loss: 0.6620 - accuracy: 0.7840 - val_loss: 0.3612 - val_accuracy: 0.8996\n\nEpoch 819/1000                                         \n252/252 - 1s - loss: 0.6486 - accuracy: 0.7917 - val_loss: 0.2784 - val_accuracy: 0.9677\n\nEpoch 820/1000                                         \n252/252 - 1s - loss: 0.7088 - accuracy: 0.7670 - val_loss: 0.7941 - val_accuracy: 0.7644\n\nEpoch 821/1000                                         \n252/252 - 1s - loss: 0.6813 - accuracy: 0.7745 - val_loss: 0.2993 - val_accuracy: 0.9329\n\nEpoch 822/1000                                         \n252/252 - 1s - loss: 0.6936 - accuracy: 0.7756 - val_loss: 0.6995 - val_accuracy: 0.7659\n\nEpoch 823/1000                                         \n252/252 - 1s - loss: 0.6680 - accuracy: 0.7871 - val_loss: 0.3554 - val_accuracy: 0.9250\n\nEpoch 824/1000                                         \n252/252 - 1s - loss: 0.6764 - accuracy: 0.7848 - val_loss: 0.3249 - val_accuracy: 0.9190\n\nEpoch 825/1000                                         \n252/252 - 1s - loss: 0.7031 - accuracy: 0.7595 - val_loss: 0.5437 - val_accuracy: 0.8569\n\nEpoch 826/1000                                         \n252/252 - 1s - loss: 0.6869 - accuracy: 0.7546 - val_loss: 0.6266 - val_accuracy: 0.7420\n\nEpoch 827/1000                                         \n252/252 - 1s - loss: 0.6770 - accuracy: 0.7637 - val_loss: 0.3008 - val_accuracy: 0.9597\n\nEpoch 828/1000                                         \n252/252 - 1s - loss: 0.7048 - accuracy: 0.7546 - val_loss: 0.3419 - val_accuracy: 0.9558\n\nEpoch 829/1000                                         \n252/252 - 1s - loss: 0.6865 - accuracy: 0.7668 - val_loss: 0.2727 - val_accuracy: 0.9602\n\nEpoch 830/1000                                         \n252/252 - 1s - loss: 0.6536 - accuracy: 0.7698 - val_loss: 0.3024 - val_accuracy: 0.9463\n\nEpoch 831/1000                                         \n252/252 - 1s - loss: 0.6709 - accuracy: 0.7645 - val_loss: 0.4356 - val_accuracy: 0.8593\n\nEpoch 832/1000                                         \n252/252 - 1s - loss: 0.6476 - accuracy: 0.7834 - val_loss: 0.3627 - val_accuracy: 0.9374\n\nEpoch 833/1000                                         \n252/252 - 1s - loss: 0.6485 - accuracy: 0.7931 - val_loss: 0.3399 - val_accuracy: 0.9210\n\nEpoch 834/1000                                         \n252/252 - 1s - loss: 0.6522 - accuracy: 0.7917 - val_loss: 0.4340 - val_accuracy: 0.8897\n\nEpoch 835/1000                                         \n252/252 - 1s - loss: 0.6257 - accuracy: 0.8064 - val_loss: 0.2947 - val_accuracy: 0.9597\n\nEpoch 836/1000                                         \n252/252 - 1s - loss: 0.6136 - accuracy: 0.8097 - val_loss: 0.3382 - val_accuracy: 0.9642\n\nEpoch 837/1000                                         \n252/252 - 1s - loss: 0.6161 - accuracy: 0.8165 - val_loss: 0.3613 - val_accuracy: 0.9453\n\nEpoch 838/1000                                         \n252/252 - 1s - loss: 0.6046 - accuracy: 0.8174 - val_loss: 0.6103 - val_accuracy: 0.8002\n\nEpoch 839/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6213 - accuracy: 0.8183 - val_loss: 0.3395 - val_accuracy: 0.9359\n\nEpoch 840/1000                                         \n252/252 - 1s - loss: 0.6039 - accuracy: 0.8181 - val_loss: 0.3945 - val_accuracy: 0.8902\n\nEpoch 841/1000                                         \n252/252 - 1s - loss: 0.6111 - accuracy: 0.8174 - val_loss: 0.2502 - val_accuracy: 0.9677\n\nEpoch 842/1000                                         \n252/252 - 1s - loss: 0.5860 - accuracy: 0.8231 - val_loss: 0.3765 - val_accuracy: 0.9011\n\nEpoch 843/1000                                         \n252/252 - 1s - loss: 0.5821 - accuracy: 0.8282 - val_loss: 0.2971 - val_accuracy: 0.9394\n\nEpoch 844/1000                                         \n252/252 - 1s - loss: 0.6003 - accuracy: 0.8253 - val_loss: 0.3776 - val_accuracy: 0.9225\n\nEpoch 845/1000                                         \n252/252 - 1s - loss: 0.6321 - accuracy: 0.8162 - val_loss: 0.3282 - val_accuracy: 0.9200\n\nEpoch 846/1000                                         \n252/252 - 1s - loss: 0.6432 - accuracy: 0.8067 - val_loss: 0.4528 - val_accuracy: 0.8623\n\nEpoch 847/1000                                         \n252/252 - 1s - loss: 0.6647 - accuracy: 0.8046 - val_loss: 0.2402 - val_accuracy: 0.9702\n\nEpoch 848/1000                                         \n252/252 - 1s - loss: 0.6450 - accuracy: 0.8112 - val_loss: 0.3031 - val_accuracy: 0.9324\n\nEpoch 849/1000                                         \n252/252 - 1s - loss: 0.6121 - accuracy: 0.8121 - val_loss: 0.3821 - val_accuracy: 0.9036\n\nEpoch 850/1000                                         \n252/252 - 1s - loss: 0.5972 - accuracy: 0.8158 - val_loss: 0.3939 - val_accuracy: 0.8907\n\nEpoch 851/1000                                         \n252/252 - 1s - loss: 0.5983 - accuracy: 0.8160 - val_loss: 0.3424 - val_accuracy: 0.9379\n\nEpoch 852/1000                                         \n252/252 - 1s - loss: 0.6047 - accuracy: 0.8195 - val_loss: 0.7838 - val_accuracy: 0.7341\n\nEpoch 853/1000                                         \n252/252 - 1s - loss: 0.6101 - accuracy: 0.8168 - val_loss: 0.3374 - val_accuracy: 0.9185\n\nEpoch 854/1000                                         \n252/252 - 1s - loss: 0.6093 - accuracy: 0.8121 - val_loss: 0.2681 - val_accuracy: 0.9657\n\nEpoch 855/1000                                         \n252/252 - 1s - loss: 0.6274 - accuracy: 0.8073 - val_loss: 0.2469 - val_accuracy: 0.9662\n\nEpoch 856/1000                                         \n252/252 - 1s - loss: 0.6558 - accuracy: 0.7980 - val_loss: 0.2763 - val_accuracy: 0.9568\n\nEpoch 857/1000                                         \n252/252 - 1s - loss: 0.6648 - accuracy: 0.7839 - val_loss: 0.2572 - val_accuracy: 0.9727\n\nEpoch 858/1000                                         \n252/252 - 1s - loss: 0.6376 - accuracy: 0.7861 - val_loss: 0.2521 - val_accuracy: 0.9587\n\nEpoch 859/1000                                         \n252/252 - 1s - loss: 0.6856 - accuracy: 0.7732 - val_loss: 0.3740 - val_accuracy: 0.9259\n\nEpoch 860/1000                                         \n252/252 - 1s - loss: 0.6462 - accuracy: 0.7810 - val_loss: 0.4156 - val_accuracy: 0.8802\n\nEpoch 861/1000                                         \n252/252 - 1s - loss: 0.6542 - accuracy: 0.7887 - val_loss: 0.3378 - val_accuracy: 0.9538\n\nEpoch 862/1000                                         \n252/252 - 1s - loss: 0.6691 - accuracy: 0.7838 - val_loss: 0.2941 - val_accuracy: 0.9602\n\nEpoch 863/1000                                         \n252/252 - 1s - loss: 0.6339 - accuracy: 0.7882 - val_loss: 0.9954 - val_accuracy: 0.6914\n\nEpoch 864/1000                                         \n252/252 - 1s - loss: 0.6472 - accuracy: 0.7824 - val_loss: 0.2450 - val_accuracy: 0.9697\n\nEpoch 865/1000                                         \n252/252 - 1s - loss: 0.6281 - accuracy: 0.7906 - val_loss: 0.4183 - val_accuracy: 0.8837\n\nEpoch 866/1000                                         \n252/252 - 1s - loss: 0.6592 - accuracy: 0.7808 - val_loss: 0.3027 - val_accuracy: 0.9528\n\nEpoch 867/1000                                         \n252/252 - 1s - loss: 0.6239 - accuracy: 0.7939 - val_loss: 1.2542 - val_accuracy: 0.4657\n\nEpoch 868/1000                                         \n252/252 - 1s - loss: 0.6633 - accuracy: 0.7704 - val_loss: 0.5741 - val_accuracy: 0.7952\n\nEpoch 869/1000                                         \n252/252 - 1s - loss: 0.6616 - accuracy: 0.7746 - val_loss: 0.2572 - val_accuracy: 0.9573\n\nEpoch 870/1000                                         \n252/252 - 1s - loss: 0.6386 - accuracy: 0.7823 - val_loss: 0.3249 - val_accuracy: 0.9110\n\nEpoch 871/1000                                         \n252/252 - 1s - loss: 0.6128 - accuracy: 0.7920 - val_loss: 0.3214 - val_accuracy: 0.9379\n\nEpoch 872/1000                                         \n252/252 - 1s - loss: 0.6478 - accuracy: 0.7884 - val_loss: 0.2673 - val_accuracy: 0.9662\n\nEpoch 873/1000                                         \n252/252 - 1s - loss: 0.6338 - accuracy: 0.7901 - val_loss: 0.9526 - val_accuracy: 0.6233\n\nEpoch 874/1000                                         \n252/252 - 1s - loss: 0.6539 - accuracy: 0.7865 - val_loss: 0.2491 - val_accuracy: 0.9528\n\nEpoch 875/1000                                         \n252/252 - 1s - loss: 0.6116 - accuracy: 0.7928 - val_loss: 0.2700 - val_accuracy: 0.9478\n\nEpoch 876/1000                                         \n252/252 - 1s - loss: 0.6324 - accuracy: 0.7818 - val_loss: 0.2900 - val_accuracy: 0.9508\n\nEpoch 877/1000                                         \n252/252 - 1s - loss: 0.6332 - accuracy: 0.7874 - val_loss: 0.2900 - val_accuracy: 0.9642\n\nEpoch 878/1000                                         \n252/252 - 1s - loss: 0.6350 - accuracy: 0.7836 - val_loss: 0.4738 - val_accuracy: 0.8693\n\nEpoch 879/1000                                         \n252/252 - 1s - loss: 0.6394 - accuracy: 0.7892 - val_loss: 0.5390 - val_accuracy: 0.8559\n\nEpoch 880/1000                                         \n252/252 - 1s - loss: 0.6481 - accuracy: 0.7825 - val_loss: 0.4989 - val_accuracy: 0.8688\n\nEpoch 881/1000                                         \n252/252 - 1s - loss: 0.6967 - accuracy: 0.7676 - val_loss: 0.4377 - val_accuracy: 0.8693\n\nEpoch 882/1000                                         \n252/252 - 1s - loss: 0.6809 - accuracy: 0.7705 - val_loss: 0.3017 - val_accuracy: 0.9468\n\nEpoch 883/1000                                         \n252/252 - 1s - loss: 0.6710 - accuracy: 0.7820 - val_loss: 0.3739 - val_accuracy: 0.9175\n\nEpoch 884/1000                                         \n252/252 - 1s - loss: 0.6768 - accuracy: 0.7774 - val_loss: 0.3762 - val_accuracy: 0.9135\n\nEpoch 885/1000                                         \n252/252 - 1s - loss: 0.6629 - accuracy: 0.7920 - val_loss: 0.2773 - val_accuracy: 0.9493\n\nEpoch 886/1000                                         \n252/252 - 1s - loss: 0.6475 - accuracy: 0.7953 - val_loss: 0.3191 - val_accuracy: 0.9389\n\nEpoch 887/1000                                         \n252/252 - 1s - loss: 0.6346 - accuracy: 0.7961 - val_loss: 0.2520 - val_accuracy: 0.9692\n\nEpoch 888/1000                                         \n252/252 - 1s - loss: 0.6602 - accuracy: 0.7886 - val_loss: 0.3600 - val_accuracy: 0.9259\n\nEpoch 889/1000                                         \n252/252 - 1s - loss: 0.6435 - accuracy: 0.7885 - val_loss: 0.2637 - val_accuracy: 0.9742\n\nEpoch 890/1000                                         \n252/252 - 1s - loss: 0.6296 - accuracy: 0.7930 - val_loss: 0.2774 - val_accuracy: 0.9627\n\nEpoch 891/1000                                         \n252/252 - 1s - loss: 0.6421 - accuracy: 0.7966 - val_loss: 0.2329 - val_accuracy: 0.9687\n\nEpoch 892/1000                                         \n252/252 - 1s - loss: 0.6500 - accuracy: 0.7917 - val_loss: 0.4998 - val_accuracy: 0.8847\n\nEpoch 893/1000                                         \n252/252 - 1s - loss: 0.6454 - accuracy: 0.7953 - val_loss: 0.3419 - val_accuracy: 0.9076\n\nEpoch 894/1000                                         \n252/252 - 1s - loss: 0.6229 - accuracy: 0.7975 - val_loss: 0.4162 - val_accuracy: 0.8713\n\nEpoch 895/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.5916 - accuracy: 0.8075 - val_loss: 0.2922 - val_accuracy: 0.9548\n\nEpoch 896/1000                                         \n252/252 - 1s - loss: 0.6092 - accuracy: 0.8049 - val_loss: 0.8850 - val_accuracy: 0.7162\n\nEpoch 897/1000                                         \n252/252 - 1s - loss: 0.6030 - accuracy: 0.8030 - val_loss: 0.4960 - val_accuracy: 0.8628\n\nEpoch 898/1000                                         \n252/252 - 1s - loss: 0.6255 - accuracy: 0.8046 - val_loss: 0.4769 - val_accuracy: 0.8618\n\nEpoch 899/1000                                         \n252/252 - 1s - loss: 0.6645 - accuracy: 0.7818 - val_loss: 0.2884 - val_accuracy: 0.9468\n\nEpoch 900/1000                                         \n252/252 - 1s - loss: 0.7064 - accuracy: 0.7707 - val_loss: 0.4157 - val_accuracy: 0.8961\n\nEpoch 901/1000                                         \n252/252 - 1s - loss: 0.6429 - accuracy: 0.7917 - val_loss: 0.4246 - val_accuracy: 0.8807\n\nEpoch 902/1000                                         \n252/252 - 1s - loss: 0.6557 - accuracy: 0.7866 - val_loss: 0.4341 - val_accuracy: 0.8956\n\nEpoch 903/1000                                         \n252/252 - 1s - loss: 0.6417 - accuracy: 0.7906 - val_loss: 0.4154 - val_accuracy: 0.8738\n\nEpoch 904/1000                                         \n252/252 - 1s - loss: 0.6735 - accuracy: 0.7872 - val_loss: 0.2880 - val_accuracy: 0.9722\n\nEpoch 905/1000                                         \n252/252 - 1s - loss: 0.6434 - accuracy: 0.7908 - val_loss: 0.2962 - val_accuracy: 0.9448\n\nEpoch 906/1000                                         \n252/252 - 1s - loss: 0.6376 - accuracy: 0.7920 - val_loss: 0.4275 - val_accuracy: 0.8579\n\nEpoch 907/1000                                         \n252/252 - 1s - loss: 0.6695 - accuracy: 0.7802 - val_loss: 0.4043 - val_accuracy: 0.8668\n\nEpoch 908/1000                                         \n252/252 - 1s - loss: 0.6315 - accuracy: 0.7964 - val_loss: 0.4174 - val_accuracy: 0.8772\n\nEpoch 909/1000                                         \n252/252 - 1s - loss: 0.5995 - accuracy: 0.8015 - val_loss: 0.2617 - val_accuracy: 0.9612\n\nEpoch 910/1000                                         \n252/252 - 1s - loss: 0.6511 - accuracy: 0.7937 - val_loss: 0.2839 - val_accuracy: 0.9587\n\nEpoch 911/1000                                         \n252/252 - 1s - loss: 0.6255 - accuracy: 0.8064 - val_loss: 0.3507 - val_accuracy: 0.9304\n\nEpoch 912/1000                                         \n252/252 - 1s - loss: 0.6577 - accuracy: 0.7903 - val_loss: 0.2629 - val_accuracy: 0.9747\n\nEpoch 913/1000                                         \n252/252 - 1s - loss: 0.6532 - accuracy: 0.7908 - val_loss: 0.3916 - val_accuracy: 0.8892\n\nEpoch 914/1000                                         \n252/252 - 1s - loss: 0.6982 - accuracy: 0.7767 - val_loss: 0.3724 - val_accuracy: 0.8976\n\nEpoch 915/1000                                         \n252/252 - 1s - loss: 0.6894 - accuracy: 0.7807 - val_loss: 0.4384 - val_accuracy: 0.8926\n\nEpoch 916/1000                                         \n252/252 - 1s - loss: 0.6458 - accuracy: 0.7895 - val_loss: 0.3544 - val_accuracy: 0.9274\n\nEpoch 917/1000                                         \n252/252 - 1s - loss: 0.6648 - accuracy: 0.7832 - val_loss: 0.3045 - val_accuracy: 0.9622\n\nEpoch 918/1000                                         \n252/252 - 1s - loss: 0.6683 - accuracy: 0.7865 - val_loss: 0.3364 - val_accuracy: 0.9324\n\nEpoch 919/1000                                         \n252/252 - 1s - loss: 0.6240 - accuracy: 0.7966 - val_loss: 0.2837 - val_accuracy: 0.9418\n\nEpoch 920/1000                                         \n252/252 - 1s - loss: 0.6458 - accuracy: 0.7956 - val_loss: 0.6360 - val_accuracy: 0.7704\n\nEpoch 921/1000                                         \n252/252 - 1s - loss: 0.6469 - accuracy: 0.7942 - val_loss: 0.3196 - val_accuracy: 0.9210\n\nEpoch 922/1000                                         \n252/252 - 1s - loss: 0.6397 - accuracy: 0.7907 - val_loss: 0.2999 - val_accuracy: 0.9458\n\nEpoch 923/1000                                         \n252/252 - 1s - loss: 0.6385 - accuracy: 0.7983 - val_loss: 0.6277 - val_accuracy: 0.8201\n\nEpoch 924/1000                                         \n252/252 - 1s - loss: 0.6149 - accuracy: 0.8076 - val_loss: 0.2947 - val_accuracy: 0.9717\n\nEpoch 925/1000                                         \n252/252 - 1s - loss: 0.6121 - accuracy: 0.7997 - val_loss: 0.2916 - val_accuracy: 0.9493\n\nEpoch 926/1000                                         \n252/252 - 1s - loss: 0.6051 - accuracy: 0.8104 - val_loss: 0.3513 - val_accuracy: 0.9592\n\nEpoch 927/1000                                         \n252/252 - 1s - loss: 0.6263 - accuracy: 0.7956 - val_loss: 0.3888 - val_accuracy: 0.8663\n\nEpoch 928/1000                                         \n252/252 - 1s - loss: 0.6158 - accuracy: 0.8075 - val_loss: 0.3269 - val_accuracy: 0.9354\n\nEpoch 929/1000                                         \n252/252 - 1s - loss: 0.6071 - accuracy: 0.8073 - val_loss: 0.2622 - val_accuracy: 0.9587\n\nEpoch 930/1000                                         \n252/252 - 1s - loss: 0.6034 - accuracy: 0.8113 - val_loss: 0.2786 - val_accuracy: 0.9513\n\nEpoch 931/1000                                         \n252/252 - 1s - loss: 0.5983 - accuracy: 0.8117 - val_loss: 0.2522 - val_accuracy: 0.9652\n\nEpoch 932/1000                                         \n252/252 - 1s - loss: 0.6261 - accuracy: 0.8045 - val_loss: 0.3975 - val_accuracy: 0.8941\n\nEpoch 933/1000                                         \n252/252 - 1s - loss: 0.5790 - accuracy: 0.8189 - val_loss: 0.2850 - val_accuracy: 0.9443\n\nEpoch 934/1000                                         \n252/252 - 1s - loss: 0.5610 - accuracy: 0.8246 - val_loss: 0.3032 - val_accuracy: 0.9498\n\nEpoch 935/1000                                         \n252/252 - 1s - loss: 0.6402 - accuracy: 0.7726 - val_loss: 0.5479 - val_accuracy: 0.8181\n\nEpoch 936/1000                                         \n252/252 - 1s - loss: 0.6190 - accuracy: 0.7860 - val_loss: 0.3592 - val_accuracy: 0.9051\n\nEpoch 937/1000                                         \n252/252 - 1s - loss: 0.6244 - accuracy: 0.7874 - val_loss: 0.2576 - val_accuracy: 0.9533\n\nEpoch 938/1000                                         \n252/252 - 1s - loss: 0.6668 - accuracy: 0.7745 - val_loss: 0.3080 - val_accuracy: 0.9652\n\nEpoch 939/1000                                         \n252/252 - 1s - loss: 0.6059 - accuracy: 0.8016 - val_loss: 0.2646 - val_accuracy: 0.9622\n\nEpoch 940/1000                                         \n252/252 - 1s - loss: 0.6084 - accuracy: 0.8023 - val_loss: 0.3435 - val_accuracy: 0.9160\n\nEpoch 941/1000                                         \n252/252 - 1s - loss: 0.6179 - accuracy: 0.8016 - val_loss: 0.4706 - val_accuracy: 0.8807\n\nEpoch 942/1000                                         \n252/252 - 1s - loss: 0.5997 - accuracy: 0.7993 - val_loss: 0.3206 - val_accuracy: 0.9185\n\nEpoch 943/1000                                         \n252/252 - 1s - loss: 0.5852 - accuracy: 0.8138 - val_loss: 0.2566 - val_accuracy: 0.9592\n\nEpoch 944/1000                                         \n252/252 - 1s - loss: 0.5947 - accuracy: 0.8109 - val_loss: 0.2798 - val_accuracy: 0.9617\n\nEpoch 945/1000                                         \n252/252 - 1s - loss: 0.6572 - accuracy: 0.7908 - val_loss: 0.3041 - val_accuracy: 0.9632\n\nEpoch 946/1000                                         \n252/252 - 1s - loss: 0.6191 - accuracy: 0.8016 - val_loss: 0.4053 - val_accuracy: 0.8897\n\nEpoch 947/1000                                         \n252/252 - 1s - loss: 0.6381 - accuracy: 0.7906 - val_loss: 0.4740 - val_accuracy: 0.8449\n\nEpoch 948/1000                                         \n252/252 - 1s - loss: 0.6761 - accuracy: 0.7868 - val_loss: 0.3684 - val_accuracy: 0.9443\n\nEpoch 949/1000                                         \n252/252 - 1s - loss: 0.6642 - accuracy: 0.7810 - val_loss: 0.8248 - val_accuracy: 0.7087\n\nEpoch 950/1000                                         \n252/252 - 1s - loss: 0.6775 - accuracy: 0.7853 - val_loss: 0.4117 - val_accuracy: 0.9006\n\nEpoch 951/1000                                         \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.6920 - accuracy: 0.7763 - val_loss: 0.4165 - val_accuracy: 0.8867\n\nEpoch 952/1000                                         \n252/252 - 1s - loss: 0.6751 - accuracy: 0.7746 - val_loss: 0.4148 - val_accuracy: 0.8842\n\nEpoch 953/1000                                         \n252/252 - 1s - loss: 0.6841 - accuracy: 0.7863 - val_loss: 0.3334 - val_accuracy: 0.9543\n\nEpoch 954/1000                                         \n252/252 - 1s - loss: 0.6869 - accuracy: 0.7760 - val_loss: 0.2943 - val_accuracy: 0.9558\n\nEpoch 955/1000                                         \n252/252 - 1s - loss: 0.6417 - accuracy: 0.8033 - val_loss: 0.2441 - val_accuracy: 0.9662\n\nEpoch 956/1000                                         \n252/252 - 1s - loss: 0.6649 - accuracy: 0.7882 - val_loss: 0.4537 - val_accuracy: 0.8902\n\nEpoch 957/1000                                         \n252/252 - 1s - loss: 0.6608 - accuracy: 0.7833 - val_loss: 0.4095 - val_accuracy: 0.8892\n\nEpoch 958/1000                                         \n252/252 - 1s - loss: 0.6425 - accuracy: 0.7920 - val_loss: 0.4315 - val_accuracy: 0.8772\n\nEpoch 959/1000                                         \n252/252 - 1s - loss: 0.6341 - accuracy: 0.7994 - val_loss: 0.2498 - val_accuracy: 0.9587\n\nEpoch 960/1000                                         \n252/252 - 1s - loss: 0.6439 - accuracy: 0.7913 - val_loss: 0.3113 - val_accuracy: 0.9384\n\nEpoch 961/1000                                         \n252/252 - 1s - loss: 0.6060 - accuracy: 0.8013 - val_loss: 0.3755 - val_accuracy: 0.8852\n\nEpoch 962/1000                                         \n252/252 - 1s - loss: 0.6135 - accuracy: 0.7998 - val_loss: 0.3830 - val_accuracy: 0.9324\n\nEpoch 963/1000                                         \n252/252 - 1s - loss: 0.6035 - accuracy: 0.8050 - val_loss: 0.3751 - val_accuracy: 0.9235\n\nEpoch 964/1000                                         \n252/252 - 1s - loss: 0.6319 - accuracy: 0.7963 - val_loss: 0.3773 - val_accuracy: 0.9056\n\nEpoch 965/1000                                         \n252/252 - 1s - loss: 0.6216 - accuracy: 0.7941 - val_loss: 0.2903 - val_accuracy: 0.9418\n\nEpoch 966/1000                                         \n252/252 - 1s - loss: 0.6383 - accuracy: 0.8008 - val_loss: 0.3429 - val_accuracy: 0.9319\n\nEpoch 967/1000                                         \n252/252 - 1s - loss: 0.6175 - accuracy: 0.8025 - val_loss: 0.2590 - val_accuracy: 0.9493\n\nEpoch 968/1000                                         \n252/252 - 1s - loss: 0.6339 - accuracy: 0.7930 - val_loss: 0.6570 - val_accuracy: 0.7078\n\nEpoch 969/1000                                         \n252/252 - 1s - loss: 0.6066 - accuracy: 0.7968 - val_loss: 0.2933 - val_accuracy: 0.9473\n\nEpoch 970/1000                                         \n252/252 - 1s - loss: 0.6102 - accuracy: 0.7885 - val_loss: 0.3999 - val_accuracy: 0.8926\n\nEpoch 971/1000                                         \n252/252 - 1s - loss: 0.6484 - accuracy: 0.7856 - val_loss: 0.3104 - val_accuracy: 0.9607\n\nEpoch 972/1000                                         \n252/252 - 1s - loss: 0.6117 - accuracy: 0.7933 - val_loss: 0.2794 - val_accuracy: 0.9553\n\nEpoch 973/1000                                         \n252/252 - 1s - loss: 0.6239 - accuracy: 0.7972 - val_loss: 0.3916 - val_accuracy: 0.8738\n\nEpoch 974/1000                                         \n252/252 - 1s - loss: 0.6322 - accuracy: 0.7922 - val_loss: 0.2920 - val_accuracy: 0.9468\n\nEpoch 975/1000                                         \n252/252 - 1s - loss: 0.5913 - accuracy: 0.8047 - val_loss: 0.3592 - val_accuracy: 0.9001\n\nEpoch 976/1000                                         \n252/252 - 1s - loss: 0.6245 - accuracy: 0.7966 - val_loss: 0.3923 - val_accuracy: 0.8946\n\nEpoch 977/1000                                         \n252/252 - 1s - loss: 0.6170 - accuracy: 0.8088 - val_loss: 0.5026 - val_accuracy: 0.8653\n\nEpoch 978/1000                                         \n252/252 - 1s - loss: 0.6376 - accuracy: 0.8028 - val_loss: 0.3227 - val_accuracy: 0.9334\n\nEpoch 979/1000                                         \n252/252 - 1s - loss: 0.5947 - accuracy: 0.8097 - val_loss: 1.1986 - val_accuracy: 0.5984\n\nEpoch 980/1000                                         \n252/252 - 1s - loss: 0.6249 - accuracy: 0.8042 - val_loss: 0.2946 - val_accuracy: 0.9369\n\nEpoch 981/1000                                         \n252/252 - 1s - loss: 0.5915 - accuracy: 0.8099 - val_loss: 0.2863 - val_accuracy: 0.9369\n\nEpoch 982/1000                                         \n252/252 - 1s - loss: 0.6062 - accuracy: 0.8096 - val_loss: 0.2934 - val_accuracy: 0.9761\n\nEpoch 983/1000                                         \n252/252 - 1s - loss: 0.6406 - accuracy: 0.7969 - val_loss: 0.2349 - val_accuracy: 0.9697\n\nEpoch 984/1000                                         \n252/252 - 1s - loss: 0.6317 - accuracy: 0.8013 - val_loss: 0.4703 - val_accuracy: 0.8976\n\nEpoch 985/1000                                         \n252/252 - 1s - loss: 0.6561 - accuracy: 0.7952 - val_loss: 0.4508 - val_accuracy: 0.8757\n\nEpoch 986/1000                                         \n252/252 - 1s - loss: 0.6514 - accuracy: 0.7902 - val_loss: 0.2643 - val_accuracy: 0.9583\n\nEpoch 987/1000                                         \n252/252 - 1s - loss: 0.6302 - accuracy: 0.8026 - val_loss: 0.3916 - val_accuracy: 0.9250\n\nEpoch 988/1000                                         \n252/252 - 1s - loss: 0.6099 - accuracy: 0.8104 - val_loss: 0.8142 - val_accuracy: 0.7391\n\nEpoch 989/1000                                         \n252/252 - 1s - loss: 0.6126 - accuracy: 0.8080 - val_loss: 0.3382 - val_accuracy: 0.9269\n\nEpoch 990/1000                                         \n252/252 - 1s - loss: 0.6345 - accuracy: 0.8054 - val_loss: 0.4466 - val_accuracy: 0.8917\n\nEpoch 991/1000                                         \n252/252 - 1s - loss: 0.6601 - accuracy: 0.8003 - val_loss: 0.6619 - val_accuracy: 0.7455\n\nEpoch 992/1000                                         \n252/252 - 1s - loss: 0.5948 - accuracy: 0.8157 - val_loss: 0.2770 - val_accuracy: 0.9558\n\nEpoch 993/1000                                         \n252/252 - 1s - loss: 0.5852 - accuracy: 0.8140 - val_loss: 0.3179 - val_accuracy: 0.9453\n\nEpoch 994/1000                                         \n252/252 - 1s - loss: 0.5755 - accuracy: 0.8229 - val_loss: 0.2715 - val_accuracy: 0.9548\n\nEpoch 995/1000                                         \n252/252 - 1s - loss: 0.5713 - accuracy: 0.8226 - val_loss: 0.3029 - val_accuracy: 0.9518\n\nEpoch 996/1000                                         \n252/252 - 1s - loss: 0.6043 - accuracy: 0.8212 - val_loss: 0.2603 - val_accuracy: 0.9682\n\nEpoch 997/1000                                         \n252/252 - 1s - loss: 0.5760 - accuracy: 0.8193 - val_loss: 0.3159 - val_accuracy: 0.9379\n\nEpoch 998/1000                                         \n252/252 - 1s - loss: 0.5860 - accuracy: 0.8191 - val_loss: 0.2775 - val_accuracy: 0.9712\n\nEpoch 999/1000                                         \n252/252 - 1s - loss: 0.5644 - accuracy: 0.8288 - val_loss: 0.2496 - val_accuracy: 0.9558\n\nEpoch 1000/1000                                        \n252/252 - 1s - loss: 0.5813 - accuracy: 0.8269 - val_loss: 0.3005 - val_accuracy: 0.9533\n\n 1/63 [..............................]                 \n - ETA: 0s - loss: 0.1917 - accuracy: 1.0000           \n                                                      \n21/63 [=========>....................]                 \n - ETA: 0s - loss: 0.3523 - accuracy: 0.9747           \n                                                      \n39/63 [=================>............]                 \n - ETA: 0s - loss: 0.3503 - accuracy: 0.9431           \n                                                      \n57/63 [==========================>...]                 \n - ETA: 0s - loss: 0.3195 - accuracy: 0.9490           \n                                                      \n63/63 [==============================]                 \n - 0s 3ms/step - loss: 0.3005 - accuracy: 0.9533       \n\nValidation Accuracy: 95.3280%                          \nValidation Loss: 0.3004775643348694                    \n","name":"stdout"},{"output_type":"stream","text":"Test Accuracy: 0.9904279279279279                      \n*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\nParameters testing:                                                                  \n{'act_reg': 1e-05, 'activation_function': 'relu', 'beta_1': 0.3, 'beta_2': 0.8, 'bias_reg': 1e-05, 'hidden1': 4096, 'ker_reg': 0.001, 'lr': 1e-07}\nEpoch 1/1000                                                                         \n252/252 - 1s - loss: 3.6425 - accuracy: 0.2655 - val_loss: 3.6140 - val_accuracy: 0.3221\n\nEpoch 2/1000                                                                         \n252/252 - 1s - loss: 3.6049 - accuracy: 0.3139 - val_loss: 3.5864 - val_accuracy: 0.3817\n\nEpoch 3/1000                                                                         \n252/252 - 1s - loss: 3.5809 - accuracy: 0.3580 - val_loss: 3.5702 - val_accuracy: 0.3802\n\nEpoch 4/1000                                                                         \n252/252 - 1s - loss: 3.5672 - accuracy: 0.3683 - val_loss: 3.5601 - val_accuracy: 0.3802\n\nEpoch 5/1000                                                                         \n252/252 - 1s - loss: 3.5579 - accuracy: 0.3810 - val_loss: 3.5525 - val_accuracy: 0.3802\n\nEpoch 6/1000                                                                         \n252/252 - 1s - loss: 3.5476 - accuracy: 0.3877 - val_loss: 3.5460 - val_accuracy: 0.3802\n\nEpoch 7/1000                                                                         \n252/252 - 1s - loss: 3.5420 - accuracy: 0.3864 - val_loss: 3.5399 - val_accuracy: 0.3802\n\nEpoch 8/1000                                                                         \n252/252 - 1s - loss: 3.5331 - accuracy: 0.3895 - val_loss: 3.5340 - val_accuracy: 0.3802\n\nEpoch 9/1000                                                                         \n252/252 - 1s - loss: 3.5236 - accuracy: 0.3970 - val_loss: 3.5282 - val_accuracy: 0.3802\n\nEpoch 10/1000                                                                        \n252/252 - 1s - loss: 3.5211 - accuracy: 0.3966 - val_loss: 3.5225 - val_accuracy: 0.3802\n\nEpoch 11/1000                                                                        \n252/252 - 1s - loss: 3.5100 - accuracy: 0.4007 - val_loss: 3.5168 - val_accuracy: 0.3802\n\nEpoch 12/1000                                                                        \n252/252 - 1s - loss: 3.5020 - accuracy: 0.4049 - val_loss: 3.5113 - val_accuracy: 0.3802\n\nEpoch 13/1000                                                                        \n252/252 - 1s - loss: 3.4949 - accuracy: 0.4070 - val_loss: 3.5057 - val_accuracy: 0.3802\n\nEpoch 14/1000                                                                        \n252/252 - 1s - loss: 3.4890 - accuracy: 0.4127 - val_loss: 3.5002 - val_accuracy: 0.3802\n\nEpoch 15/1000                                                                        \n252/252 - 1s - loss: 3.4803 - accuracy: 0.4104 - val_loss: 3.4948 - val_accuracy: 0.3802\n\nEpoch 16/1000                                                                        \n252/252 - 1s - loss: 3.4723 - accuracy: 0.4168 - val_loss: 3.4894 - val_accuracy: 0.3802\n\nEpoch 17/1000                                                                        \n252/252 - 1s - loss: 3.4691 - accuracy: 0.4193 - val_loss: 3.4840 - val_accuracy: 0.3807\n\nEpoch 18/1000                                                                        \n252/252 - 1s - loss: 3.4585 - accuracy: 0.4306 - val_loss: 3.4788 - val_accuracy: 0.3807\n\nEpoch 19/1000                                                                        \n252/252 - 1s - loss: 3.4544 - accuracy: 0.4227 - val_loss: 3.4736 - val_accuracy: 0.3807\n\nEpoch 20/1000                                                                        \n252/252 - 1s - loss: 3.4504 - accuracy: 0.4301 - val_loss: 3.4685 - val_accuracy: 0.3812\n\nEpoch 21/1000                                                                        \n252/252 - 1s - loss: 3.4399 - accuracy: 0.4332 - val_loss: 3.4634 - val_accuracy: 0.3812\n\nEpoch 22/1000                                                                        \n252/252 - 1s - loss: 3.4316 - accuracy: 0.4401 - val_loss: 3.4583 - val_accuracy: 0.3817\n\nEpoch 23/1000                                                                        \n252/252 - 1s - loss: 3.4246 - accuracy: 0.4439 - val_loss: 3.4533 - val_accuracy: 0.3817\n\nEpoch 24/1000                                                                        \n252/252 - 1s - loss: 3.4193 - accuracy: 0.4421 - val_loss: 3.4483 - val_accuracy: 0.3827\n\nEpoch 25/1000                                                                        \n252/252 - 1s - loss: 3.4132 - accuracy: 0.4497 - val_loss: 3.4434 - val_accuracy: 0.3847\n\nEpoch 26/1000                                                                        \n252/252 - 1s - loss: 3.4098 - accuracy: 0.4553 - val_loss: 3.4386 - val_accuracy: 0.3872\n\nEpoch 27/1000                                                                        \n252/252 - 1s - loss: 3.4001 - accuracy: 0.4574 - val_loss: 3.4337 - val_accuracy: 0.3897\n\nEpoch 28/1000                                                                        \n252/252 - 1s - loss: 3.3934 - accuracy: 0.4640 - val_loss: 3.4289 - val_accuracy: 0.3936\n\nEpoch 29/1000                                                                        \n252/252 - 1s - loss: 3.3876 - accuracy: 0.4693 - val_loss: 3.4241 - val_accuracy: 0.4006\n\nEpoch 30/1000                                                                        \n252/252 - 1s - loss: 3.3790 - accuracy: 0.4766 - val_loss: 3.4194 - val_accuracy: 0.4041\n\nEpoch 31/1000                                                                        \n252/252 - 1s - loss: 3.3751 - accuracy: 0.4767 - val_loss: 3.4147 - val_accuracy: 0.4081\n\nEpoch 32/1000                                                                        \n252/252 - 1s - loss: 3.3692 - accuracy: 0.4854 - val_loss: 3.4101 - val_accuracy: 0.4150\n\nEpoch 33/1000                                                                        \n252/252 - 1s - loss: 3.3678 - accuracy: 0.4821 - val_loss: 3.4055 - val_accuracy: 0.4225\n\nEpoch 34/1000                                                                        \n252/252 - 1s - loss: 3.3581 - accuracy: 0.4874 - val_loss: 3.4009 - val_accuracy: 0.4269\n\nEpoch 35/1000                                                                        \n252/252 - 1s - loss: 3.3518 - accuracy: 0.4922 - val_loss: 3.3964 - val_accuracy: 0.4309\n\nEpoch 36/1000                                                                        \n252/252 - 1s - loss: 3.3461 - accuracy: 0.4899 - val_loss: 3.3919 - val_accuracy: 0.4394\n\nEpoch 37/1000                                                                        \n252/252 - 1s - loss: 3.3392 - accuracy: 0.5025 - val_loss: 3.3874 - val_accuracy: 0.4473\n\nEpoch 38/1000                                                                        \n252/252 - 1s - loss: 3.3342 - accuracy: 0.5102 - val_loss: 3.3830 - val_accuracy: 0.4543\n\nEpoch 39/1000                                                                        \n252/252 - 1s - loss: 3.3303 - accuracy: 0.5112 - val_loss: 3.3786 - val_accuracy: 0.4597\n\nEpoch 40/1000                                                                        \n252/252 - 1s - loss: 3.3212 - accuracy: 0.5143 - val_loss: 3.3742 - val_accuracy: 0.4687\n\nEpoch 41/1000                                                                        \n252/252 - 1s - loss: 3.3202 - accuracy: 0.5158 - val_loss: 3.3699 - val_accuracy: 0.4801\n\nEpoch 42/1000                                                                        \n252/252 - 1s - loss: 3.3165 - accuracy: 0.5171 - val_loss: 3.3656 - val_accuracy: 0.4886\n\nEpoch 43/1000                                                                        \n252/252 - 1s - loss: 3.3088 - accuracy: 0.5240 - val_loss: 3.3613 - val_accuracy: 0.4955\n\nEpoch 44/1000                                                                        \n252/252 - 1s - loss: 3.3057 - accuracy: 0.5229 - val_loss: 3.3571 - val_accuracy: 0.5030\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 45/1000                                                                        \n252/252 - 1s - loss: 3.2982 - accuracy: 0.5322 - val_loss: 3.3528 - val_accuracy: 0.5099\n\nEpoch 46/1000                                                                        \n252/252 - 1s - loss: 3.2886 - accuracy: 0.5396 - val_loss: 3.3486 - val_accuracy: 0.5184\n\nEpoch 47/1000                                                                        \n252/252 - 1s - loss: 3.2867 - accuracy: 0.5417 - val_loss: 3.3444 - val_accuracy: 0.5239\n\nEpoch 48/1000                                                                        \n252/252 - 1s - loss: 3.2838 - accuracy: 0.5369 - val_loss: 3.3402 - val_accuracy: 0.5253\n\nEpoch 49/1000                                                                        \n252/252 - 1s - loss: 3.2763 - accuracy: 0.5462 - val_loss: 3.3361 - val_accuracy: 0.5318\n\nEpoch 50/1000                                                                        \n252/252 - 1s - loss: 3.2745 - accuracy: 0.5461 - val_loss: 3.3320 - val_accuracy: 0.5432\n\nEpoch 51/1000                                                                        \n252/252 - 1s - loss: 3.2653 - accuracy: 0.5638 - val_loss: 3.3279 - val_accuracy: 0.5487\n\nEpoch 52/1000                                                                        \n252/252 - 1s - loss: 3.2616 - accuracy: 0.5579 - val_loss: 3.3238 - val_accuracy: 0.5577\n\nEpoch 53/1000                                                                        \n252/252 - 1s - loss: 3.2557 - accuracy: 0.5633 - val_loss: 3.3197 - val_accuracy: 0.5626\n\nEpoch 54/1000                                                                        \n252/252 - 1s - loss: 3.2520 - accuracy: 0.5700 - val_loss: 3.3156 - val_accuracy: 0.5696\n\nEpoch 55/1000                                                                        \n252/252 - 1s - loss: 3.2439 - accuracy: 0.5731 - val_loss: 3.3115 - val_accuracy: 0.5755\n\nEpoch 56/1000                                                                        \n252/252 - 1s - loss: 3.2438 - accuracy: 0.5647 - val_loss: 3.3076 - val_accuracy: 0.5805\n\nEpoch 57/1000                                                                        \n252/252 - 1s - loss: 3.2359 - accuracy: 0.5775 - val_loss: 3.3035 - val_accuracy: 0.5820\n\nEpoch 58/1000                                                                        \n252/252 - 1s - loss: 3.2295 - accuracy: 0.5813 - val_loss: 3.2995 - val_accuracy: 0.5890\n\nEpoch 59/1000                                                                        \n252/252 - 1s - loss: 3.2271 - accuracy: 0.5817 - val_loss: 3.2956 - val_accuracy: 0.5969\n\nEpoch 60/1000                                                                        \n252/252 - 1s - loss: 3.2204 - accuracy: 0.5874 - val_loss: 3.2916 - val_accuracy: 0.5984\n\nEpoch 61/1000                                                                        \n252/252 - 1s - loss: 3.2179 - accuracy: 0.5947 - val_loss: 3.2877 - val_accuracy: 0.6044\n\nEpoch 62/1000                                                                        \n252/252 - 1s - loss: 3.2120 - accuracy: 0.6024 - val_loss: 3.2836 - val_accuracy: 0.6074\n\nEpoch 63/1000                                                                        \n252/252 - 1s - loss: 3.2099 - accuracy: 0.5915 - val_loss: 3.2797 - val_accuracy: 0.6113\n\nEpoch 64/1000                                                                        \n252/252 - 1s - loss: 3.2029 - accuracy: 0.6004 - val_loss: 3.2757 - val_accuracy: 0.6138\n\nEpoch 65/1000                                                                        \n252/252 - 1s - loss: 3.1990 - accuracy: 0.6054 - val_loss: 3.2718 - val_accuracy: 0.6188\n\nEpoch 66/1000                                                                        \n252/252 - 1s - loss: 3.1900 - accuracy: 0.6077 - val_loss: 3.2680 - val_accuracy: 0.6218\n\nEpoch 67/1000                                                                        \n252/252 - 1s - loss: 3.1863 - accuracy: 0.6125 - val_loss: 3.2640 - val_accuracy: 0.6228\n\nEpoch 68/1000                                                                        \n252/252 - 1s - loss: 3.1831 - accuracy: 0.6169 - val_loss: 3.2601 - val_accuracy: 0.6252\n\nEpoch 69/1000                                                                        \n252/252 - 1s - loss: 3.1779 - accuracy: 0.6163 - val_loss: 3.2563 - val_accuracy: 0.6312\n\nEpoch 70/1000                                                                        \n252/252 - 1s - loss: 3.1709 - accuracy: 0.6261 - val_loss: 3.2522 - val_accuracy: 0.6312\n\nEpoch 71/1000                                                                        \n252/252 - 1s - loss: 3.1691 - accuracy: 0.6277 - val_loss: 3.2483 - val_accuracy: 0.6327\n\nEpoch 72/1000                                                                        \n252/252 - 1s - loss: 3.1613 - accuracy: 0.6328 - val_loss: 3.2446 - val_accuracy: 0.6367\n\nEpoch 73/1000                                                                        \n252/252 - 1s - loss: 3.1591 - accuracy: 0.6332 - val_loss: 3.2408 - val_accuracy: 0.6382\n\nEpoch 74/1000                                                                        \n252/252 - 1s - loss: 3.1500 - accuracy: 0.6431 - val_loss: 3.2368 - val_accuracy: 0.6397\n\nEpoch 75/1000                                                                        \n252/252 - 1s - loss: 3.1482 - accuracy: 0.6401 - val_loss: 3.2331 - val_accuracy: 0.6431\n\nEpoch 76/1000                                                                        \n252/252 - 1s - loss: 3.1470 - accuracy: 0.6361 - val_loss: 3.2294 - val_accuracy: 0.6476\n\nEpoch 77/1000                                                                        \n252/252 - 1s - loss: 3.1381 - accuracy: 0.6458 - val_loss: 3.2256 - val_accuracy: 0.6526\n\nEpoch 78/1000                                                                        \n252/252 - 1s - loss: 3.1355 - accuracy: 0.6414 - val_loss: 3.2219 - val_accuracy: 0.6546\n\nEpoch 79/1000                                                                        \n252/252 - 1s - loss: 3.1316 - accuracy: 0.6507 - val_loss: 3.2181 - val_accuracy: 0.6551\n\nEpoch 80/1000                                                                        \n252/252 - 1s - loss: 3.1215 - accuracy: 0.6616 - val_loss: 3.2143 - val_accuracy: 0.6576\n\nEpoch 81/1000                                                                        \n252/252 - 1s - loss: 3.1205 - accuracy: 0.6592 - val_loss: 3.2105 - val_accuracy: 0.6600\n\nEpoch 82/1000                                                                        \n252/252 - 1s - loss: 3.1172 - accuracy: 0.6512 - val_loss: 3.2067 - val_accuracy: 0.6605\n\nEpoch 83/1000                                                                        \n252/252 - 1s - loss: 3.1118 - accuracy: 0.6620 - val_loss: 3.2030 - val_accuracy: 0.6635\n\nEpoch 84/1000                                                                        \n252/252 - 1s - loss: 3.1066 - accuracy: 0.6684 - val_loss: 3.1992 - val_accuracy: 0.6655\n\nEpoch 85/1000                                                                        \n252/252 - 1s - loss: 3.0997 - accuracy: 0.6789 - val_loss: 3.1956 - val_accuracy: 0.6690\n\nEpoch 86/1000                                                                        \n252/252 - 1s - loss: 3.0995 - accuracy: 0.6700 - val_loss: 3.1918 - val_accuracy: 0.6695\n\nEpoch 87/1000                                                                        \n252/252 - 1s - loss: 3.0930 - accuracy: 0.6708 - val_loss: 3.1882 - val_accuracy: 0.6720\n\nEpoch 88/1000                                                                        \n252/252 - 1s - loss: 3.0876 - accuracy: 0.6811 - val_loss: 3.1846 - val_accuracy: 0.6735\n\nEpoch 89/1000                                                                        \n252/252 - 1s - loss: 3.0860 - accuracy: 0.6785 - val_loss: 3.1807 - val_accuracy: 0.6740\n\nEpoch 90/1000                                                                        \n252/252 - 1s - loss: 3.0796 - accuracy: 0.6835 - val_loss: 3.1772 - val_accuracy: 0.6779\n\nEpoch 91/1000                                                                        \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 3.0747 - accuracy: 0.6760 - val_loss: 3.1735 - val_accuracy: 0.6784\n\nEpoch 92/1000                                                                        \n252/252 - 1s - loss: 3.0704 - accuracy: 0.6858 - val_loss: 3.1700 - val_accuracy: 0.6794\n\nEpoch 93/1000                                                                        \n252/252 - 1s - loss: 3.0664 - accuracy: 0.6887 - val_loss: 3.1663 - val_accuracy: 0.6829\n\nEpoch 94/1000                                                                        \n252/252 - 1s - loss: 3.0655 - accuracy: 0.6910 - val_loss: 3.1625 - val_accuracy: 0.6834\n\nEpoch 95/1000                                                                        \n252/252 - 1s - loss: 3.0618 - accuracy: 0.6854 - val_loss: 3.1589 - val_accuracy: 0.6864\n\nEpoch 96/1000                                                                        \n252/252 - 1s - loss: 3.0526 - accuracy: 0.6964 - val_loss: 3.1554 - val_accuracy: 0.6894\n\nEpoch 97/1000                                                                        \n252/252 - 1s - loss: 3.0487 - accuracy: 0.6933 - val_loss: 3.1516 - val_accuracy: 0.6894\n\nEpoch 98/1000                                                                        \n252/252 - 1s - loss: 3.0471 - accuracy: 0.7014 - val_loss: 3.1479 - val_accuracy: 0.6909\n\nEpoch 99/1000                                                                        \n252/252 - 1s - loss: 3.0412 - accuracy: 0.6948 - val_loss: 3.1444 - val_accuracy: 0.6973\n\nEpoch 100/1000                                                                       \n252/252 - 1s - loss: 3.0391 - accuracy: 0.7005 - val_loss: 3.1409 - val_accuracy: 0.6993\n\nEpoch 101/1000                                                                       \n252/252 - 1s - loss: 3.0352 - accuracy: 0.7043 - val_loss: 3.1373 - val_accuracy: 0.7003\n\nEpoch 102/1000                                                                       \n252/252 - 1s - loss: 3.0286 - accuracy: 0.7116 - val_loss: 3.1338 - val_accuracy: 0.7018\n\nEpoch 103/1000                                                                       \n252/252 - 1s - loss: 3.0279 - accuracy: 0.7119 - val_loss: 3.1303 - val_accuracy: 0.7048\n\nEpoch 104/1000                                                                       \n252/252 - 1s - loss: 3.0201 - accuracy: 0.7175 - val_loss: 3.1265 - val_accuracy: 0.7048\n\nEpoch 105/1000                                                                       \n252/252 - 1s - loss: 3.0164 - accuracy: 0.7133 - val_loss: 3.1229 - val_accuracy: 0.7063\n\nEpoch 106/1000                                                                       \n252/252 - 1s - loss: 3.0090 - accuracy: 0.7184 - val_loss: 3.1194 - val_accuracy: 0.7097\n\nEpoch 107/1000                                                                       \n252/252 - 1s - loss: 3.0078 - accuracy: 0.7159 - val_loss: 3.1158 - val_accuracy: 0.7097\n\nEpoch 108/1000                                                                       \n252/252 - 1s - loss: 3.0011 - accuracy: 0.7241 - val_loss: 3.1123 - val_accuracy: 0.7112\n\nEpoch 109/1000                                                                       \n252/252 - 1s - loss: 2.9941 - accuracy: 0.7275 - val_loss: 3.1088 - val_accuracy: 0.7107\n\nEpoch 110/1000                                                                       \n252/252 - 1s - loss: 2.9938 - accuracy: 0.7252 - val_loss: 3.1052 - val_accuracy: 0.7127\n\nEpoch 111/1000                                                                       \n252/252 - 1s - loss: 2.9875 - accuracy: 0.7330 - val_loss: 3.1017 - val_accuracy: 0.7142\n\nEpoch 112/1000                                                                       \n252/252 - 1s - loss: 2.9865 - accuracy: 0.7275 - val_loss: 3.0980 - val_accuracy: 0.7152\n\nEpoch 113/1000                                                                       \n252/252 - 1s - loss: 2.9794 - accuracy: 0.7319 - val_loss: 3.0947 - val_accuracy: 0.7167\n\nEpoch 114/1000                                                                       \n252/252 - 1s - loss: 2.9775 - accuracy: 0.7297 - val_loss: 3.0913 - val_accuracy: 0.7202\n\nEpoch 115/1000                                                                       \n252/252 - 1s - loss: 2.9746 - accuracy: 0.7363 - val_loss: 3.0878 - val_accuracy: 0.7222\n\nEpoch 116/1000                                                                       \n252/252 - 1s - loss: 2.9686 - accuracy: 0.7453 - val_loss: 3.0841 - val_accuracy: 0.7217\n\nEpoch 117/1000                                                                       \n252/252 - 1s - loss: 2.9666 - accuracy: 0.7369 - val_loss: 3.0806 - val_accuracy: 0.7232\n\nEpoch 118/1000                                                                       \n252/252 - 1s - loss: 2.9609 - accuracy: 0.7432 - val_loss: 3.0771 - val_accuracy: 0.7237\n\nEpoch 119/1000                                                                       \n252/252 - 1s - loss: 2.9572 - accuracy: 0.7474 - val_loss: 3.0737 - val_accuracy: 0.7237\n\nEpoch 120/1000                                                                       \n252/252 - 1s - loss: 2.9548 - accuracy: 0.7451 - val_loss: 3.0701 - val_accuracy: 0.7242\n\nEpoch 121/1000                                                                       \n252/252 - 1s - loss: 2.9466 - accuracy: 0.7466 - val_loss: 3.0667 - val_accuracy: 0.7261\n\nEpoch 122/1000                                                                       \n252/252 - 1s - loss: 2.9446 - accuracy: 0.7441 - val_loss: 3.0634 - val_accuracy: 0.7286\n\nEpoch 123/1000                                                                       \n252/252 - 1s - loss: 2.9403 - accuracy: 0.7533 - val_loss: 3.0601 - val_accuracy: 0.7276\n\nEpoch 124/1000                                                                       \n252/252 - 1s - loss: 2.9329 - accuracy: 0.7559 - val_loss: 3.0568 - val_accuracy: 0.7286\n\nEpoch 125/1000                                                                       \n252/252 - 1s - loss: 2.9356 - accuracy: 0.7494 - val_loss: 3.0535 - val_accuracy: 0.7301\n\nEpoch 126/1000                                                                       \n252/252 - 1s - loss: 2.9313 - accuracy: 0.7565 - val_loss: 3.0498 - val_accuracy: 0.7301\n\nEpoch 127/1000                                                                       \n252/252 - 1s - loss: 2.9254 - accuracy: 0.7557 - val_loss: 3.0464 - val_accuracy: 0.7301\n\nEpoch 128/1000                                                                       \n252/252 - 1s - loss: 2.9202 - accuracy: 0.7640 - val_loss: 3.0430 - val_accuracy: 0.7326\n\nEpoch 129/1000                                                                       \n252/252 - 1s - loss: 2.9156 - accuracy: 0.7673 - val_loss: 3.0397 - val_accuracy: 0.7321\n\nEpoch 130/1000                                                                       \n252/252 - 1s - loss: 2.9150 - accuracy: 0.7636 - val_loss: 3.0362 - val_accuracy: 0.7326\n\nEpoch 131/1000                                                                       \n252/252 - 1s - loss: 2.9076 - accuracy: 0.7722 - val_loss: 3.0331 - val_accuracy: 0.7371\n\nEpoch 132/1000                                                                       \n252/252 - 1s - loss: 2.9054 - accuracy: 0.7721 - val_loss: 3.0296 - val_accuracy: 0.7381\n\nEpoch 133/1000                                                                       \n252/252 - 1s - loss: 2.9004 - accuracy: 0.7706 - val_loss: 3.0262 - val_accuracy: 0.7386\n\nEpoch 134/1000                                                                       \n252/252 - 1s - loss: 2.8972 - accuracy: 0.7649 - val_loss: 3.0230 - val_accuracy: 0.7391\n\nEpoch 135/1000                                                                       \n252/252 - 1s - loss: 2.8930 - accuracy: 0.7712 - val_loss: 3.0197 - val_accuracy: 0.7396\n\nEpoch 136/1000                                                                       \n252/252 - 1s - loss: 2.8903 - accuracy: 0.7737 - val_loss: 3.0166 - val_accuracy: 0.7420\n\nEpoch 137/1000                                                                       \n252/252 - 1s - loss: 2.8847 - accuracy: 0.7756 - val_loss: 3.0130 - val_accuracy: 0.7425\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 138/1000                                                                       \n252/252 - 1s - loss: 2.8804 - accuracy: 0.7750 - val_loss: 3.0098 - val_accuracy: 0.7450\n\nEpoch 139/1000                                                                       \n252/252 - 1s - loss: 2.8779 - accuracy: 0.7782 - val_loss: 3.0064 - val_accuracy: 0.7455\n\nEpoch 140/1000                                                                       \n252/252 - 1s - loss: 2.8715 - accuracy: 0.7812 - val_loss: 3.0031 - val_accuracy: 0.7465\n\nEpoch 141/1000                                                                       \n252/252 - 1s - loss: 2.8670 - accuracy: 0.7861 - val_loss: 2.9999 - val_accuracy: 0.7460\n\nEpoch 142/1000                                                                       \n252/252 - 1s - loss: 2.8638 - accuracy: 0.7838 - val_loss: 2.9966 - val_accuracy: 0.7460\n\nEpoch 143/1000                                                                       \n252/252 - 1s - loss: 2.8634 - accuracy: 0.7866 - val_loss: 2.9933 - val_accuracy: 0.7455\n\nEpoch 144/1000                                                                       \n252/252 - 1s - loss: 2.8566 - accuracy: 0.7946 - val_loss: 2.9896 - val_accuracy: 0.7475\n\nEpoch 145/1000                                                                       \n252/252 - 1s - loss: 2.8527 - accuracy: 0.7858 - val_loss: 2.9863 - val_accuracy: 0.7485\n\nEpoch 146/1000                                                                       \n252/252 - 1s - loss: 2.8484 - accuracy: 0.7886 - val_loss: 2.9833 - val_accuracy: 0.7490\n\nEpoch 147/1000                                                                       \n252/252 - 1s - loss: 2.8485 - accuracy: 0.7884 - val_loss: 2.9802 - val_accuracy: 0.7500\n\nEpoch 148/1000                                                                       \n252/252 - 1s - loss: 2.8423 - accuracy: 0.7963 - val_loss: 2.9768 - val_accuracy: 0.7490\n\nEpoch 149/1000                                                                       \n252/252 - 1s - loss: 2.8395 - accuracy: 0.7953 - val_loss: 2.9737 - val_accuracy: 0.7545\n\nEpoch 150/1000                                                                       \n252/252 - 1s - loss: 2.8345 - accuracy: 0.7972 - val_loss: 2.9706 - val_accuracy: 0.7560\n\nEpoch 151/1000                                                                       \n252/252 - 1s - loss: 2.8312 - accuracy: 0.7964 - val_loss: 2.9674 - val_accuracy: 0.7565\n\nEpoch 152/1000                                                                       \n252/252 - 1s - loss: 2.8267 - accuracy: 0.8047 - val_loss: 2.9639 - val_accuracy: 0.7555\n\nEpoch 153/1000                                                                       \n252/252 - 1s - loss: 2.8235 - accuracy: 0.8082 - val_loss: 2.9607 - val_accuracy: 0.7565\n\nEpoch 154/1000                                                                       \n252/252 - 1s - loss: 2.8209 - accuracy: 0.8026 - val_loss: 2.9577 - val_accuracy: 0.7580\n\nEpoch 155/1000                                                                       \n252/252 - 1s - loss: 2.8157 - accuracy: 0.8029 - val_loss: 2.9540 - val_accuracy: 0.7580\n\nEpoch 156/1000                                                                       \n252/252 - 1s - loss: 2.8123 - accuracy: 0.8049 - val_loss: 2.9509 - val_accuracy: 0.7599\n\nEpoch 157/1000                                                                       \n252/252 - 1s - loss: 2.8060 - accuracy: 0.8085 - val_loss: 2.9477 - val_accuracy: 0.7619\n\nEpoch 158/1000                                                                       \n252/252 - 1s - loss: 2.8027 - accuracy: 0.8067 - val_loss: 2.9446 - val_accuracy: 0.7614\n\nEpoch 159/1000                                                                       \n252/252 - 1s - loss: 2.8008 - accuracy: 0.8104 - val_loss: 2.9414 - val_accuracy: 0.7609\n\nEpoch 160/1000                                                                       \n252/252 - 1s - loss: 2.7939 - accuracy: 0.8127 - val_loss: 2.9383 - val_accuracy: 0.7609\n\nEpoch 161/1000                                                                       \n252/252 - 1s - loss: 2.7936 - accuracy: 0.8055 - val_loss: 2.9354 - val_accuracy: 0.7604\n\nEpoch 162/1000                                                                       \n252/252 - 1s - loss: 2.7907 - accuracy: 0.8121 - val_loss: 2.9322 - val_accuracy: 0.7599\n\nEpoch 163/1000                                                                       \n252/252 - 1s - loss: 2.7862 - accuracy: 0.8071 - val_loss: 2.9292 - val_accuracy: 0.7609\n\nEpoch 164/1000                                                                       \n252/252 - 1s - loss: 2.7830 - accuracy: 0.8166 - val_loss: 2.9257 - val_accuracy: 0.7624\n\nEpoch 165/1000                                                                       \n252/252 - 1s - loss: 2.7813 - accuracy: 0.8150 - val_loss: 2.9225 - val_accuracy: 0.7644\n\nEpoch 166/1000                                                                       \n252/252 - 1s - loss: 2.7725 - accuracy: 0.8183 - val_loss: 2.9196 - val_accuracy: 0.7654\n\nEpoch 167/1000                                                                       \n252/252 - 1s - loss: 2.7733 - accuracy: 0.8214 - val_loss: 2.9164 - val_accuracy: 0.7654\n\nEpoch 168/1000                                                                       \n252/252 - 1s - loss: 2.7703 - accuracy: 0.8142 - val_loss: 2.9136 - val_accuracy: 0.7664\n\nEpoch 169/1000                                                                       \n252/252 - 1s - loss: 2.7657 - accuracy: 0.8232 - val_loss: 2.9107 - val_accuracy: 0.7674\n\nEpoch 170/1000                                                                       \n252/252 - 1s - loss: 2.7610 - accuracy: 0.8276 - val_loss: 2.9075 - val_accuracy: 0.7679\n\nEpoch 171/1000                                                                       \n252/252 - 1s - loss: 2.7573 - accuracy: 0.8229 - val_loss: 2.9042 - val_accuracy: 0.7689\n\nEpoch 172/1000                                                                       \n252/252 - 1s - loss: 2.7558 - accuracy: 0.8183 - val_loss: 2.9012 - val_accuracy: 0.7709\n\nEpoch 173/1000                                                                       \n252/252 - 1s - loss: 2.7543 - accuracy: 0.8227 - val_loss: 2.8980 - val_accuracy: 0.7704\n\nEpoch 174/1000                                                                       \n252/252 - 1s - loss: 2.7475 - accuracy: 0.8295 - val_loss: 2.8950 - val_accuracy: 0.7714\n\nEpoch 175/1000                                                                       \n252/252 - 1s - loss: 2.7460 - accuracy: 0.8260 - val_loss: 2.8923 - val_accuracy: 0.7729\n\nEpoch 176/1000                                                                       \n252/252 - 1s - loss: 2.7419 - accuracy: 0.8257 - val_loss: 2.8892 - val_accuracy: 0.7749\n\nEpoch 177/1000                                                                       \n252/252 - 1s - loss: 2.7409 - accuracy: 0.8286 - val_loss: 2.8862 - val_accuracy: 0.7739\n\nEpoch 178/1000                                                                       \n252/252 - 1s - loss: 2.7298 - accuracy: 0.8380 - val_loss: 2.8831 - val_accuracy: 0.7739\n\nEpoch 179/1000                                                                       \n252/252 - 1s - loss: 2.7309 - accuracy: 0.8317 - val_loss: 2.8798 - val_accuracy: 0.7749\n\nEpoch 180/1000                                                                       \n252/252 - 1s - loss: 2.7271 - accuracy: 0.8355 - val_loss: 2.8766 - val_accuracy: 0.7749\n\nEpoch 181/1000                                                                       \n252/252 - 1s - loss: 2.7244 - accuracy: 0.8340 - val_loss: 2.8738 - val_accuracy: 0.7758\n\nEpoch 182/1000                                                                       \n252/252 - 1s - loss: 2.7215 - accuracy: 0.8264 - val_loss: 2.8710 - val_accuracy: 0.7778\n\nEpoch 183/1000                                                                       \n252/252 - 1s - loss: 2.7183 - accuracy: 0.8295 - val_loss: 2.8681 - val_accuracy: 0.7788\n\nEpoch 184/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 2.7126 - accuracy: 0.8402 - val_loss: 2.8649 - val_accuracy: 0.7778\n\nEpoch 185/1000                                                                       \n252/252 - 1s - loss: 2.7090 - accuracy: 0.8366 - val_loss: 2.8619 - val_accuracy: 0.7813\n\nEpoch 186/1000                                                                       \n252/252 - 1s - loss: 2.7046 - accuracy: 0.8392 - val_loss: 2.8588 - val_accuracy: 0.7838\n\nEpoch 187/1000                                                                       \n252/252 - 1s - loss: 2.7014 - accuracy: 0.8418 - val_loss: 2.8556 - val_accuracy: 0.7828\n\nEpoch 188/1000                                                                       \n252/252 - 1s - loss: 2.7007 - accuracy: 0.8392 - val_loss: 2.8529 - val_accuracy: 0.7843\n\nEpoch 189/1000                                                                       \n252/252 - 1s - loss: 2.6954 - accuracy: 0.8436 - val_loss: 2.8501 - val_accuracy: 0.7853\n\nEpoch 190/1000                                                                       \n252/252 - 1s - loss: 2.6931 - accuracy: 0.8386 - val_loss: 2.8472 - val_accuracy: 0.7858\n\nEpoch 191/1000                                                                       \n252/252 - 1s - loss: 2.6879 - accuracy: 0.8496 - val_loss: 2.8441 - val_accuracy: 0.7858\n\nEpoch 192/1000                                                                       \n252/252 - 1s - loss: 2.6854 - accuracy: 0.8442 - val_loss: 2.8410 - val_accuracy: 0.7853\n\nEpoch 193/1000                                                                       \n252/252 - 1s - loss: 2.6849 - accuracy: 0.8420 - val_loss: 2.8381 - val_accuracy: 0.7883\n\nEpoch 194/1000                                                                       \n252/252 - 1s - loss: 2.6795 - accuracy: 0.8492 - val_loss: 2.8354 - val_accuracy: 0.7863\n\nEpoch 195/1000                                                                       \n252/252 - 1s - loss: 2.6758 - accuracy: 0.8447 - val_loss: 2.8325 - val_accuracy: 0.7883\n\nEpoch 196/1000                                                                       \n252/252 - 1s - loss: 2.6746 - accuracy: 0.8470 - val_loss: 2.8296 - val_accuracy: 0.7878\n\nEpoch 197/1000                                                                       \n252/252 - 1s - loss: 2.6677 - accuracy: 0.8472 - val_loss: 2.8270 - val_accuracy: 0.7893\n\nEpoch 198/1000                                                                       \n252/252 - 1s - loss: 2.6654 - accuracy: 0.8504 - val_loss: 2.8237 - val_accuracy: 0.7903\n\nEpoch 199/1000                                                                       \n252/252 - 1s - loss: 2.6630 - accuracy: 0.8501 - val_loss: 2.8209 - val_accuracy: 0.7898\n\nEpoch 200/1000                                                                       \n252/252 - 1s - loss: 2.6574 - accuracy: 0.8580 - val_loss: 2.8178 - val_accuracy: 0.7903\n\nEpoch 201/1000                                                                       \n252/252 - 1s - loss: 2.6552 - accuracy: 0.8583 - val_loss: 2.8149 - val_accuracy: 0.7903\n\nEpoch 202/1000                                                                       \n252/252 - 1s - loss: 2.6516 - accuracy: 0.8540 - val_loss: 2.8125 - val_accuracy: 0.7922\n\nEpoch 203/1000                                                                       \n252/252 - 1s - loss: 2.6497 - accuracy: 0.8565 - val_loss: 2.8094 - val_accuracy: 0.7932\n\nEpoch 204/1000                                                                       \n252/252 - 1s - loss: 2.6463 - accuracy: 0.8521 - val_loss: 2.8064 - val_accuracy: 0.7932\n\nEpoch 205/1000                                                                       \n252/252 - 1s - loss: 2.6426 - accuracy: 0.8544 - val_loss: 2.8032 - val_accuracy: 0.7942\n\nEpoch 206/1000                                                                       \n252/252 - 1s - loss: 2.6401 - accuracy: 0.8540 - val_loss: 2.8007 - val_accuracy: 0.7927\n\nEpoch 207/1000                                                                       \n252/252 - 1s - loss: 2.6383 - accuracy: 0.8526 - val_loss: 2.7979 - val_accuracy: 0.7947\n\nEpoch 208/1000                                                                       \n252/252 - 1s - loss: 2.6347 - accuracy: 0.8592 - val_loss: 2.7950 - val_accuracy: 0.7947\n\nEpoch 209/1000                                                                       \n252/252 - 1s - loss: 2.6296 - accuracy: 0.8587 - val_loss: 2.7919 - val_accuracy: 0.7967\n\nEpoch 210/1000                                                                       \n252/252 - 1s - loss: 2.6262 - accuracy: 0.8623 - val_loss: 2.7890 - val_accuracy: 0.7967\n\nEpoch 211/1000                                                                       \n252/252 - 1s - loss: 2.6254 - accuracy: 0.8578 - val_loss: 2.7865 - val_accuracy: 0.7947\n\nEpoch 212/1000                                                                       \n252/252 - 1s - loss: 2.6220 - accuracy: 0.8591 - val_loss: 2.7839 - val_accuracy: 0.7947\n\nEpoch 213/1000                                                                       \n252/252 - 1s - loss: 2.6181 - accuracy: 0.8644 - val_loss: 2.7806 - val_accuracy: 0.7972\n\nEpoch 214/1000                                                                       \n252/252 - 1s - loss: 2.6132 - accuracy: 0.8645 - val_loss: 2.7778 - val_accuracy: 0.7977\n\nEpoch 215/1000                                                                       \n252/252 - 1s - loss: 2.6102 - accuracy: 0.8653 - val_loss: 2.7750 - val_accuracy: 0.7977\n\nEpoch 216/1000                                                                       \n252/252 - 1s - loss: 2.6093 - accuracy: 0.8640 - val_loss: 2.7723 - val_accuracy: 0.7972\n\nEpoch 217/1000                                                                       \n252/252 - 1s - loss: 2.6021 - accuracy: 0.8688 - val_loss: 2.7693 - val_accuracy: 0.7972\n\nEpoch 218/1000                                                                       \n252/252 - 1s - loss: 2.5989 - accuracy: 0.8705 - val_loss: 2.7666 - val_accuracy: 0.7972\n\nEpoch 219/1000                                                                       \n252/252 - 1s - loss: 2.5978 - accuracy: 0.8679 - val_loss: 2.7638 - val_accuracy: 0.7977\n\nEpoch 220/1000                                                                       \n252/252 - 1s - loss: 2.5972 - accuracy: 0.8664 - val_loss: 2.7611 - val_accuracy: 0.7977\n\nEpoch 221/1000                                                                       \n252/252 - 1s - loss: 2.5920 - accuracy: 0.8665 - val_loss: 2.7585 - val_accuracy: 0.7972\n\nEpoch 222/1000                                                                       \n252/252 - 1s - loss: 2.5870 - accuracy: 0.8707 - val_loss: 2.7555 - val_accuracy: 0.7982\n\nEpoch 223/1000                                                                       \n252/252 - 1s - loss: 2.5878 - accuracy: 0.8691 - val_loss: 2.7527 - val_accuracy: 0.7982\n\nEpoch 224/1000                                                                       \n252/252 - 1s - loss: 2.5831 - accuracy: 0.8679 - val_loss: 2.7502 - val_accuracy: 0.8022\n\nEpoch 225/1000                                                                       \n252/252 - 1s - loss: 2.5799 - accuracy: 0.8735 - val_loss: 2.7474 - val_accuracy: 0.8032\n\nEpoch 226/1000                                                                       \n252/252 - 1s - loss: 2.5771 - accuracy: 0.8726 - val_loss: 2.7450 - val_accuracy: 0.8012\n\nEpoch 227/1000                                                                       \n252/252 - 1s - loss: 2.5738 - accuracy: 0.8779 - val_loss: 2.7416 - val_accuracy: 0.8017\n\nEpoch 228/1000                                                                       \n252/252 - 1s - loss: 2.5702 - accuracy: 0.8731 - val_loss: 2.7392 - val_accuracy: 0.8027\n\nEpoch 229/1000                                                                       \n252/252 - 1s - loss: 2.5682 - accuracy: 0.8746 - val_loss: 2.7366 - val_accuracy: 0.8027\n\nEpoch 230/1000                                                                       \n252/252 - 1s - loss: 2.5608 - accuracy: 0.8731 - val_loss: 2.7339 - val_accuracy: 0.8022\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 231/1000                                                                       \n252/252 - 1s - loss: 2.5624 - accuracy: 0.8731 - val_loss: 2.7311 - val_accuracy: 0.8032\n\nEpoch 232/1000                                                                       \n252/252 - 1s - loss: 2.5592 - accuracy: 0.8761 - val_loss: 2.7281 - val_accuracy: 0.8042\n\nEpoch 233/1000                                                                       \n252/252 - 1s - loss: 2.5529 - accuracy: 0.8776 - val_loss: 2.7256 - val_accuracy: 0.8047\n\nEpoch 234/1000                                                                       \n252/252 - 1s - loss: 2.5528 - accuracy: 0.8726 - val_loss: 2.7227 - val_accuracy: 0.8067\n\nEpoch 235/1000                                                                       \n252/252 - 1s - loss: 2.5490 - accuracy: 0.8763 - val_loss: 2.7199 - val_accuracy: 0.8072\n\nEpoch 236/1000                                                                       \n252/252 - 1s - loss: 2.5451 - accuracy: 0.8815 - val_loss: 2.7176 - val_accuracy: 0.8072\n\nEpoch 237/1000                                                                       \n252/252 - 1s - loss: 2.5451 - accuracy: 0.8779 - val_loss: 2.7150 - val_accuracy: 0.8072\n\nEpoch 238/1000                                                                       \n252/252 - 1s - loss: 2.5412 - accuracy: 0.8777 - val_loss: 2.7124 - val_accuracy: 0.8077\n\nEpoch 239/1000                                                                       \n252/252 - 1s - loss: 2.5364 - accuracy: 0.8829 - val_loss: 2.7093 - val_accuracy: 0.8082\n\nEpoch 240/1000                                                                       \n252/252 - 1s - loss: 2.5350 - accuracy: 0.8821 - val_loss: 2.7073 - val_accuracy: 0.8072\n\nEpoch 241/1000                                                                       \n252/252 - 1s - loss: 2.5312 - accuracy: 0.8854 - val_loss: 2.7045 - val_accuracy: 0.8091\n\nEpoch 242/1000                                                                       \n252/252 - 1s - loss: 2.5289 - accuracy: 0.8833 - val_loss: 2.7015 - val_accuracy: 0.8086\n\nEpoch 243/1000                                                                       \n252/252 - 1s - loss: 2.5257 - accuracy: 0.8823 - val_loss: 2.6989 - val_accuracy: 0.8091\n\nEpoch 244/1000                                                                       \n252/252 - 1s - loss: 2.5213 - accuracy: 0.8812 - val_loss: 2.6961 - val_accuracy: 0.8091\n\nEpoch 245/1000                                                                       \n252/252 - 1s - loss: 2.5218 - accuracy: 0.8794 - val_loss: 2.6939 - val_accuracy: 0.8086\n\nEpoch 246/1000                                                                       \n252/252 - 1s - loss: 2.5148 - accuracy: 0.8852 - val_loss: 2.6910 - val_accuracy: 0.8096\n\nEpoch 247/1000                                                                       \n252/252 - 1s - loss: 2.5145 - accuracy: 0.8871 - val_loss: 2.6882 - val_accuracy: 0.8101\n\nEpoch 248/1000                                                                       \n252/252 - 1s - loss: 2.5092 - accuracy: 0.8871 - val_loss: 2.6858 - val_accuracy: 0.8101\n\nEpoch 249/1000                                                                       \n252/252 - 1s - loss: 2.5062 - accuracy: 0.8890 - val_loss: 2.6835 - val_accuracy: 0.8106\n\nEpoch 250/1000                                                                       \n252/252 - 1s - loss: 2.5076 - accuracy: 0.8872 - val_loss: 2.6807 - val_accuracy: 0.8116\n\nEpoch 251/1000                                                                       \n252/252 - 1s - loss: 2.5025 - accuracy: 0.8890 - val_loss: 2.6782 - val_accuracy: 0.8121\n\nEpoch 252/1000                                                                       \n252/252 - 1s - loss: 2.5001 - accuracy: 0.8927 - val_loss: 2.6755 - val_accuracy: 0.8121\n\nEpoch 253/1000                                                                       \n252/252 - 1s - loss: 2.4972 - accuracy: 0.8864 - val_loss: 2.6730 - val_accuracy: 0.8121\n\nEpoch 254/1000                                                                       \n252/252 - 1s - loss: 2.4939 - accuracy: 0.8871 - val_loss: 2.6702 - val_accuracy: 0.8131\n\nEpoch 255/1000                                                                       \n252/252 - 1s - loss: 2.4924 - accuracy: 0.8897 - val_loss: 2.6674 - val_accuracy: 0.8136\n\nEpoch 256/1000                                                                       \n252/252 - 1s - loss: 2.4892 - accuracy: 0.8885 - val_loss: 2.6650 - val_accuracy: 0.8126\n\nEpoch 257/1000                                                                       \n252/252 - 1s - loss: 2.4841 - accuracy: 0.8913 - val_loss: 2.6622 - val_accuracy: 0.8136\n\nEpoch 258/1000                                                                       \n252/252 - 1s - loss: 2.4814 - accuracy: 0.8949 - val_loss: 2.6600 - val_accuracy: 0.8136\n\nEpoch 259/1000                                                                       \n252/252 - 1s - loss: 2.4797 - accuracy: 0.8948 - val_loss: 2.6574 - val_accuracy: 0.8141\n\nEpoch 260/1000                                                                       \n252/252 - 1s - loss: 2.4765 - accuracy: 0.8923 - val_loss: 2.6551 - val_accuracy: 0.8131\n\nEpoch 261/1000                                                                       \n252/252 - 1s - loss: 2.4719 - accuracy: 0.8917 - val_loss: 2.6525 - val_accuracy: 0.8156\n\nEpoch 262/1000                                                                       \n252/252 - 1s - loss: 2.4697 - accuracy: 0.8954 - val_loss: 2.6497 - val_accuracy: 0.8156\n\nEpoch 263/1000                                                                       \n252/252 - 1s - loss: 2.4662 - accuracy: 0.8922 - val_loss: 2.6472 - val_accuracy: 0.8166\n\nEpoch 264/1000                                                                       \n252/252 - 1s - loss: 2.4654 - accuracy: 0.8948 - val_loss: 2.6447 - val_accuracy: 0.8166\n\nEpoch 265/1000                                                                       \n252/252 - 1s - loss: 2.4669 - accuracy: 0.8937 - val_loss: 2.6422 - val_accuracy: 0.8181\n\nEpoch 266/1000                                                                       \n252/252 - 1s - loss: 2.4624 - accuracy: 0.8917 - val_loss: 2.6399 - val_accuracy: 0.8176\n\nEpoch 267/1000                                                                       \n252/252 - 1s - loss: 2.4575 - accuracy: 0.8998 - val_loss: 2.6370 - val_accuracy: 0.8171\n\nEpoch 268/1000                                                                       \n252/252 - 1s - loss: 2.4524 - accuracy: 0.8939 - val_loss: 2.6348 - val_accuracy: 0.8191\n\nEpoch 269/1000                                                                       \n252/252 - 1s - loss: 2.4499 - accuracy: 0.8968 - val_loss: 2.6325 - val_accuracy: 0.8186\n\nEpoch 270/1000                                                                       \n252/252 - 1s - loss: 2.4480 - accuracy: 0.8999 - val_loss: 2.6296 - val_accuracy: 0.8181\n\nEpoch 271/1000                                                                       \n252/252 - 1s - loss: 2.4483 - accuracy: 0.8951 - val_loss: 2.6269 - val_accuracy: 0.8186\n\nEpoch 272/1000                                                                       \n252/252 - 1s - loss: 2.4432 - accuracy: 0.8979 - val_loss: 2.6246 - val_accuracy: 0.8196\n\nEpoch 273/1000                                                                       \n252/252 - 1s - loss: 2.4425 - accuracy: 0.9013 - val_loss: 2.6219 - val_accuracy: 0.8191\n\nEpoch 274/1000                                                                       \n252/252 - 1s - loss: 2.4386 - accuracy: 0.8984 - val_loss: 2.6191 - val_accuracy: 0.8201\n\nEpoch 275/1000                                                                       \n252/252 - 1s - loss: 2.4342 - accuracy: 0.8991 - val_loss: 2.6168 - val_accuracy: 0.8201\n\nEpoch 276/1000                                                                       \n252/252 - 1s - loss: 2.4336 - accuracy: 0.8963 - val_loss: 2.6142 - val_accuracy: 0.8206\n\nEpoch 277/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 2.4288 - accuracy: 0.8988 - val_loss: 2.6121 - val_accuracy: 0.8196\n\nEpoch 278/1000                                                                       \n252/252 - 1s - loss: 2.4253 - accuracy: 0.9030 - val_loss: 2.6101 - val_accuracy: 0.8191\n\nEpoch 279/1000                                                                       \n252/252 - 1s - loss: 2.4270 - accuracy: 0.8991 - val_loss: 2.6073 - val_accuracy: 0.8196\n\nEpoch 280/1000                                                                       \n252/252 - 1s - loss: 2.4217 - accuracy: 0.9037 - val_loss: 2.6050 - val_accuracy: 0.8196\n\nEpoch 281/1000                                                                       \n252/252 - 1s - loss: 2.4205 - accuracy: 0.9042 - val_loss: 2.6022 - val_accuracy: 0.8196\n\nEpoch 282/1000                                                                       \n252/252 - 1s - loss: 2.4155 - accuracy: 0.9047 - val_loss: 2.6001 - val_accuracy: 0.8206\n\nEpoch 283/1000                                                                       \n252/252 - 1s - loss: 2.4146 - accuracy: 0.9010 - val_loss: 2.5982 - val_accuracy: 0.8206\n\nEpoch 284/1000                                                                       \n252/252 - 1s - loss: 2.4089 - accuracy: 0.9041 - val_loss: 2.5956 - val_accuracy: 0.8226\n\nEpoch 285/1000                                                                       \n252/252 - 1s - loss: 2.4077 - accuracy: 0.9089 - val_loss: 2.5930 - val_accuracy: 0.8216\n\nEpoch 286/1000                                                                       \n252/252 - 1s - loss: 2.4037 - accuracy: 0.9092 - val_loss: 2.5901 - val_accuracy: 0.8216\n\nEpoch 287/1000                                                                       \n252/252 - 1s - loss: 2.4023 - accuracy: 0.9060 - val_loss: 2.5878 - val_accuracy: 0.8221\n\nEpoch 288/1000                                                                       \n252/252 - 1s - loss: 2.4018 - accuracy: 0.9071 - val_loss: 2.5851 - val_accuracy: 0.8221\n\nEpoch 289/1000                                                                       \n252/252 - 1s - loss: 2.3976 - accuracy: 0.9050 - val_loss: 2.5828 - val_accuracy: 0.8236\n\nEpoch 290/1000                                                                       \n252/252 - 1s - loss: 2.3952 - accuracy: 0.9031 - val_loss: 2.5807 - val_accuracy: 0.8246\n\nEpoch 291/1000                                                                       \n252/252 - 1s - loss: 2.3933 - accuracy: 0.9077 - val_loss: 2.5786 - val_accuracy: 0.8255\n\nEpoch 292/1000                                                                       \n252/252 - 1s - loss: 2.3897 - accuracy: 0.9063 - val_loss: 2.5759 - val_accuracy: 0.8265\n\nEpoch 293/1000                                                                       \n252/252 - 1s - loss: 2.3883 - accuracy: 0.9070 - val_loss: 2.5732 - val_accuracy: 0.8270\n\nEpoch 294/1000                                                                       \n252/252 - 1s - loss: 2.3850 - accuracy: 0.9055 - val_loss: 2.5705 - val_accuracy: 0.8270\n\nEpoch 295/1000                                                                       \n252/252 - 1s - loss: 2.3795 - accuracy: 0.9109 - val_loss: 2.5686 - val_accuracy: 0.8280\n\nEpoch 296/1000                                                                       \n252/252 - 1s - loss: 2.3817 - accuracy: 0.9075 - val_loss: 2.5660 - val_accuracy: 0.8270\n\nEpoch 297/1000                                                                       \n252/252 - 1s - loss: 2.3774 - accuracy: 0.9113 - val_loss: 2.5639 - val_accuracy: 0.8270\n\nEpoch 298/1000                                                                       \n252/252 - 1s - loss: 2.3770 - accuracy: 0.9094 - val_loss: 2.5614 - val_accuracy: 0.8305\n\nEpoch 299/1000                                                                       \n252/252 - 1s - loss: 2.3723 - accuracy: 0.9070 - val_loss: 2.5592 - val_accuracy: 0.8310\n\nEpoch 300/1000                                                                       \n252/252 - 1s - loss: 2.3687 - accuracy: 0.9088 - val_loss: 2.5570 - val_accuracy: 0.8315\n\nEpoch 301/1000                                                                       \n252/252 - 1s - loss: 2.3698 - accuracy: 0.9091 - val_loss: 2.5547 - val_accuracy: 0.8325\n\nEpoch 302/1000                                                                       \n252/252 - 1s - loss: 2.3637 - accuracy: 0.9128 - val_loss: 2.5524 - val_accuracy: 0.8335\n\nEpoch 303/1000                                                                       \n252/252 - 1s - loss: 2.3634 - accuracy: 0.9096 - val_loss: 2.5498 - val_accuracy: 0.8310\n\nEpoch 304/1000                                                                       \n252/252 - 1s - loss: 2.3584 - accuracy: 0.9114 - val_loss: 2.5472 - val_accuracy: 0.8325\n\nEpoch 305/1000                                                                       \n252/252 - 1s - loss: 2.3584 - accuracy: 0.9120 - val_loss: 2.5446 - val_accuracy: 0.8325\n\nEpoch 306/1000                                                                       \n252/252 - 1s - loss: 2.3566 - accuracy: 0.9093 - val_loss: 2.5425 - val_accuracy: 0.8340\n\nEpoch 307/1000                                                                       \n252/252 - 1s - loss: 2.3539 - accuracy: 0.9099 - val_loss: 2.5400 - val_accuracy: 0.8340\n\nEpoch 308/1000                                                                       \n252/252 - 1s - loss: 2.3501 - accuracy: 0.9122 - val_loss: 2.5377 - val_accuracy: 0.8340\n\nEpoch 309/1000                                                                       \n252/252 - 1s - loss: 2.3465 - accuracy: 0.9154 - val_loss: 2.5352 - val_accuracy: 0.8340\n\nEpoch 310/1000                                                                       \n252/252 - 1s - loss: 2.3441 - accuracy: 0.9138 - val_loss: 2.5328 - val_accuracy: 0.8345\n\nEpoch 311/1000                                                                       \n252/252 - 1s - loss: 2.3411 - accuracy: 0.9186 - val_loss: 2.5303 - val_accuracy: 0.8350\n\nEpoch 312/1000                                                                       \n252/252 - 1s - loss: 2.3380 - accuracy: 0.9163 - val_loss: 2.5284 - val_accuracy: 0.8360\n\nEpoch 313/1000                                                                       \n252/252 - 1s - loss: 2.3376 - accuracy: 0.9166 - val_loss: 2.5261 - val_accuracy: 0.8360\n\nEpoch 314/1000                                                                       \n252/252 - 1s - loss: 2.3348 - accuracy: 0.9169 - val_loss: 2.5241 - val_accuracy: 0.8360\n\nEpoch 315/1000                                                                       \n252/252 - 1s - loss: 2.3315 - accuracy: 0.9186 - val_loss: 2.5219 - val_accuracy: 0.8360\n\nEpoch 316/1000                                                                       \n252/252 - 1s - loss: 2.3305 - accuracy: 0.9174 - val_loss: 2.5197 - val_accuracy: 0.8355\n\nEpoch 317/1000                                                                       \n252/252 - 1s - loss: 2.3266 - accuracy: 0.9184 - val_loss: 2.5172 - val_accuracy: 0.8370\n\nEpoch 318/1000                                                                       \n252/252 - 1s - loss: 2.3264 - accuracy: 0.9137 - val_loss: 2.5153 - val_accuracy: 0.8380\n\nEpoch 319/1000                                                                       \n252/252 - 1s - loss: 2.3235 - accuracy: 0.9156 - val_loss: 2.5134 - val_accuracy: 0.8380\n\nEpoch 320/1000                                                                       \n252/252 - 1s - loss: 2.3208 - accuracy: 0.9161 - val_loss: 2.5103 - val_accuracy: 0.8395\n\nEpoch 321/1000                                                                       \n252/252 - 1s - loss: 2.3167 - accuracy: 0.9159 - val_loss: 2.5079 - val_accuracy: 0.8395\n\nEpoch 322/1000                                                                       \n252/252 - 1s - loss: 2.3150 - accuracy: 0.9145 - val_loss: 2.5056 - val_accuracy: 0.8395\n\nEpoch 323/1000                                                                       \n252/252 - 1s - loss: 2.3142 - accuracy: 0.9179 - val_loss: 2.5037 - val_accuracy: 0.8390\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 324/1000                                                                       \n252/252 - 1s - loss: 2.3114 - accuracy: 0.9201 - val_loss: 2.5015 - val_accuracy: 0.8390\n\nEpoch 325/1000                                                                       \n252/252 - 1s - loss: 2.3081 - accuracy: 0.9171 - val_loss: 2.4988 - val_accuracy: 0.8400\n\nEpoch 326/1000                                                                       \n252/252 - 1s - loss: 2.3067 - accuracy: 0.9161 - val_loss: 2.4970 - val_accuracy: 0.8400\n\nEpoch 327/1000                                                                       \n252/252 - 1s - loss: 2.3032 - accuracy: 0.9190 - val_loss: 2.4950 - val_accuracy: 0.8410\n\nEpoch 328/1000                                                                       \n252/252 - 1s - loss: 2.2985 - accuracy: 0.9228 - val_loss: 2.4927 - val_accuracy: 0.8405\n\nEpoch 329/1000                                                                       \n252/252 - 1s - loss: 2.2991 - accuracy: 0.9178 - val_loss: 2.4902 - val_accuracy: 0.8410\n\nEpoch 330/1000                                                                       \n252/252 - 1s - loss: 2.2965 - accuracy: 0.9204 - val_loss: 2.4881 - val_accuracy: 0.8400\n\nEpoch 331/1000                                                                       \n252/252 - 1s - loss: 2.2951 - accuracy: 0.9184 - val_loss: 2.4859 - val_accuracy: 0.8410\n\nEpoch 332/1000                                                                       \n252/252 - 1s - loss: 2.2913 - accuracy: 0.9232 - val_loss: 2.4839 - val_accuracy: 0.8415\n\nEpoch 333/1000                                                                       \n252/252 - 1s - loss: 2.2877 - accuracy: 0.9274 - val_loss: 2.4816 - val_accuracy: 0.8415\n\nEpoch 334/1000                                                                       \n252/252 - 1s - loss: 2.2858 - accuracy: 0.9196 - val_loss: 2.4795 - val_accuracy: 0.8419\n\nEpoch 335/1000                                                                       \n252/252 - 1s - loss: 2.2853 - accuracy: 0.9230 - val_loss: 2.4770 - val_accuracy: 0.8429\n\nEpoch 336/1000                                                                       \n252/252 - 1s - loss: 2.2820 - accuracy: 0.9217 - val_loss: 2.4747 - val_accuracy: 0.8429\n\nEpoch 337/1000                                                                       \n252/252 - 1s - loss: 2.2788 - accuracy: 0.9238 - val_loss: 2.4728 - val_accuracy: 0.8424\n\nEpoch 338/1000                                                                       \n252/252 - 1s - loss: 2.2772 - accuracy: 0.9236 - val_loss: 2.4701 - val_accuracy: 0.8434\n\nEpoch 339/1000                                                                       \n252/252 - 1s - loss: 2.2744 - accuracy: 0.9209 - val_loss: 2.4680 - val_accuracy: 0.8449\n\nEpoch 340/1000                                                                       \n252/252 - 1s - loss: 2.2738 - accuracy: 0.9232 - val_loss: 2.4659 - val_accuracy: 0.8439\n\nEpoch 341/1000                                                                       \n252/252 - 1s - loss: 2.2698 - accuracy: 0.9251 - val_loss: 2.4636 - val_accuracy: 0.8449\n\nEpoch 342/1000                                                                       \n252/252 - 1s - loss: 2.2679 - accuracy: 0.9276 - val_loss: 2.4614 - val_accuracy: 0.8459\n\nEpoch 343/1000                                                                       \n252/252 - 1s - loss: 2.2673 - accuracy: 0.9235 - val_loss: 2.4595 - val_accuracy: 0.8464\n\nEpoch 344/1000                                                                       \n252/252 - 1s - loss: 2.2646 - accuracy: 0.9243 - val_loss: 2.4576 - val_accuracy: 0.8464\n\nEpoch 345/1000                                                                       \n252/252 - 1s - loss: 2.2611 - accuracy: 0.9266 - val_loss: 2.4554 - val_accuracy: 0.8454\n\nEpoch 346/1000                                                                       \n252/252 - 1s - loss: 2.2581 - accuracy: 0.9249 - val_loss: 2.4529 - val_accuracy: 0.8464\n\nEpoch 347/1000                                                                       \n252/252 - 1s - loss: 2.2546 - accuracy: 0.9279 - val_loss: 2.4510 - val_accuracy: 0.8459\n\nEpoch 348/1000                                                                       \n252/252 - 1s - loss: 2.2546 - accuracy: 0.9259 - val_loss: 2.4492 - val_accuracy: 0.8464\n\nEpoch 349/1000                                                                       \n252/252 - 1s - loss: 2.2524 - accuracy: 0.9253 - val_loss: 2.4469 - val_accuracy: 0.8464\n\nEpoch 350/1000                                                                       \n252/252 - 1s - loss: 2.2496 - accuracy: 0.9277 - val_loss: 2.4445 - val_accuracy: 0.8469\n\nEpoch 351/1000                                                                       \n252/252 - 1s - loss: 2.2488 - accuracy: 0.9261 - val_loss: 2.4418 - val_accuracy: 0.8489\n\nEpoch 352/1000                                                                       \n252/252 - 1s - loss: 2.2471 - accuracy: 0.9245 - val_loss: 2.4398 - val_accuracy: 0.8479\n\nEpoch 353/1000                                                                       \n252/252 - 1s - loss: 2.2433 - accuracy: 0.9273 - val_loss: 2.4377 - val_accuracy: 0.8479\n\nEpoch 354/1000                                                                       \n252/252 - 1s - loss: 2.2403 - accuracy: 0.9240 - val_loss: 2.4361 - val_accuracy: 0.8479\n\nEpoch 355/1000                                                                       \n252/252 - 1s - loss: 2.2397 - accuracy: 0.9305 - val_loss: 2.4340 - val_accuracy: 0.8474\n\nEpoch 356/1000                                                                       \n252/252 - 1s - loss: 2.2375 - accuracy: 0.9258 - val_loss: 2.4316 - val_accuracy: 0.8479\n\nEpoch 357/1000                                                                       \n252/252 - 1s - loss: 2.2347 - accuracy: 0.9280 - val_loss: 2.4301 - val_accuracy: 0.8479\n\nEpoch 358/1000                                                                       \n252/252 - 1s - loss: 2.2313 - accuracy: 0.9300 - val_loss: 2.4276 - val_accuracy: 0.8474\n\nEpoch 359/1000                                                                       \n252/252 - 1s - loss: 2.2295 - accuracy: 0.9274 - val_loss: 2.4251 - val_accuracy: 0.8484\n\nEpoch 360/1000                                                                       \n252/252 - 1s - loss: 2.2287 - accuracy: 0.9274 - val_loss: 2.4233 - val_accuracy: 0.8484\n\nEpoch 361/1000                                                                       \n252/252 - 1s - loss: 2.2250 - accuracy: 0.9292 - val_loss: 2.4212 - val_accuracy: 0.8489\n\nEpoch 362/1000                                                                       \n252/252 - 1s - loss: 2.2235 - accuracy: 0.9299 - val_loss: 2.4191 - val_accuracy: 0.8494\n\nEpoch 363/1000                                                                       \n252/252 - 1s - loss: 2.2214 - accuracy: 0.9303 - val_loss: 2.4172 - val_accuracy: 0.8489\n\nEpoch 364/1000                                                                       \n252/252 - 1s - loss: 2.2193 - accuracy: 0.9285 - val_loss: 2.4154 - val_accuracy: 0.8494\n\nEpoch 365/1000                                                                       \n252/252 - 1s - loss: 2.2181 - accuracy: 0.9282 - val_loss: 2.4131 - val_accuracy: 0.8504\n\nEpoch 366/1000                                                                       \n252/252 - 1s - loss: 2.2131 - accuracy: 0.9308 - val_loss: 2.4106 - val_accuracy: 0.8504\n\nEpoch 367/1000                                                                       \n252/252 - 1s - loss: 2.2119 - accuracy: 0.9299 - val_loss: 2.4089 - val_accuracy: 0.8509\n\nEpoch 368/1000                                                                       \n252/252 - 1s - loss: 2.2095 - accuracy: 0.9304 - val_loss: 2.4071 - val_accuracy: 0.8514\n\nEpoch 369/1000                                                                       \n252/252 - 1s - loss: 2.2080 - accuracy: 0.9302 - val_loss: 2.4049 - val_accuracy: 0.8519\n\nEpoch 370/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 2.2050 - accuracy: 0.9318 - val_loss: 2.4030 - val_accuracy: 0.8514\n\nEpoch 371/1000                                                                       \n252/252 - 1s - loss: 2.2017 - accuracy: 0.9334 - val_loss: 2.4006 - val_accuracy: 0.8514\n\nEpoch 372/1000                                                                       \n252/252 - 1s - loss: 2.2009 - accuracy: 0.9314 - val_loss: 2.3985 - val_accuracy: 0.8524\n\nEpoch 373/1000                                                                       \n252/252 - 1s - loss: 2.1980 - accuracy: 0.9302 - val_loss: 2.3966 - val_accuracy: 0.8524\n\nEpoch 374/1000                                                                       \n252/252 - 1s - loss: 2.1969 - accuracy: 0.9331 - val_loss: 2.3943 - val_accuracy: 0.8529\n\nEpoch 375/1000                                                                       \n252/252 - 1s - loss: 2.1933 - accuracy: 0.9330 - val_loss: 2.3926 - val_accuracy: 0.8524\n\nEpoch 376/1000                                                                       \n252/252 - 1s - loss: 2.1933 - accuracy: 0.9316 - val_loss: 2.3906 - val_accuracy: 0.8534\n\nEpoch 377/1000                                                                       \n252/252 - 1s - loss: 2.1924 - accuracy: 0.9325 - val_loss: 2.3887 - val_accuracy: 0.8534\n\nEpoch 378/1000                                                                       \n252/252 - 1s - loss: 2.1913 - accuracy: 0.9309 - val_loss: 2.3867 - val_accuracy: 0.8534\n\nEpoch 379/1000                                                                       \n252/252 - 1s - loss: 2.1902 - accuracy: 0.9300 - val_loss: 2.3847 - val_accuracy: 0.8534\n\nEpoch 380/1000                                                                       \n252/252 - 1s - loss: 2.1847 - accuracy: 0.9316 - val_loss: 2.3823 - val_accuracy: 0.8539\n\nEpoch 381/1000                                                                       \n252/252 - 1s - loss: 2.1832 - accuracy: 0.9325 - val_loss: 2.3804 - val_accuracy: 0.8539\n\nEpoch 382/1000                                                                       \n252/252 - 1s - loss: 2.1803 - accuracy: 0.9330 - val_loss: 2.3788 - val_accuracy: 0.8539\n\nEpoch 383/1000                                                                       \n252/252 - 1s - loss: 2.1806 - accuracy: 0.9331 - val_loss: 2.3765 - val_accuracy: 0.8549\n\nEpoch 384/1000                                                                       \n252/252 - 1s - loss: 2.1767 - accuracy: 0.9343 - val_loss: 2.3746 - val_accuracy: 0.8569\n\nEpoch 385/1000                                                                       \n252/252 - 1s - loss: 2.1753 - accuracy: 0.9396 - val_loss: 2.3724 - val_accuracy: 0.8564\n\nEpoch 386/1000                                                                       \n252/252 - 1s - loss: 2.1721 - accuracy: 0.9341 - val_loss: 2.3701 - val_accuracy: 0.8559\n\nEpoch 387/1000                                                                       \n252/252 - 1s - loss: 2.1709 - accuracy: 0.9344 - val_loss: 2.3685 - val_accuracy: 0.8574\n\nEpoch 388/1000                                                                       \n252/252 - 1s - loss: 2.1667 - accuracy: 0.9352 - val_loss: 2.3665 - val_accuracy: 0.8579\n\nEpoch 389/1000                                                                       \n252/252 - 1s - loss: 2.1655 - accuracy: 0.9343 - val_loss: 2.3643 - val_accuracy: 0.8574\n\nEpoch 390/1000                                                                       \n252/252 - 1s - loss: 2.1651 - accuracy: 0.9310 - val_loss: 2.3624 - val_accuracy: 0.8574\n\nEpoch 391/1000                                                                       \n252/252 - 1s - loss: 2.1630 - accuracy: 0.9324 - val_loss: 2.3603 - val_accuracy: 0.8564\n\nEpoch 392/1000                                                                       \n252/252 - 1s - loss: 2.1595 - accuracy: 0.9314 - val_loss: 2.3584 - val_accuracy: 0.8569\n\nEpoch 393/1000                                                                       \n252/252 - 1s - loss: 2.1596 - accuracy: 0.9356 - val_loss: 2.3561 - val_accuracy: 0.8574\n\nEpoch 394/1000                                                                       \n252/252 - 1s - loss: 2.1557 - accuracy: 0.9362 - val_loss: 2.3539 - val_accuracy: 0.8574\n\nEpoch 395/1000                                                                       \n252/252 - 1s - loss: 2.1562 - accuracy: 0.9377 - val_loss: 2.3526 - val_accuracy: 0.8583\n\nEpoch 396/1000                                                                       \n252/252 - 1s - loss: 2.1520 - accuracy: 0.9388 - val_loss: 2.3508 - val_accuracy: 0.8574\n\nEpoch 397/1000                                                                       \n252/252 - 1s - loss: 2.1495 - accuracy: 0.9378 - val_loss: 2.3484 - val_accuracy: 0.8579\n\nEpoch 398/1000                                                                       \n252/252 - 1s - loss: 2.1485 - accuracy: 0.9361 - val_loss: 2.3467 - val_accuracy: 0.8574\n\nEpoch 399/1000                                                                       \n252/252 - 1s - loss: 2.1481 - accuracy: 0.9356 - val_loss: 2.3448 - val_accuracy: 0.8579\n\nEpoch 400/1000                                                                       \n252/252 - 1s - loss: 2.1465 - accuracy: 0.9374 - val_loss: 2.3424 - val_accuracy: 0.8583\n\nEpoch 401/1000                                                                       \n252/252 - 1s - loss: 2.1423 - accuracy: 0.9367 - val_loss: 2.3406 - val_accuracy: 0.8583\n\nEpoch 402/1000                                                                       \n252/252 - 1s - loss: 2.1410 - accuracy: 0.9374 - val_loss: 2.3386 - val_accuracy: 0.8583\n\nEpoch 403/1000                                                                       \n252/252 - 1s - loss: 2.1387 - accuracy: 0.9362 - val_loss: 2.3366 - val_accuracy: 0.8583\n\nEpoch 404/1000                                                                       \n252/252 - 1s - loss: 2.1345 - accuracy: 0.9378 - val_loss: 2.3349 - val_accuracy: 0.8583\n\nEpoch 405/1000                                                                       \n252/252 - 1s - loss: 2.1348 - accuracy: 0.9355 - val_loss: 2.3326 - val_accuracy: 0.8593\n\nEpoch 406/1000                                                                       \n252/252 - 1s - loss: 2.1305 - accuracy: 0.9386 - val_loss: 2.3310 - val_accuracy: 0.8583\n\nEpoch 407/1000                                                                       \n252/252 - 1s - loss: 2.1288 - accuracy: 0.9396 - val_loss: 2.3293 - val_accuracy: 0.8603\n\nEpoch 408/1000                                                                       \n252/252 - 1s - loss: 2.1274 - accuracy: 0.9391 - val_loss: 2.3273 - val_accuracy: 0.8598\n\nEpoch 409/1000                                                                       \n252/252 - 1s - loss: 2.1267 - accuracy: 0.9392 - val_loss: 2.3254 - val_accuracy: 0.8598\n\nEpoch 410/1000                                                                       \n252/252 - 1s - loss: 2.1249 - accuracy: 0.9393 - val_loss: 2.3232 - val_accuracy: 0.8603\n\nEpoch 411/1000                                                                       \n252/252 - 1s - loss: 2.1225 - accuracy: 0.9403 - val_loss: 2.3221 - val_accuracy: 0.8593\n\nEpoch 412/1000                                                                       \n252/252 - 1s - loss: 2.1221 - accuracy: 0.9377 - val_loss: 2.3198 - val_accuracy: 0.8598\n\nEpoch 413/1000                                                                       \n252/252 - 1s - loss: 2.1182 - accuracy: 0.9402 - val_loss: 2.3180 - val_accuracy: 0.8603\n\nEpoch 414/1000                                                                       \n252/252 - 1s - loss: 2.1160 - accuracy: 0.9396 - val_loss: 2.3162 - val_accuracy: 0.8608\n\nEpoch 415/1000                                                                       \n252/252 - 1s - loss: 2.1171 - accuracy: 0.9370 - val_loss: 2.3141 - val_accuracy: 0.8608\n\nEpoch 416/1000                                                                       \n252/252 - 1s - loss: 2.1112 - accuracy: 0.9398 - val_loss: 2.3126 - val_accuracy: 0.8608\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 417/1000                                                                       \n252/252 - 1s - loss: 2.1096 - accuracy: 0.9416 - val_loss: 2.3106 - val_accuracy: 0.8618\n\nEpoch 418/1000                                                                       \n252/252 - 1s - loss: 2.1106 - accuracy: 0.9382 - val_loss: 2.3085 - val_accuracy: 0.8613\n\nEpoch 419/1000                                                                       \n252/252 - 1s - loss: 2.1073 - accuracy: 0.9391 - val_loss: 2.3063 - val_accuracy: 0.8618\n\nEpoch 420/1000                                                                       \n252/252 - 1s - loss: 2.1053 - accuracy: 0.9401 - val_loss: 2.3050 - val_accuracy: 0.8623\n\nEpoch 421/1000                                                                       \n252/252 - 1s - loss: 2.1057 - accuracy: 0.9386 - val_loss: 2.3035 - val_accuracy: 0.8623\n\nEpoch 422/1000                                                                       \n252/252 - 1s - loss: 2.1023 - accuracy: 0.9369 - val_loss: 2.3017 - val_accuracy: 0.8628\n\nEpoch 423/1000                                                                       \n252/252 - 1s - loss: 2.0996 - accuracy: 0.9391 - val_loss: 2.2991 - val_accuracy: 0.8623\n\nEpoch 424/1000                                                                       \n252/252 - 1s - loss: 2.0960 - accuracy: 0.9458 - val_loss: 2.2977 - val_accuracy: 0.8623\n\nEpoch 425/1000                                                                       \n252/252 - 1s - loss: 2.0983 - accuracy: 0.9419 - val_loss: 2.2961 - val_accuracy: 0.8628\n\nEpoch 426/1000                                                                       \n252/252 - 1s - loss: 2.0937 - accuracy: 0.9400 - val_loss: 2.2935 - val_accuracy: 0.8623\n\nEpoch 427/1000                                                                       \n252/252 - 1s - loss: 2.0894 - accuracy: 0.9428 - val_loss: 2.2916 - val_accuracy: 0.8623\n\nEpoch 428/1000                                                                       \n252/252 - 1s - loss: 2.0884 - accuracy: 0.9436 - val_loss: 2.2895 - val_accuracy: 0.8633\n\nEpoch 429/1000                                                                       \n252/252 - 1s - loss: 2.0874 - accuracy: 0.9427 - val_loss: 2.2884 - val_accuracy: 0.8633\n\nEpoch 430/1000                                                                       \n252/252 - 1s - loss: 2.0888 - accuracy: 0.9427 - val_loss: 2.2861 - val_accuracy: 0.8628\n\nEpoch 431/1000                                                                       \n252/252 - 1s - loss: 2.0835 - accuracy: 0.9431 - val_loss: 2.2844 - val_accuracy: 0.8638\n\nEpoch 432/1000                                                                       \n252/252 - 1s - loss: 2.0836 - accuracy: 0.9413 - val_loss: 2.2830 - val_accuracy: 0.8633\n\nEpoch 433/1000                                                                       \n252/252 - 1s - loss: 2.0814 - accuracy: 0.9419 - val_loss: 2.2810 - val_accuracy: 0.8628\n\nEpoch 434/1000                                                                       \n252/252 - 1s - loss: 2.0778 - accuracy: 0.9439 - val_loss: 2.2794 - val_accuracy: 0.8633\n\nEpoch 435/1000                                                                       \n252/252 - 1s - loss: 2.0778 - accuracy: 0.9412 - val_loss: 2.2775 - val_accuracy: 0.8633\n\nEpoch 436/1000                                                                       \n252/252 - 1s - loss: 2.0762 - accuracy: 0.9434 - val_loss: 2.2758 - val_accuracy: 0.8633\n\nEpoch 437/1000                                                                       \n252/252 - 1s - loss: 2.0736 - accuracy: 0.9432 - val_loss: 2.2742 - val_accuracy: 0.8638\n\nEpoch 438/1000                                                                       \n252/252 - 1s - loss: 2.0730 - accuracy: 0.9433 - val_loss: 2.2721 - val_accuracy: 0.8643\n\nEpoch 439/1000                                                                       \n252/252 - 1s - loss: 2.0706 - accuracy: 0.9441 - val_loss: 2.2705 - val_accuracy: 0.8648\n\nEpoch 440/1000                                                                       \n252/252 - 1s - loss: 2.0674 - accuracy: 0.9454 - val_loss: 2.2688 - val_accuracy: 0.8653\n\nEpoch 441/1000                                                                       \n252/252 - 1s - loss: 2.0677 - accuracy: 0.9429 - val_loss: 2.2668 - val_accuracy: 0.8663\n\nEpoch 442/1000                                                                       \n252/252 - 1s - loss: 2.0659 - accuracy: 0.9455 - val_loss: 2.2648 - val_accuracy: 0.8663\n\nEpoch 443/1000                                                                       \n252/252 - 1s - loss: 2.0633 - accuracy: 0.9438 - val_loss: 2.2624 - val_accuracy: 0.8653\n\nEpoch 444/1000                                                                       \n252/252 - 1s - loss: 2.0606 - accuracy: 0.9447 - val_loss: 2.2612 - val_accuracy: 0.8668\n\nEpoch 445/1000                                                                       \n252/252 - 1s - loss: 2.0592 - accuracy: 0.9431 - val_loss: 2.2601 - val_accuracy: 0.8653\n\nEpoch 446/1000                                                                       \n252/252 - 1s - loss: 2.0576 - accuracy: 0.9419 - val_loss: 2.2579 - val_accuracy: 0.8663\n\nEpoch 447/1000                                                                       \n252/252 - 1s - loss: 2.0566 - accuracy: 0.9447 - val_loss: 2.2563 - val_accuracy: 0.8663\n\nEpoch 448/1000                                                                       \n252/252 - 1s - loss: 2.0549 - accuracy: 0.9448 - val_loss: 2.2539 - val_accuracy: 0.8673\n\nEpoch 449/1000                                                                       \n252/252 - 1s - loss: 2.0530 - accuracy: 0.9423 - val_loss: 2.2525 - val_accuracy: 0.8673\n\nEpoch 450/1000                                                                       \n252/252 - 1s - loss: 2.0498 - accuracy: 0.9444 - val_loss: 2.2509 - val_accuracy: 0.8668\n\nEpoch 451/1000                                                                       \n252/252 - 1s - loss: 2.0493 - accuracy: 0.9441 - val_loss: 2.2495 - val_accuracy: 0.8668\n\nEpoch 452/1000                                                                       \n252/252 - 1s - loss: 2.0468 - accuracy: 0.9478 - val_loss: 2.2475 - val_accuracy: 0.8668\n\nEpoch 453/1000                                                                       \n252/252 - 1s - loss: 2.0435 - accuracy: 0.9458 - val_loss: 2.2456 - val_accuracy: 0.8668\n\nEpoch 454/1000                                                                       \n252/252 - 1s - loss: 2.0452 - accuracy: 0.9429 - val_loss: 2.2441 - val_accuracy: 0.8668\n\nEpoch 455/1000                                                                       \n252/252 - 1s - loss: 2.0424 - accuracy: 0.9449 - val_loss: 2.2422 - val_accuracy: 0.8668\n\nEpoch 456/1000                                                                       \n252/252 - 1s - loss: 2.0377 - accuracy: 0.9454 - val_loss: 2.2401 - val_accuracy: 0.8683\n\nEpoch 457/1000                                                                       \n252/252 - 1s - loss: 2.0368 - accuracy: 0.9458 - val_loss: 2.2381 - val_accuracy: 0.8673\n\nEpoch 458/1000                                                                       \n252/252 - 1s - loss: 2.0349 - accuracy: 0.9480 - val_loss: 2.2369 - val_accuracy: 0.8668\n\nEpoch 459/1000                                                                       \n252/252 - 1s - loss: 2.0334 - accuracy: 0.9463 - val_loss: 2.2352 - val_accuracy: 0.8673\n\nEpoch 460/1000                                                                       \n252/252 - 1s - loss: 2.0337 - accuracy: 0.9460 - val_loss: 2.2332 - val_accuracy: 0.8673\n\nEpoch 461/1000                                                                       \n252/252 - 1s - loss: 2.0314 - accuracy: 0.9465 - val_loss: 2.2321 - val_accuracy: 0.8673\n\nEpoch 462/1000                                                                       \n252/252 - 1s - loss: 2.0272 - accuracy: 0.9468 - val_loss: 2.2304 - val_accuracy: 0.8678\n\nEpoch 463/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 2.0281 - accuracy: 0.9488 - val_loss: 2.2287 - val_accuracy: 0.8678\n\nEpoch 464/1000                                                                       \n252/252 - 1s - loss: 2.0242 - accuracy: 0.9486 - val_loss: 2.2266 - val_accuracy: 0.8688\n\nEpoch 465/1000                                                                       \n252/252 - 1s - loss: 2.0235 - accuracy: 0.9458 - val_loss: 2.2250 - val_accuracy: 0.8693\n\nEpoch 466/1000                                                                       \n252/252 - 1s - loss: 2.0227 - accuracy: 0.9480 - val_loss: 2.2235 - val_accuracy: 0.8688\n\nEpoch 467/1000                                                                       \n252/252 - 1s - loss: 2.0220 - accuracy: 0.9445 - val_loss: 2.2217 - val_accuracy: 0.8688\n\nEpoch 468/1000                                                                       \n252/252 - 1s - loss: 2.0180 - accuracy: 0.9481 - val_loss: 2.2197 - val_accuracy: 0.8698\n\nEpoch 469/1000                                                                       \n252/252 - 1s - loss: 2.0162 - accuracy: 0.9484 - val_loss: 2.2175 - val_accuracy: 0.8693\n\nEpoch 470/1000                                                                       \n252/252 - 1s - loss: 2.0167 - accuracy: 0.9462 - val_loss: 2.2161 - val_accuracy: 0.8698\n\nEpoch 471/1000                                                                       \n252/252 - 1s - loss: 2.0129 - accuracy: 0.9475 - val_loss: 2.2141 - val_accuracy: 0.8693\n\nEpoch 472/1000                                                                       \n252/252 - 1s - loss: 2.0109 - accuracy: 0.9467 - val_loss: 2.2124 - val_accuracy: 0.8698\n\nEpoch 473/1000                                                                       \n252/252 - 1s - loss: 2.0092 - accuracy: 0.9476 - val_loss: 2.2114 - val_accuracy: 0.8708\n\nEpoch 474/1000                                                                       \n252/252 - 1s - loss: 2.0092 - accuracy: 0.9490 - val_loss: 2.2100 - val_accuracy: 0.8718\n\nEpoch 475/1000                                                                       \n252/252 - 1s - loss: 2.0053 - accuracy: 0.9505 - val_loss: 2.2081 - val_accuracy: 0.8708\n\nEpoch 476/1000                                                                       \n252/252 - 1s - loss: 2.0060 - accuracy: 0.9496 - val_loss: 2.2062 - val_accuracy: 0.8698\n\nEpoch 477/1000                                                                       \n252/252 - 1s - loss: 2.0039 - accuracy: 0.9475 - val_loss: 2.2046 - val_accuracy: 0.8693\n\nEpoch 478/1000                                                                       \n252/252 - 1s - loss: 2.0026 - accuracy: 0.9467 - val_loss: 2.2032 - val_accuracy: 0.8713\n\nEpoch 479/1000                                                                       \n252/252 - 1s - loss: 2.0009 - accuracy: 0.9467 - val_loss: 2.2008 - val_accuracy: 0.8703\n\nEpoch 480/1000                                                                       \n252/252 - 1s - loss: 1.9976 - accuracy: 0.9503 - val_loss: 2.2000 - val_accuracy: 0.8718\n\nEpoch 481/1000                                                                       \n252/252 - 1s - loss: 1.9977 - accuracy: 0.9504 - val_loss: 2.1984 - val_accuracy: 0.8718\n\nEpoch 482/1000                                                                       \n252/252 - 1s - loss: 1.9950 - accuracy: 0.9509 - val_loss: 2.1971 - val_accuracy: 0.8708\n\nEpoch 483/1000                                                                       \n252/252 - 1s - loss: 1.9963 - accuracy: 0.9505 - val_loss: 2.1952 - val_accuracy: 0.8708\n\nEpoch 484/1000                                                                       \n252/252 - 1s - loss: 1.9918 - accuracy: 0.9500 - val_loss: 2.1933 - val_accuracy: 0.8713\n\nEpoch 485/1000                                                                       \n252/252 - 1s - loss: 1.9919 - accuracy: 0.9478 - val_loss: 2.1915 - val_accuracy: 0.8713\n\nEpoch 486/1000                                                                       \n252/252 - 1s - loss: 1.9879 - accuracy: 0.9506 - val_loss: 2.1895 - val_accuracy: 0.8718\n\nEpoch 487/1000                                                                       \n252/252 - 1s - loss: 1.9878 - accuracy: 0.9488 - val_loss: 2.1878 - val_accuracy: 0.8718\n\nEpoch 488/1000                                                                       \n252/252 - 1s - loss: 1.9862 - accuracy: 0.9503 - val_loss: 2.1863 - val_accuracy: 0.8718\n\nEpoch 489/1000                                                                       \n252/252 - 1s - loss: 1.9829 - accuracy: 0.9504 - val_loss: 2.1850 - val_accuracy: 0.8708\n\nEpoch 490/1000                                                                       \n252/252 - 1s - loss: 1.9824 - accuracy: 0.9510 - val_loss: 2.1830 - val_accuracy: 0.8723\n\nEpoch 491/1000                                                                       \n252/252 - 1s - loss: 1.9813 - accuracy: 0.9515 - val_loss: 2.1815 - val_accuracy: 0.8723\n\nEpoch 492/1000                                                                       \n252/252 - 1s - loss: 1.9784 - accuracy: 0.9509 - val_loss: 2.1802 - val_accuracy: 0.8723\n\nEpoch 493/1000                                                                       \n252/252 - 1s - loss: 1.9787 - accuracy: 0.9517 - val_loss: 2.1786 - val_accuracy: 0.8723\n\nEpoch 494/1000                                                                       \n252/252 - 1s - loss: 1.9776 - accuracy: 0.9486 - val_loss: 2.1762 - val_accuracy: 0.8728\n\nEpoch 495/1000                                                                       \n252/252 - 1s - loss: 1.9760 - accuracy: 0.9489 - val_loss: 2.1751 - val_accuracy: 0.8723\n\nEpoch 496/1000                                                                       \n252/252 - 1s - loss: 1.9735 - accuracy: 0.9505 - val_loss: 2.1733 - val_accuracy: 0.8728\n\nEpoch 497/1000                                                                       \n252/252 - 1s - loss: 1.9696 - accuracy: 0.9521 - val_loss: 2.1717 - val_accuracy: 0.8728\n\nEpoch 498/1000                                                                       \n252/252 - 1s - loss: 1.9682 - accuracy: 0.9511 - val_loss: 2.1703 - val_accuracy: 0.8733\n\nEpoch 499/1000                                                                       \n252/252 - 1s - loss: 1.9693 - accuracy: 0.9510 - val_loss: 2.1684 - val_accuracy: 0.8733\n\nEpoch 500/1000                                                                       \n252/252 - 1s - loss: 1.9668 - accuracy: 0.9498 - val_loss: 2.1672 - val_accuracy: 0.8733\n\nEpoch 501/1000                                                                       \n252/252 - 1s - loss: 1.9655 - accuracy: 0.9514 - val_loss: 2.1658 - val_accuracy: 0.8738\n\nEpoch 502/1000                                                                       \n252/252 - 1s - loss: 1.9609 - accuracy: 0.9522 - val_loss: 2.1638 - val_accuracy: 0.8743\n\nEpoch 503/1000                                                                       \n252/252 - 1s - loss: 1.9612 - accuracy: 0.9530 - val_loss: 2.1617 - val_accuracy: 0.8752\n\nEpoch 504/1000                                                                       \n252/252 - 1s - loss: 1.9599 - accuracy: 0.9526 - val_loss: 2.1603 - val_accuracy: 0.8748\n\nEpoch 505/1000                                                                       \n252/252 - 1s - loss: 1.9595 - accuracy: 0.9524 - val_loss: 2.1591 - val_accuracy: 0.8743\n\nEpoch 506/1000                                                                       \n252/252 - 1s - loss: 1.9561 - accuracy: 0.9521 - val_loss: 2.1579 - val_accuracy: 0.8738\n\nEpoch 507/1000                                                                       \n252/252 - 1s - loss: 1.9550 - accuracy: 0.9522 - val_loss: 2.1558 - val_accuracy: 0.8748\n\nEpoch 508/1000                                                                       \n252/252 - 1s - loss: 1.9546 - accuracy: 0.9506 - val_loss: 2.1543 - val_accuracy: 0.8743\n\nEpoch 509/1000                                                                       \n252/252 - 1s - loss: 1.9525 - accuracy: 0.9529 - val_loss: 2.1524 - val_accuracy: 0.8752\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 510/1000                                                                       \n252/252 - 1s - loss: 1.9516 - accuracy: 0.9505 - val_loss: 2.1504 - val_accuracy: 0.8767\n\nEpoch 511/1000                                                                       \n252/252 - 1s - loss: 1.9488 - accuracy: 0.9532 - val_loss: 2.1492 - val_accuracy: 0.8752\n\nEpoch 512/1000                                                                       \n252/252 - 1s - loss: 1.9467 - accuracy: 0.9529 - val_loss: 2.1475 - val_accuracy: 0.8772\n\nEpoch 513/1000                                                                       \n252/252 - 1s - loss: 1.9452 - accuracy: 0.9561 - val_loss: 2.1466 - val_accuracy: 0.8748\n\nEpoch 514/1000                                                                       \n252/252 - 1s - loss: 1.9438 - accuracy: 0.9536 - val_loss: 2.1454 - val_accuracy: 0.8733\n\nEpoch 515/1000                                                                       \n252/252 - 1s - loss: 1.9436 - accuracy: 0.9534 - val_loss: 2.1442 - val_accuracy: 0.8733\n\nEpoch 516/1000                                                                       \n252/252 - 1s - loss: 1.9417 - accuracy: 0.9524 - val_loss: 2.1427 - val_accuracy: 0.8738\n\nEpoch 517/1000                                                                       \n252/252 - 1s - loss: 1.9397 - accuracy: 0.9543 - val_loss: 2.1411 - val_accuracy: 0.8738\n\nEpoch 518/1000                                                                       \n252/252 - 1s - loss: 1.9389 - accuracy: 0.9557 - val_loss: 2.1394 - val_accuracy: 0.8738\n\nEpoch 519/1000                                                                       \n252/252 - 1s - loss: 1.9376 - accuracy: 0.9540 - val_loss: 2.1378 - val_accuracy: 0.8743\n\nEpoch 520/1000                                                                       \n252/252 - 1s - loss: 1.9352 - accuracy: 0.9548 - val_loss: 2.1356 - val_accuracy: 0.8767\n\nEpoch 521/1000                                                                       \n252/252 - 1s - loss: 1.9361 - accuracy: 0.9530 - val_loss: 2.1347 - val_accuracy: 0.8748\n\nEpoch 522/1000                                                                       \n252/252 - 1s - loss: 1.9312 - accuracy: 0.9551 - val_loss: 2.1329 - val_accuracy: 0.8757\n\nEpoch 523/1000                                                                       \n252/252 - 1s - loss: 1.9297 - accuracy: 0.9561 - val_loss: 2.1313 - val_accuracy: 0.8767\n\nEpoch 524/1000                                                                       \n252/252 - 1s - loss: 1.9293 - accuracy: 0.9534 - val_loss: 2.1297 - val_accuracy: 0.8777\n\nEpoch 525/1000                                                                       \n252/252 - 1s - loss: 1.9290 - accuracy: 0.9540 - val_loss: 2.1287 - val_accuracy: 0.8757\n\nEpoch 526/1000                                                                       \n252/252 - 1s - loss: 1.9261 - accuracy: 0.9547 - val_loss: 2.1268 - val_accuracy: 0.8777\n\nEpoch 527/1000                                                                       \n252/252 - 1s - loss: 1.9245 - accuracy: 0.9514 - val_loss: 2.1251 - val_accuracy: 0.8782\n\nEpoch 528/1000                                                                       \n252/252 - 1s - loss: 1.9236 - accuracy: 0.9552 - val_loss: 2.1237 - val_accuracy: 0.8777\n\nEpoch 529/1000                                                                       \n252/252 - 1s - loss: 1.9221 - accuracy: 0.9553 - val_loss: 2.1224 - val_accuracy: 0.8777\n\nEpoch 530/1000                                                                       \n252/252 - 1s - loss: 1.9198 - accuracy: 0.9562 - val_loss: 2.1208 - val_accuracy: 0.8787\n\nEpoch 531/1000                                                                       \n252/252 - 1s - loss: 1.9205 - accuracy: 0.9542 - val_loss: 2.1189 - val_accuracy: 0.8787\n\nEpoch 532/1000                                                                       \n252/252 - 1s - loss: 1.9171 - accuracy: 0.9552 - val_loss: 2.1176 - val_accuracy: 0.8787\n\nEpoch 533/1000                                                                       \n252/252 - 1s - loss: 1.9151 - accuracy: 0.9535 - val_loss: 2.1161 - val_accuracy: 0.8787\n\nEpoch 534/1000                                                                       \n252/252 - 1s - loss: 1.9136 - accuracy: 0.9556 - val_loss: 2.1143 - val_accuracy: 0.8787\n\nEpoch 535/1000                                                                       \n252/252 - 1s - loss: 1.9129 - accuracy: 0.9550 - val_loss: 2.1135 - val_accuracy: 0.8792\n\nEpoch 536/1000                                                                       \n252/252 - 1s - loss: 1.9122 - accuracy: 0.9547 - val_loss: 2.1117 - val_accuracy: 0.8797\n\nEpoch 537/1000                                                                       \n252/252 - 1s - loss: 1.9100 - accuracy: 0.9557 - val_loss: 2.1106 - val_accuracy: 0.8797\n\nEpoch 538/1000                                                                       \n252/252 - 1s - loss: 1.9073 - accuracy: 0.9570 - val_loss: 2.1091 - val_accuracy: 0.8782\n\nEpoch 539/1000                                                                       \n252/252 - 1s - loss: 1.9071 - accuracy: 0.9527 - val_loss: 2.1078 - val_accuracy: 0.8792\n\nEpoch 540/1000                                                                       \n252/252 - 1s - loss: 1.9064 - accuracy: 0.9546 - val_loss: 2.1058 - val_accuracy: 0.8797\n\nEpoch 541/1000                                                                       \n252/252 - 1s - loss: 1.9031 - accuracy: 0.9562 - val_loss: 2.1040 - val_accuracy: 0.8792\n\nEpoch 542/1000                                                                       \n252/252 - 1s - loss: 1.9022 - accuracy: 0.9551 - val_loss: 2.1027 - val_accuracy: 0.8797\n\nEpoch 543/1000                                                                       \n252/252 - 1s - loss: 1.8998 - accuracy: 0.9570 - val_loss: 2.1016 - val_accuracy: 0.8807\n\nEpoch 544/1000                                                                       \n252/252 - 1s - loss: 1.8991 - accuracy: 0.9574 - val_loss: 2.0997 - val_accuracy: 0.8822\n\nEpoch 545/1000                                                                       \n252/252 - 1s - loss: 1.8979 - accuracy: 0.9570 - val_loss: 2.0983 - val_accuracy: 0.8822\n\nEpoch 546/1000                                                                       \n252/252 - 1s - loss: 1.8942 - accuracy: 0.9588 - val_loss: 2.0968 - val_accuracy: 0.8827\n\nEpoch 547/1000                                                                       \n252/252 - 1s - loss: 1.8959 - accuracy: 0.9565 - val_loss: 2.0954 - val_accuracy: 0.8817\n\nEpoch 548/1000                                                                       \n252/252 - 1s - loss: 1.8962 - accuracy: 0.9547 - val_loss: 2.0938 - val_accuracy: 0.8827\n\nEpoch 549/1000                                                                       \n252/252 - 1s - loss: 1.8938 - accuracy: 0.9552 - val_loss: 2.0923 - val_accuracy: 0.8827\n\nEpoch 550/1000                                                                       \n252/252 - 1s - loss: 1.8905 - accuracy: 0.9568 - val_loss: 2.0909 - val_accuracy: 0.8832\n\nEpoch 551/1000                                                                       \n252/252 - 1s - loss: 1.8893 - accuracy: 0.9583 - val_loss: 2.0894 - val_accuracy: 0.8832\n\nEpoch 552/1000                                                                       \n252/252 - 1s - loss: 1.8870 - accuracy: 0.9539 - val_loss: 2.0887 - val_accuracy: 0.8837\n\nEpoch 553/1000                                                                       \n252/252 - 1s - loss: 1.8854 - accuracy: 0.9589 - val_loss: 2.0869 - val_accuracy: 0.8832\n\nEpoch 554/1000                                                                       \n252/252 - 1s - loss: 1.8847 - accuracy: 0.9565 - val_loss: 2.0850 - val_accuracy: 0.8837\n\nEpoch 555/1000                                                                       \n252/252 - 1s - loss: 1.8842 - accuracy: 0.9587 - val_loss: 2.0838 - val_accuracy: 0.8837\n\nEpoch 556/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 1.8838 - accuracy: 0.9566 - val_loss: 2.0827 - val_accuracy: 0.8837\n\nEpoch 557/1000                                                                       \n252/252 - 1s - loss: 1.8809 - accuracy: 0.9572 - val_loss: 2.0812 - val_accuracy: 0.8832\n\nEpoch 558/1000                                                                       \n252/252 - 1s - loss: 1.8792 - accuracy: 0.9573 - val_loss: 2.0794 - val_accuracy: 0.8837\n\nEpoch 559/1000                                                                       \n252/252 - 1s - loss: 1.8796 - accuracy: 0.9563 - val_loss: 2.0786 - val_accuracy: 0.8827\n\nEpoch 560/1000                                                                       \n252/252 - 1s - loss: 1.8774 - accuracy: 0.9579 - val_loss: 2.0766 - val_accuracy: 0.8837\n\nEpoch 561/1000                                                                       \n252/252 - 1s - loss: 1.8760 - accuracy: 0.9572 - val_loss: 2.0747 - val_accuracy: 0.8837\n\nEpoch 562/1000                                                                       \n252/252 - 1s - loss: 1.8756 - accuracy: 0.9551 - val_loss: 2.0740 - val_accuracy: 0.8837\n\nEpoch 563/1000                                                                       \n252/252 - 1s - loss: 1.8724 - accuracy: 0.9597 - val_loss: 2.0721 - val_accuracy: 0.8837\n\nEpoch 564/1000                                                                       \n252/252 - 1s - loss: 1.8722 - accuracy: 0.9570 - val_loss: 2.0711 - val_accuracy: 0.8842\n\nEpoch 565/1000                                                                       \n252/252 - 1s - loss: 1.8717 - accuracy: 0.9567 - val_loss: 2.0691 - val_accuracy: 0.8842\n\nEpoch 566/1000                                                                       \n252/252 - 1s - loss: 1.8681 - accuracy: 0.9591 - val_loss: 2.0669 - val_accuracy: 0.8842\n\nEpoch 567/1000                                                                       \n252/252 - 1s - loss: 1.8677 - accuracy: 0.9581 - val_loss: 2.0661 - val_accuracy: 0.8842\n\nEpoch 568/1000                                                                       \n252/252 - 1s - loss: 1.8650 - accuracy: 0.9602 - val_loss: 2.0650 - val_accuracy: 0.8842\n\nEpoch 569/1000                                                                       \n252/252 - 1s - loss: 1.8642 - accuracy: 0.9577 - val_loss: 2.0640 - val_accuracy: 0.8837\n\nEpoch 570/1000                                                                       \n252/252 - 1s - loss: 1.8645 - accuracy: 0.9570 - val_loss: 2.0626 - val_accuracy: 0.8842\n\nEpoch 571/1000                                                                       \n252/252 - 1s - loss: 1.8618 - accuracy: 0.9610 - val_loss: 2.0615 - val_accuracy: 0.8842\n\nEpoch 572/1000                                                                       \n252/252 - 1s - loss: 1.8596 - accuracy: 0.9582 - val_loss: 2.0593 - val_accuracy: 0.8842\n\nEpoch 573/1000                                                                       \n252/252 - 1s - loss: 1.8584 - accuracy: 0.9584 - val_loss: 2.0587 - val_accuracy: 0.8837\n\nEpoch 574/1000                                                                       \n252/252 - 1s - loss: 1.8579 - accuracy: 0.9592 - val_loss: 2.0565 - val_accuracy: 0.8847\n\nEpoch 575/1000                                                                       \n252/252 - 1s - loss: 1.8553 - accuracy: 0.9591 - val_loss: 2.0554 - val_accuracy: 0.8847\n\nEpoch 576/1000                                                                       \n252/252 - 1s - loss: 1.8550 - accuracy: 0.9581 - val_loss: 2.0542 - val_accuracy: 0.8842\n\nEpoch 577/1000                                                                       \n252/252 - 1s - loss: 1.8529 - accuracy: 0.9606 - val_loss: 2.0527 - val_accuracy: 0.8847\n\nEpoch 578/1000                                                                       \n252/252 - 1s - loss: 1.8520 - accuracy: 0.9591 - val_loss: 2.0518 - val_accuracy: 0.8847\n\nEpoch 579/1000                                                                       \n252/252 - 1s - loss: 1.8500 - accuracy: 0.9588 - val_loss: 2.0514 - val_accuracy: 0.8842\n\nEpoch 580/1000                                                                       \n252/252 - 1s - loss: 1.8493 - accuracy: 0.9598 - val_loss: 2.0494 - val_accuracy: 0.8842\n\nEpoch 581/1000                                                                       \n252/252 - 1s - loss: 1.8487 - accuracy: 0.9584 - val_loss: 2.0484 - val_accuracy: 0.8842\n\nEpoch 582/1000                                                                       \n252/252 - 1s - loss: 1.8472 - accuracy: 0.9596 - val_loss: 2.0468 - val_accuracy: 0.8842\n\nEpoch 583/1000                                                                       \n252/252 - 1s - loss: 1.8457 - accuracy: 0.9591 - val_loss: 2.0447 - val_accuracy: 0.8852\n\nEpoch 584/1000                                                                       \n252/252 - 1s - loss: 1.8428 - accuracy: 0.9602 - val_loss: 2.0430 - val_accuracy: 0.8852\n\nEpoch 585/1000                                                                       \n252/252 - 1s - loss: 1.8426 - accuracy: 0.9619 - val_loss: 2.0415 - val_accuracy: 0.8852\n\nEpoch 586/1000                                                                       \n252/252 - 1s - loss: 1.8401 - accuracy: 0.9613 - val_loss: 2.0396 - val_accuracy: 0.8847\n\nEpoch 587/1000                                                                       \n252/252 - 1s - loss: 1.8401 - accuracy: 0.9607 - val_loss: 2.0391 - val_accuracy: 0.8847\n\nEpoch 588/1000                                                                       \n252/252 - 1s - loss: 1.8389 - accuracy: 0.9578 - val_loss: 2.0375 - val_accuracy: 0.8847\n\nEpoch 589/1000                                                                       \n252/252 - 1s - loss: 1.8379 - accuracy: 0.9579 - val_loss: 2.0360 - val_accuracy: 0.8852\n\nEpoch 590/1000                                                                       \n252/252 - 1s - loss: 1.8360 - accuracy: 0.9603 - val_loss: 2.0351 - val_accuracy: 0.8847\n\nEpoch 591/1000                                                                       \n252/252 - 1s - loss: 1.8350 - accuracy: 0.9604 - val_loss: 2.0329 - val_accuracy: 0.8852\n\nEpoch 592/1000                                                                       \n252/252 - 1s - loss: 1.8340 - accuracy: 0.9599 - val_loss: 2.0323 - val_accuracy: 0.8847\n\nEpoch 593/1000                                                                       \n252/252 - 1s - loss: 1.8350 - accuracy: 0.9592 - val_loss: 2.0310 - val_accuracy: 0.8852\n\nEpoch 594/1000                                                                       \n252/252 - 1s - loss: 1.8315 - accuracy: 0.9587 - val_loss: 2.0294 - val_accuracy: 0.8852\n\nEpoch 595/1000                                                                       \n252/252 - 1s - loss: 1.8297 - accuracy: 0.9601 - val_loss: 2.0285 - val_accuracy: 0.8857\n\nEpoch 596/1000                                                                       \n252/252 - 1s - loss: 1.8281 - accuracy: 0.9643 - val_loss: 2.0270 - val_accuracy: 0.8852\n\nEpoch 597/1000                                                                       \n252/252 - 1s - loss: 1.8279 - accuracy: 0.9601 - val_loss: 2.0253 - val_accuracy: 0.8857\n\nEpoch 598/1000                                                                       \n252/252 - 1s - loss: 1.8283 - accuracy: 0.9615 - val_loss: 2.0245 - val_accuracy: 0.8857\n\nEpoch 599/1000                                                                       \n252/252 - 1s - loss: 1.8244 - accuracy: 0.9592 - val_loss: 2.0231 - val_accuracy: 0.8867\n\nEpoch 600/1000                                                                       \n252/252 - 1s - loss: 1.8246 - accuracy: 0.9606 - val_loss: 2.0221 - val_accuracy: 0.8862\n\nEpoch 601/1000                                                                       \n252/252 - 1s - loss: 1.8222 - accuracy: 0.9625 - val_loss: 2.0212 - val_accuracy: 0.8852\n\nEpoch 602/1000                                                                       \n252/252 - 1s - loss: 1.8227 - accuracy: 0.9592 - val_loss: 2.0194 - val_accuracy: 0.8862\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 603/1000                                                                       \n252/252 - 1s - loss: 1.8197 - accuracy: 0.9609 - val_loss: 2.0186 - val_accuracy: 0.8852\n\nEpoch 604/1000                                                                       \n252/252 - 1s - loss: 1.8191 - accuracy: 0.9624 - val_loss: 2.0166 - val_accuracy: 0.8867\n\nEpoch 605/1000                                                                       \n252/252 - 1s - loss: 1.8180 - accuracy: 0.9615 - val_loss: 2.0153 - val_accuracy: 0.8872\n\nEpoch 606/1000                                                                       \n252/252 - 1s - loss: 1.8135 - accuracy: 0.9620 - val_loss: 2.0138 - val_accuracy: 0.8872\n\nEpoch 607/1000                                                                       \n252/252 - 1s - loss: 1.8136 - accuracy: 0.9602 - val_loss: 2.0123 - val_accuracy: 0.8877\n\nEpoch 608/1000                                                                       \n252/252 - 1s - loss: 1.8118 - accuracy: 0.9601 - val_loss: 2.0109 - val_accuracy: 0.8872\n\nEpoch 609/1000                                                                       \n252/252 - 1s - loss: 1.8129 - accuracy: 0.9603 - val_loss: 2.0098 - val_accuracy: 0.8872\n\nEpoch 610/1000                                                                       \n252/252 - 1s - loss: 1.8128 - accuracy: 0.9591 - val_loss: 2.0088 - val_accuracy: 0.8862\n\nEpoch 611/1000                                                                       \n252/252 - 1s - loss: 1.8103 - accuracy: 0.9614 - val_loss: 2.0078 - val_accuracy: 0.8857\n\nEpoch 612/1000                                                                       \n252/252 - 1s - loss: 1.8095 - accuracy: 0.9610 - val_loss: 2.0063 - val_accuracy: 0.8867\n\nEpoch 613/1000                                                                       \n252/252 - 1s - loss: 1.8066 - accuracy: 0.9619 - val_loss: 2.0044 - val_accuracy: 0.8882\n\nEpoch 614/1000                                                                       \n252/252 - 1s - loss: 1.8052 - accuracy: 0.9609 - val_loss: 2.0028 - val_accuracy: 0.8882\n\nEpoch 615/1000                                                                       \n252/252 - 1s - loss: 1.8053 - accuracy: 0.9637 - val_loss: 2.0014 - val_accuracy: 0.8882\n\nEpoch 616/1000                                                                       \n252/252 - 1s - loss: 1.8038 - accuracy: 0.9589 - val_loss: 2.0006 - val_accuracy: 0.8882\n\nEpoch 617/1000                                                                       \n252/252 - 1s - loss: 1.8015 - accuracy: 0.9637 - val_loss: 1.9990 - val_accuracy: 0.8882\n\nEpoch 618/1000                                                                       \n252/252 - 1s - loss: 1.8018 - accuracy: 0.9615 - val_loss: 1.9979 - val_accuracy: 0.8887\n\nEpoch 619/1000                                                                       \n252/252 - 1s - loss: 1.7995 - accuracy: 0.9623 - val_loss: 1.9967 - val_accuracy: 0.8877\n\nEpoch 620/1000                                                                       \n252/252 - 1s - loss: 1.7988 - accuracy: 0.9630 - val_loss: 1.9954 - val_accuracy: 0.8882\n\nEpoch 621/1000                                                                       \n252/252 - 1s - loss: 1.7982 - accuracy: 0.9608 - val_loss: 1.9945 - val_accuracy: 0.8882\n\nEpoch 622/1000                                                                       \n252/252 - 1s - loss: 1.7960 - accuracy: 0.9606 - val_loss: 1.9927 - val_accuracy: 0.8882\n\nEpoch 623/1000                                                                       \n252/252 - 1s - loss: 1.7940 - accuracy: 0.9596 - val_loss: 1.9923 - val_accuracy: 0.8877\n\nEpoch 624/1000                                                                       \n252/252 - 1s - loss: 1.7944 - accuracy: 0.9620 - val_loss: 1.9903 - val_accuracy: 0.8882\n\nEpoch 625/1000                                                                       \n252/252 - 1s - loss: 1.7919 - accuracy: 0.9643 - val_loss: 1.9894 - val_accuracy: 0.8882\n\nEpoch 626/1000                                                                       \n252/252 - 1s - loss: 1.7893 - accuracy: 0.9637 - val_loss: 1.9881 - val_accuracy: 0.8882\n\nEpoch 627/1000                                                                       \n252/252 - 1s - loss: 1.7908 - accuracy: 0.9627 - val_loss: 1.9865 - val_accuracy: 0.8882\n\nEpoch 628/1000                                                                       \n252/252 - 1s - loss: 1.7892 - accuracy: 0.9617 - val_loss: 1.9848 - val_accuracy: 0.8882\n\nEpoch 629/1000                                                                       \n252/252 - 1s - loss: 1.7873 - accuracy: 0.9625 - val_loss: 1.9837 - val_accuracy: 0.8882\n\nEpoch 630/1000                                                                       \n252/252 - 1s - loss: 1.7861 - accuracy: 0.9615 - val_loss: 1.9822 - val_accuracy: 0.8892\n\nEpoch 631/1000                                                                       \n252/252 - 1s - loss: 1.7835 - accuracy: 0.9643 - val_loss: 1.9815 - val_accuracy: 0.8882\n\nEpoch 632/1000                                                                       \n252/252 - 1s - loss: 1.7841 - accuracy: 0.9612 - val_loss: 1.9799 - val_accuracy: 0.8882\n\nEpoch 633/1000                                                                       \n252/252 - 1s - loss: 1.7843 - accuracy: 0.9609 - val_loss: 1.9791 - val_accuracy: 0.8882\n\nEpoch 634/1000                                                                       \n252/252 - 1s - loss: 1.7814 - accuracy: 0.9643 - val_loss: 1.9772 - val_accuracy: 0.8887\n\nEpoch 635/1000                                                                       \n252/252 - 1s - loss: 1.7793 - accuracy: 0.9655 - val_loss: 1.9764 - val_accuracy: 0.8882\n\nEpoch 636/1000                                                                       \n252/252 - 1s - loss: 1.7787 - accuracy: 0.9629 - val_loss: 1.9749 - val_accuracy: 0.8887\n\nEpoch 637/1000                                                                       \n252/252 - 1s - loss: 1.7782 - accuracy: 0.9619 - val_loss: 1.9736 - val_accuracy: 0.8887\n\nEpoch 638/1000                                                                       \n252/252 - 1s - loss: 1.7765 - accuracy: 0.9638 - val_loss: 1.9730 - val_accuracy: 0.8882\n\nEpoch 639/1000                                                                       \n252/252 - 1s - loss: 1.7746 - accuracy: 0.9633 - val_loss: 1.9719 - val_accuracy: 0.8882\n\nEpoch 640/1000                                                                       \n252/252 - 1s - loss: 1.7747 - accuracy: 0.9620 - val_loss: 1.9703 - val_accuracy: 0.8882\n\nEpoch 641/1000                                                                       \n252/252 - 1s - loss: 1.7730 - accuracy: 0.9632 - val_loss: 1.9693 - val_accuracy: 0.8887\n\nEpoch 642/1000                                                                       \n252/252 - 1s - loss: 1.7708 - accuracy: 0.9646 - val_loss: 1.9683 - val_accuracy: 0.8897\n\nEpoch 643/1000                                                                       \n252/252 - 1s - loss: 1.7694 - accuracy: 0.9640 - val_loss: 1.9668 - val_accuracy: 0.8887\n\nEpoch 644/1000                                                                       \n252/252 - 1s - loss: 1.7689 - accuracy: 0.9637 - val_loss: 1.9651 - val_accuracy: 0.8897\n\nEpoch 645/1000                                                                       \n252/252 - 1s - loss: 1.7671 - accuracy: 0.9651 - val_loss: 1.9641 - val_accuracy: 0.8897\n\nEpoch 646/1000                                                                       \n252/252 - 1s - loss: 1.7685 - accuracy: 0.9633 - val_loss: 1.9626 - val_accuracy: 0.8892\n\nEpoch 647/1000                                                                       \n252/252 - 1s - loss: 1.7664 - accuracy: 0.9613 - val_loss: 1.9612 - val_accuracy: 0.8892\n\nEpoch 648/1000                                                                       \n252/252 - 1s - loss: 1.7643 - accuracy: 0.9645 - val_loss: 1.9604 - val_accuracy: 0.8897\n\nEpoch 649/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 1.7641 - accuracy: 0.9648 - val_loss: 1.9592 - val_accuracy: 0.8902\n\nEpoch 650/1000                                                                       \n252/252 - 1s - loss: 1.7616 - accuracy: 0.9643 - val_loss: 1.9572 - val_accuracy: 0.8897\n\nEpoch 651/1000                                                                       \n252/252 - 1s - loss: 1.7621 - accuracy: 0.9645 - val_loss: 1.9569 - val_accuracy: 0.8897\n\nEpoch 652/1000                                                                       \n252/252 - 1s - loss: 1.7601 - accuracy: 0.9627 - val_loss: 1.9550 - val_accuracy: 0.8902\n\nEpoch 653/1000                                                                       \n252/252 - 1s - loss: 1.7600 - accuracy: 0.9638 - val_loss: 1.9538 - val_accuracy: 0.8902\n\nEpoch 654/1000                                                                       \n252/252 - 1s - loss: 1.7585 - accuracy: 0.9651 - val_loss: 1.9532 - val_accuracy: 0.8902\n\nEpoch 655/1000                                                                       \n252/252 - 1s - loss: 1.7569 - accuracy: 0.9650 - val_loss: 1.9520 - val_accuracy: 0.8902\n\nEpoch 656/1000                                                                       \n252/252 - 1s - loss: 1.7564 - accuracy: 0.9628 - val_loss: 1.9512 - val_accuracy: 0.8912\n\nEpoch 657/1000                                                                       \n252/252 - 1s - loss: 1.7529 - accuracy: 0.9661 - val_loss: 1.9502 - val_accuracy: 0.8912\n\nEpoch 658/1000                                                                       \n252/252 - 1s - loss: 1.7534 - accuracy: 0.9651 - val_loss: 1.9489 - val_accuracy: 0.8902\n\nEpoch 659/1000                                                                       \n252/252 - 1s - loss: 1.7523 - accuracy: 0.9648 - val_loss: 1.9474 - val_accuracy: 0.8897\n\nEpoch 660/1000                                                                       \n252/252 - 1s - loss: 1.7534 - accuracy: 0.9627 - val_loss: 1.9462 - val_accuracy: 0.8902\n\nEpoch 661/1000                                                                       \n252/252 - 1s - loss: 1.7489 - accuracy: 0.9644 - val_loss: 1.9442 - val_accuracy: 0.8902\n\nEpoch 662/1000                                                                       \n252/252 - 1s - loss: 1.7500 - accuracy: 0.9638 - val_loss: 1.9432 - val_accuracy: 0.8902\n\nEpoch 663/1000                                                                       \n252/252 - 1s - loss: 1.7479 - accuracy: 0.9654 - val_loss: 1.9429 - val_accuracy: 0.8907\n\nEpoch 664/1000                                                                       \n252/252 - 1s - loss: 1.7462 - accuracy: 0.9648 - val_loss: 1.9408 - val_accuracy: 0.8902\n\nEpoch 665/1000                                                                       \n252/252 - 1s - loss: 1.7467 - accuracy: 0.9646 - val_loss: 1.9402 - val_accuracy: 0.8902\n\nEpoch 666/1000                                                                       \n252/252 - 1s - loss: 1.7445 - accuracy: 0.9619 - val_loss: 1.9391 - val_accuracy: 0.8912\n\nEpoch 667/1000                                                                       \n252/252 - 1s - loss: 1.7441 - accuracy: 0.9624 - val_loss: 1.9381 - val_accuracy: 0.8912\n\nEpoch 668/1000                                                                       \n252/252 - 1s - loss: 1.7421 - accuracy: 0.9661 - val_loss: 1.9368 - val_accuracy: 0.8912\n\nEpoch 669/1000                                                                       \n252/252 - 1s - loss: 1.7403 - accuracy: 0.9650 - val_loss: 1.9353 - val_accuracy: 0.8917\n\nEpoch 670/1000                                                                       \n252/252 - 1s - loss: 1.7395 - accuracy: 0.9654 - val_loss: 1.9344 - val_accuracy: 0.8921\n\nEpoch 671/1000                                                                       \n252/252 - 1s - loss: 1.7380 - accuracy: 0.9656 - val_loss: 1.9328 - val_accuracy: 0.8917\n\nEpoch 672/1000                                                                       \n252/252 - 1s - loss: 1.7378 - accuracy: 0.9664 - val_loss: 1.9312 - val_accuracy: 0.8917\n\nEpoch 673/1000                                                                       \n252/252 - 1s - loss: 1.7369 - accuracy: 0.9655 - val_loss: 1.9310 - val_accuracy: 0.8926\n\nEpoch 674/1000                                                                       \n252/252 - 1s - loss: 1.7349 - accuracy: 0.9643 - val_loss: 1.9295 - val_accuracy: 0.8917\n\nEpoch 675/1000                                                                       \n252/252 - 1s - loss: 1.7351 - accuracy: 0.9639 - val_loss: 1.9280 - val_accuracy: 0.8917\n\nEpoch 676/1000                                                                       \n252/252 - 1s - loss: 1.7345 - accuracy: 0.9653 - val_loss: 1.9263 - val_accuracy: 0.8921\n\nEpoch 677/1000                                                                       \n252/252 - 1s - loss: 1.7326 - accuracy: 0.9622 - val_loss: 1.9257 - val_accuracy: 0.8921\n\nEpoch 678/1000                                                                       \n252/252 - 1s - loss: 1.7312 - accuracy: 0.9632 - val_loss: 1.9243 - val_accuracy: 0.8921\n\nEpoch 679/1000                                                                       \n252/252 - 1s - loss: 1.7316 - accuracy: 0.9638 - val_loss: 1.9237 - val_accuracy: 0.8921\n\nEpoch 680/1000                                                                       \n252/252 - 1s - loss: 1.7295 - accuracy: 0.9658 - val_loss: 1.9226 - val_accuracy: 0.8931\n\nEpoch 681/1000                                                                       \n252/252 - 1s - loss: 1.7288 - accuracy: 0.9632 - val_loss: 1.9221 - val_accuracy: 0.8931\n\nEpoch 682/1000                                                                       \n252/252 - 1s - loss: 1.7272 - accuracy: 0.9679 - val_loss: 1.9203 - val_accuracy: 0.8921\n\nEpoch 683/1000                                                                       \n252/252 - 1s - loss: 1.7272 - accuracy: 0.9639 - val_loss: 1.9190 - val_accuracy: 0.8921\n\nEpoch 684/1000                                                                       \n252/252 - 1s - loss: 1.7239 - accuracy: 0.9648 - val_loss: 1.9175 - val_accuracy: 0.8921\n\nEpoch 685/1000                                                                       \n252/252 - 1s - loss: 1.7221 - accuracy: 0.9680 - val_loss: 1.9160 - val_accuracy: 0.8926\n\nEpoch 686/1000                                                                       \n252/252 - 1s - loss: 1.7222 - accuracy: 0.9648 - val_loss: 1.9155 - val_accuracy: 0.8926\n\nEpoch 687/1000                                                                       \n252/252 - 1s - loss: 1.7223 - accuracy: 0.9645 - val_loss: 1.9145 - val_accuracy: 0.8936\n\nEpoch 688/1000                                                                       \n252/252 - 1s - loss: 1.7190 - accuracy: 0.9666 - val_loss: 1.9130 - val_accuracy: 0.8941\n\nEpoch 689/1000                                                                       \n252/252 - 1s - loss: 1.7196 - accuracy: 0.9651 - val_loss: 1.9113 - val_accuracy: 0.8941\n\nEpoch 690/1000                                                                       \n252/252 - 1s - loss: 1.7179 - accuracy: 0.9674 - val_loss: 1.9110 - val_accuracy: 0.8926\n\nEpoch 691/1000                                                                       \n252/252 - 1s - loss: 1.7160 - accuracy: 0.9676 - val_loss: 1.9098 - val_accuracy: 0.8926\n\nEpoch 692/1000                                                                       \n252/252 - 1s - loss: 1.7166 - accuracy: 0.9643 - val_loss: 1.9087 - val_accuracy: 0.8941\n\nEpoch 693/1000                                                                       \n252/252 - 1s - loss: 1.7158 - accuracy: 0.9645 - val_loss: 1.9070 - val_accuracy: 0.8946\n\nEpoch 694/1000                                                                       \n252/252 - 1s - loss: 1.7135 - accuracy: 0.9665 - val_loss: 1.9067 - val_accuracy: 0.8946\n\nEpoch 695/1000                                                                       \n252/252 - 1s - loss: 1.7120 - accuracy: 0.9672 - val_loss: 1.9046 - val_accuracy: 0.8946\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 696/1000                                                                       \n252/252 - 1s - loss: 1.7128 - accuracy: 0.9645 - val_loss: 1.9038 - val_accuracy: 0.8951\n\nEpoch 697/1000                                                                       \n252/252 - 1s - loss: 1.7105 - accuracy: 0.9650 - val_loss: 1.9032 - val_accuracy: 0.8946\n\nEpoch 698/1000                                                                       \n252/252 - 1s - loss: 1.7103 - accuracy: 0.9656 - val_loss: 1.9026 - val_accuracy: 0.8946\n\nEpoch 699/1000                                                                       \n252/252 - 1s - loss: 1.7088 - accuracy: 0.9674 - val_loss: 1.9012 - val_accuracy: 0.8946\n\nEpoch 700/1000                                                                       \n252/252 - 1s - loss: 1.7073 - accuracy: 0.9645 - val_loss: 1.8996 - val_accuracy: 0.8951\n\nEpoch 701/1000                                                                       \n252/252 - 1s - loss: 1.7062 - accuracy: 0.9672 - val_loss: 1.8990 - val_accuracy: 0.8946\n\nEpoch 702/1000                                                                       \n252/252 - 1s - loss: 1.7046 - accuracy: 0.9677 - val_loss: 1.8978 - val_accuracy: 0.8941\n\nEpoch 703/1000                                                                       \n252/252 - 1s - loss: 1.7050 - accuracy: 0.9664 - val_loss: 1.8965 - val_accuracy: 0.8941\n\nEpoch 704/1000                                                                       \n252/252 - 1s - loss: 1.7026 - accuracy: 0.9676 - val_loss: 1.8963 - val_accuracy: 0.8951\n\nEpoch 705/1000                                                                       \n252/252 - 1s - loss: 1.7030 - accuracy: 0.9684 - val_loss: 1.8945 - val_accuracy: 0.8941\n\nEpoch 706/1000                                                                       \n252/252 - 1s - loss: 1.7012 - accuracy: 0.9641 - val_loss: 1.8933 - val_accuracy: 0.8941\n\nEpoch 707/1000                                                                       \n252/252 - 1s - loss: 1.7015 - accuracy: 0.9655 - val_loss: 1.8919 - val_accuracy: 0.8946\n\nEpoch 708/1000                                                                       \n252/252 - 1s - loss: 1.7012 - accuracy: 0.9655 - val_loss: 1.8904 - val_accuracy: 0.8951\n\nEpoch 709/1000                                                                       \n252/252 - 1s - loss: 1.6982 - accuracy: 0.9664 - val_loss: 1.8901 - val_accuracy: 0.8961\n\nEpoch 710/1000                                                                       \n252/252 - 1s - loss: 1.6962 - accuracy: 0.9675 - val_loss: 1.8884 - val_accuracy: 0.8956\n\nEpoch 711/1000                                                                       \n252/252 - 1s - loss: 1.6971 - accuracy: 0.9654 - val_loss: 1.8877 - val_accuracy: 0.8961\n\nEpoch 712/1000                                                                       \n252/252 - 1s - loss: 1.6956 - accuracy: 0.9676 - val_loss: 1.8868 - val_accuracy: 0.8966\n\nEpoch 713/1000                                                                       \n252/252 - 1s - loss: 1.6942 - accuracy: 0.9671 - val_loss: 1.8855 - val_accuracy: 0.8961\n\nEpoch 714/1000                                                                       \n252/252 - 1s - loss: 1.6935 - accuracy: 0.9663 - val_loss: 1.8846 - val_accuracy: 0.8966\n\nEpoch 715/1000                                                                       \n252/252 - 1s - loss: 1.6896 - accuracy: 0.9675 - val_loss: 1.8830 - val_accuracy: 0.8956\n\nEpoch 716/1000                                                                       \n252/252 - 1s - loss: 1.6893 - accuracy: 0.9681 - val_loss: 1.8822 - val_accuracy: 0.8966\n\nEpoch 717/1000                                                                       \n252/252 - 1s - loss: 1.6904 - accuracy: 0.9675 - val_loss: 1.8814 - val_accuracy: 0.8966\n\nEpoch 718/1000                                                                       \n252/252 - 1s - loss: 1.6883 - accuracy: 0.9682 - val_loss: 1.8797 - val_accuracy: 0.8956\n\nEpoch 719/1000                                                                       \n252/252 - 1s - loss: 1.6877 - accuracy: 0.9676 - val_loss: 1.8784 - val_accuracy: 0.8956\n\nEpoch 720/1000                                                                       \n252/252 - 1s - loss: 1.6856 - accuracy: 0.9694 - val_loss: 1.8777 - val_accuracy: 0.8956\n\nEpoch 721/1000                                                                       \n252/252 - 1s - loss: 1.6852 - accuracy: 0.9675 - val_loss: 1.8771 - val_accuracy: 0.8966\n\nEpoch 722/1000                                                                       \n252/252 - 1s - loss: 1.6848 - accuracy: 0.9670 - val_loss: 1.8761 - val_accuracy: 0.8966\n\nEpoch 723/1000                                                                       \n252/252 - 1s - loss: 1.6843 - accuracy: 0.9685 - val_loss: 1.8750 - val_accuracy: 0.8966\n\nEpoch 724/1000                                                                       \n252/252 - 1s - loss: 1.6832 - accuracy: 0.9675 - val_loss: 1.8741 - val_accuracy: 0.8966\n\nEpoch 725/1000                                                                       \n252/252 - 1s - loss: 1.6824 - accuracy: 0.9671 - val_loss: 1.8726 - val_accuracy: 0.8966\n\nEpoch 726/1000                                                                       \n252/252 - 1s - loss: 1.6805 - accuracy: 0.9679 - val_loss: 1.8712 - val_accuracy: 0.8966\n\nEpoch 727/1000                                                                       \n252/252 - 1s - loss: 1.6781 - accuracy: 0.9695 - val_loss: 1.8705 - val_accuracy: 0.8961\n\nEpoch 728/1000                                                                       \n252/252 - 1s - loss: 1.6791 - accuracy: 0.9691 - val_loss: 1.8696 - val_accuracy: 0.8961\n\nEpoch 729/1000                                                                       \n252/252 - 1s - loss: 1.6774 - accuracy: 0.9658 - val_loss: 1.8692 - val_accuracy: 0.8966\n\nEpoch 730/1000                                                                       \n252/252 - 1s - loss: 1.6784 - accuracy: 0.9669 - val_loss: 1.8679 - val_accuracy: 0.8966\n\nEpoch 731/1000                                                                       \n252/252 - 1s - loss: 1.6754 - accuracy: 0.9710 - val_loss: 1.8669 - val_accuracy: 0.8966\n\nEpoch 732/1000                                                                       \n252/252 - 1s - loss: 1.6753 - accuracy: 0.9659 - val_loss: 1.8652 - val_accuracy: 0.8966\n\nEpoch 733/1000                                                                       \n252/252 - 1s - loss: 1.6739 - accuracy: 0.9670 - val_loss: 1.8649 - val_accuracy: 0.8971\n\nEpoch 734/1000                                                                       \n252/252 - 1s - loss: 1.6733 - accuracy: 0.9686 - val_loss: 1.8637 - val_accuracy: 0.8971\n\nEpoch 735/1000                                                                       \n252/252 - 1s - loss: 1.6719 - accuracy: 0.9660 - val_loss: 1.8623 - val_accuracy: 0.8971\n\nEpoch 736/1000                                                                       \n252/252 - 1s - loss: 1.6706 - accuracy: 0.9684 - val_loss: 1.8610 - val_accuracy: 0.8961\n\nEpoch 737/1000                                                                       \n252/252 - 1s - loss: 1.6704 - accuracy: 0.9689 - val_loss: 1.8601 - val_accuracy: 0.8966\n\nEpoch 738/1000                                                                       \n252/252 - 1s - loss: 1.6704 - accuracy: 0.9677 - val_loss: 1.8587 - val_accuracy: 0.8966\n\nEpoch 739/1000                                                                       \n252/252 - 1s - loss: 1.6678 - accuracy: 0.9686 - val_loss: 1.8576 - val_accuracy: 0.8966\n\nEpoch 740/1000                                                                       \n252/252 - 1s - loss: 1.6668 - accuracy: 0.9686 - val_loss: 1.8564 - val_accuracy: 0.8966\n\nEpoch 741/1000                                                                       \n252/252 - 1s - loss: 1.6666 - accuracy: 0.9687 - val_loss: 1.8549 - val_accuracy: 0.8971\n\nEpoch 742/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 1.6648 - accuracy: 0.9694 - val_loss: 1.8543 - val_accuracy: 0.8966\n\nEpoch 743/1000                                                                       \n252/252 - 1s - loss: 1.6642 - accuracy: 0.9681 - val_loss: 1.8532 - val_accuracy: 0.8971\n\nEpoch 744/1000                                                                       \n252/252 - 1s - loss: 1.6627 - accuracy: 0.9690 - val_loss: 1.8526 - val_accuracy: 0.8981\n\nEpoch 745/1000                                                                       \n252/252 - 1s - loss: 1.6624 - accuracy: 0.9675 - val_loss: 1.8515 - val_accuracy: 0.8981\n\nEpoch 746/1000                                                                       \n252/252 - 1s - loss: 1.6610 - accuracy: 0.9676 - val_loss: 1.8504 - val_accuracy: 0.8981\n\nEpoch 747/1000                                                                       \n252/252 - 1s - loss: 1.6588 - accuracy: 0.9685 - val_loss: 1.8489 - val_accuracy: 0.8971\n\nEpoch 748/1000                                                                       \n252/252 - 1s - loss: 1.6593 - accuracy: 0.9704 - val_loss: 1.8486 - val_accuracy: 0.8981\n\nEpoch 749/1000                                                                       \n252/252 - 1s - loss: 1.6584 - accuracy: 0.9692 - val_loss: 1.8473 - val_accuracy: 0.8966\n\nEpoch 750/1000                                                                       \n252/252 - 1s - loss: 1.6574 - accuracy: 0.9700 - val_loss: 1.8465 - val_accuracy: 0.8966\n\nEpoch 751/1000                                                                       \n252/252 - 1s - loss: 1.6552 - accuracy: 0.9708 - val_loss: 1.8446 - val_accuracy: 0.8976\n\nEpoch 752/1000                                                                       \n252/252 - 1s - loss: 1.6543 - accuracy: 0.9684 - val_loss: 1.8442 - val_accuracy: 0.8966\n\nEpoch 753/1000                                                                       \n252/252 - 1s - loss: 1.6541 - accuracy: 0.9684 - val_loss: 1.8431 - val_accuracy: 0.8966\n\nEpoch 754/1000                                                                       \n252/252 - 1s - loss: 1.6538 - accuracy: 0.9694 - val_loss: 1.8421 - val_accuracy: 0.8971\n\nEpoch 755/1000                                                                       \n252/252 - 1s - loss: 1.6519 - accuracy: 0.9691 - val_loss: 1.8416 - val_accuracy: 0.8971\n\nEpoch 756/1000                                                                       \n252/252 - 1s - loss: 1.6519 - accuracy: 0.9684 - val_loss: 1.8405 - val_accuracy: 0.8976\n\nEpoch 757/1000                                                                       \n252/252 - 1s - loss: 1.6522 - accuracy: 0.9680 - val_loss: 1.8390 - val_accuracy: 0.8971\n\nEpoch 758/1000                                                                       \n252/252 - 1s - loss: 1.6507 - accuracy: 0.9690 - val_loss: 1.8382 - val_accuracy: 0.8976\n\nEpoch 759/1000                                                                       \n252/252 - 1s - loss: 1.6482 - accuracy: 0.9689 - val_loss: 1.8370 - val_accuracy: 0.8981\n\nEpoch 760/1000                                                                       \n252/252 - 1s - loss: 1.6494 - accuracy: 0.9700 - val_loss: 1.8364 - val_accuracy: 0.8976\n\nEpoch 761/1000                                                                       \n252/252 - 1s - loss: 1.6475 - accuracy: 0.9676 - val_loss: 1.8348 - val_accuracy: 0.8981\n\nEpoch 762/1000                                                                       \n252/252 - 1s - loss: 1.6447 - accuracy: 0.9721 - val_loss: 1.8340 - val_accuracy: 0.8971\n\nEpoch 763/1000                                                                       \n252/252 - 1s - loss: 1.6452 - accuracy: 0.9707 - val_loss: 1.8322 - val_accuracy: 0.8981\n\nEpoch 764/1000                                                                       \n252/252 - 1s - loss: 1.6437 - accuracy: 0.9708 - val_loss: 1.8318 - val_accuracy: 0.8981\n\nEpoch 765/1000                                                                       \n252/252 - 1s - loss: 1.6431 - accuracy: 0.9704 - val_loss: 1.8315 - val_accuracy: 0.8976\n\nEpoch 766/1000                                                                       \n252/252 - 1s - loss: 1.6428 - accuracy: 0.9691 - val_loss: 1.8305 - val_accuracy: 0.8991\n\nEpoch 767/1000                                                                       \n252/252 - 1s - loss: 1.6421 - accuracy: 0.9677 - val_loss: 1.8294 - val_accuracy: 0.8981\n\nEpoch 768/1000                                                                       \n252/252 - 1s - loss: 1.6398 - accuracy: 0.9704 - val_loss: 1.8278 - val_accuracy: 0.8986\n\nEpoch 769/1000                                                                       \n252/252 - 1s - loss: 1.6403 - accuracy: 0.9684 - val_loss: 1.8269 - val_accuracy: 0.8986\n\nEpoch 770/1000                                                                       \n252/252 - 1s - loss: 1.6388 - accuracy: 0.9686 - val_loss: 1.8260 - val_accuracy: 0.8981\n\nEpoch 771/1000                                                                       \n252/252 - 1s - loss: 1.6373 - accuracy: 0.9715 - val_loss: 1.8248 - val_accuracy: 0.8981\n\nEpoch 772/1000                                                                       \n252/252 - 1s - loss: 1.6364 - accuracy: 0.9708 - val_loss: 1.8245 - val_accuracy: 0.8981\n\nEpoch 773/1000                                                                       \n252/252 - 1s - loss: 1.6376 - accuracy: 0.9716 - val_loss: 1.8236 - val_accuracy: 0.8981\n\nEpoch 774/1000                                                                       \n252/252 - 1s - loss: 1.6344 - accuracy: 0.9685 - val_loss: 1.8218 - val_accuracy: 0.8991\n\nEpoch 775/1000                                                                       \n252/252 - 1s - loss: 1.6345 - accuracy: 0.9696 - val_loss: 1.8210 - val_accuracy: 0.8991\n\nEpoch 776/1000                                                                       \n252/252 - 1s - loss: 1.6331 - accuracy: 0.9702 - val_loss: 1.8199 - val_accuracy: 0.8991\n\nEpoch 777/1000                                                                       \n252/252 - 1s - loss: 1.6325 - accuracy: 0.9694 - val_loss: 1.8188 - val_accuracy: 0.8996\n\nEpoch 778/1000                                                                       \n252/252 - 1s - loss: 1.6322 - accuracy: 0.9710 - val_loss: 1.8183 - val_accuracy: 0.8991\n\nEpoch 779/1000                                                                       \n252/252 - 1s - loss: 1.6302 - accuracy: 0.9715 - val_loss: 1.8176 - val_accuracy: 0.8991\n\nEpoch 780/1000                                                                       \n252/252 - 1s - loss: 1.6298 - accuracy: 0.9699 - val_loss: 1.8168 - val_accuracy: 0.9001\n\nEpoch 781/1000                                                                       \n252/252 - 1s - loss: 1.6296 - accuracy: 0.9702 - val_loss: 1.8155 - val_accuracy: 0.9001\n\nEpoch 782/1000                                                                       \n252/252 - 1s - loss: 1.6263 - accuracy: 0.9721 - val_loss: 1.8142 - val_accuracy: 0.8991\n\nEpoch 783/1000                                                                       \n252/252 - 1s - loss: 1.6263 - accuracy: 0.9712 - val_loss: 1.8133 - val_accuracy: 0.8991\n\nEpoch 784/1000                                                                       \n252/252 - 1s - loss: 1.6261 - accuracy: 0.9715 - val_loss: 1.8127 - val_accuracy: 0.8986\n\nEpoch 785/1000                                                                       \n252/252 - 1s - loss: 1.6235 - accuracy: 0.9716 - val_loss: 1.8115 - val_accuracy: 0.8991\n\nEpoch 786/1000                                                                       \n252/252 - 1s - loss: 1.6237 - accuracy: 0.9708 - val_loss: 1.8110 - val_accuracy: 0.8996\n\nEpoch 787/1000                                                                       \n252/252 - 1s - loss: 1.6221 - accuracy: 0.9727 - val_loss: 1.8096 - val_accuracy: 0.8991\n\nEpoch 788/1000                                                                       \n252/252 - 1s - loss: 1.6221 - accuracy: 0.9717 - val_loss: 1.8093 - val_accuracy: 0.8996\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 789/1000                                                                       \n252/252 - 1s - loss: 1.6229 - accuracy: 0.9708 - val_loss: 1.8080 - val_accuracy: 0.8996\n\nEpoch 790/1000                                                                       \n252/252 - 1s - loss: 1.6207 - accuracy: 0.9715 - val_loss: 1.8069 - val_accuracy: 0.9006\n\nEpoch 791/1000                                                                       \n252/252 - 1s - loss: 1.6195 - accuracy: 0.9721 - val_loss: 1.8055 - val_accuracy: 0.9001\n\nEpoch 792/1000                                                                       \n252/252 - 1s - loss: 1.6176 - accuracy: 0.9708 - val_loss: 1.8046 - val_accuracy: 0.9006\n\nEpoch 793/1000                                                                       \n252/252 - 1s - loss: 1.6178 - accuracy: 0.9718 - val_loss: 1.8034 - val_accuracy: 0.9006\n\nEpoch 794/1000                                                                       \n252/252 - 1s - loss: 1.6176 - accuracy: 0.9715 - val_loss: 1.8025 - val_accuracy: 0.9001\n\nEpoch 795/1000                                                                       \n252/252 - 1s - loss: 1.6158 - accuracy: 0.9722 - val_loss: 1.8011 - val_accuracy: 0.9011\n\nEpoch 796/1000                                                                       \n252/252 - 1s - loss: 1.6144 - accuracy: 0.9708 - val_loss: 1.8005 - val_accuracy: 0.9006\n\nEpoch 797/1000                                                                       \n252/252 - 1s - loss: 1.6149 - accuracy: 0.9702 - val_loss: 1.7986 - val_accuracy: 0.9026\n\nEpoch 798/1000                                                                       \n252/252 - 1s - loss: 1.6142 - accuracy: 0.9697 - val_loss: 1.7984 - val_accuracy: 0.9011\n\nEpoch 799/1000                                                                       \n252/252 - 1s - loss: 1.6130 - accuracy: 0.9706 - val_loss: 1.7977 - val_accuracy: 0.9016\n\nEpoch 800/1000                                                                       \n252/252 - 1s - loss: 1.6129 - accuracy: 0.9717 - val_loss: 1.7970 - val_accuracy: 0.9021\n\nEpoch 801/1000                                                                       \n252/252 - 1s - loss: 1.6117 - accuracy: 0.9721 - val_loss: 1.7955 - val_accuracy: 0.9036\n\nEpoch 802/1000                                                                       \n252/252 - 1s - loss: 1.6094 - accuracy: 0.9731 - val_loss: 1.7950 - val_accuracy: 0.9021\n\nEpoch 803/1000                                                                       \n252/252 - 1s - loss: 1.6089 - accuracy: 0.9704 - val_loss: 1.7941 - val_accuracy: 0.9021\n\nEpoch 804/1000                                                                       \n252/252 - 1s - loss: 1.6075 - accuracy: 0.9722 - val_loss: 1.7930 - val_accuracy: 0.9021\n\nEpoch 805/1000                                                                       \n252/252 - 1s - loss: 1.6081 - accuracy: 0.9695 - val_loss: 1.7921 - val_accuracy: 0.9021\n\nEpoch 806/1000                                                                       \n252/252 - 1s - loss: 1.6066 - accuracy: 0.9732 - val_loss: 1.7909 - val_accuracy: 0.9036\n\nEpoch 807/1000                                                                       \n252/252 - 1s - loss: 1.6060 - accuracy: 0.9720 - val_loss: 1.7907 - val_accuracy: 0.9021\n\nEpoch 808/1000                                                                       \n252/252 - 1s - loss: 1.6044 - accuracy: 0.9725 - val_loss: 1.7901 - val_accuracy: 0.9011\n\nEpoch 809/1000                                                                       \n252/252 - 1s - loss: 1.6050 - accuracy: 0.9702 - val_loss: 1.7888 - val_accuracy: 0.9021\n\nEpoch 810/1000                                                                       \n252/252 - 1s - loss: 1.6034 - accuracy: 0.9716 - val_loss: 1.7876 - val_accuracy: 0.9026\n\nEpoch 811/1000                                                                       \n252/252 - 1s - loss: 1.6021 - accuracy: 0.9730 - val_loss: 1.7864 - val_accuracy: 0.9036\n\nEpoch 812/1000                                                                       \n252/252 - 1s - loss: 1.5992 - accuracy: 0.9736 - val_loss: 1.7858 - val_accuracy: 0.9026\n\nEpoch 813/1000                                                                       \n252/252 - 1s - loss: 1.6015 - accuracy: 0.9723 - val_loss: 1.7852 - val_accuracy: 0.9026\n\nEpoch 814/1000                                                                       \n252/252 - 1s - loss: 1.6000 - accuracy: 0.9717 - val_loss: 1.7847 - val_accuracy: 0.9021\n\nEpoch 815/1000                                                                       \n252/252 - 1s - loss: 1.5989 - accuracy: 0.9731 - val_loss: 1.7832 - val_accuracy: 0.9026\n\nEpoch 816/1000                                                                       \n252/252 - 1s - loss: 1.5966 - accuracy: 0.9716 - val_loss: 1.7818 - val_accuracy: 0.9036\n\nEpoch 817/1000                                                                       \n252/252 - 1s - loss: 1.5968 - accuracy: 0.9697 - val_loss: 1.7813 - val_accuracy: 0.9031\n\nEpoch 818/1000                                                                       \n252/252 - 1s - loss: 1.5966 - accuracy: 0.9728 - val_loss: 1.7810 - val_accuracy: 0.9016\n\nEpoch 819/1000                                                                       \n252/252 - 1s - loss: 1.5961 - accuracy: 0.9713 - val_loss: 1.7797 - val_accuracy: 0.9026\n\nEpoch 820/1000                                                                       \n252/252 - 1s - loss: 1.5950 - accuracy: 0.9720 - val_loss: 1.7787 - val_accuracy: 0.9031\n\nEpoch 821/1000                                                                       \n252/252 - 1s - loss: 1.5950 - accuracy: 0.9707 - val_loss: 1.7783 - val_accuracy: 0.9031\n\nEpoch 822/1000                                                                       \n252/252 - 1s - loss: 1.5938 - accuracy: 0.9726 - val_loss: 1.7773 - val_accuracy: 0.9031\n\nEpoch 823/1000                                                                       \n252/252 - 1s - loss: 1.5913 - accuracy: 0.9732 - val_loss: 1.7761 - val_accuracy: 0.9041\n\nEpoch 824/1000                                                                       \n252/252 - 1s - loss: 1.5923 - accuracy: 0.9708 - val_loss: 1.7753 - val_accuracy: 0.9036\n\nEpoch 825/1000                                                                       \n252/252 - 1s - loss: 1.5910 - accuracy: 0.9723 - val_loss: 1.7746 - val_accuracy: 0.9031\n\nEpoch 826/1000                                                                       \n252/252 - 1s - loss: 1.5905 - accuracy: 0.9716 - val_loss: 1.7735 - val_accuracy: 0.9036\n\nEpoch 827/1000                                                                       \n252/252 - 1s - loss: 1.5888 - accuracy: 0.9730 - val_loss: 1.7723 - val_accuracy: 0.9036\n\nEpoch 828/1000                                                                       \n252/252 - 1s - loss: 1.5877 - accuracy: 0.9735 - val_loss: 1.7714 - val_accuracy: 0.9036\n\nEpoch 829/1000                                                                       \n252/252 - 1s - loss: 1.5870 - accuracy: 0.9723 - val_loss: 1.7705 - val_accuracy: 0.9036\n\nEpoch 830/1000                                                                       \n252/252 - 1s - loss: 1.5859 - accuracy: 0.9711 - val_loss: 1.7694 - val_accuracy: 0.9036\n\nEpoch 831/1000                                                                       \n252/252 - 1s - loss: 1.5865 - accuracy: 0.9717 - val_loss: 1.7679 - val_accuracy: 0.9041\n\nEpoch 832/1000                                                                       \n252/252 - 1s - loss: 1.5846 - accuracy: 0.9721 - val_loss: 1.7671 - val_accuracy: 0.9041\n\nEpoch 833/1000                                                                       \n252/252 - 1s - loss: 1.5832 - accuracy: 0.9725 - val_loss: 1.7660 - val_accuracy: 0.9041\n\nEpoch 834/1000                                                                       \n252/252 - 1s - loss: 1.5832 - accuracy: 0.9705 - val_loss: 1.7659 - val_accuracy: 0.9041\n\nEpoch 835/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 1.5832 - accuracy: 0.9728 - val_loss: 1.7655 - val_accuracy: 0.9036\n\nEpoch 836/1000                                                                       \n252/252 - 1s - loss: 1.5805 - accuracy: 0.9744 - val_loss: 1.7644 - val_accuracy: 0.9036\n\nEpoch 837/1000                                                                       \n252/252 - 1s - loss: 1.5800 - accuracy: 0.9726 - val_loss: 1.7631 - val_accuracy: 0.9041\n\nEpoch 838/1000                                                                       \n252/252 - 1s - loss: 1.5806 - accuracy: 0.9720 - val_loss: 1.7623 - val_accuracy: 0.9041\n\nEpoch 839/1000                                                                       \n252/252 - 1s - loss: 1.5791 - accuracy: 0.9732 - val_loss: 1.7610 - val_accuracy: 0.9046\n\nEpoch 840/1000                                                                       \n252/252 - 1s - loss: 1.5762 - accuracy: 0.9727 - val_loss: 1.7605 - val_accuracy: 0.9046\n\nEpoch 841/1000                                                                       \n252/252 - 1s - loss: 1.5770 - accuracy: 0.9718 - val_loss: 1.7597 - val_accuracy: 0.9051\n\nEpoch 842/1000                                                                       \n252/252 - 1s - loss: 1.5760 - accuracy: 0.9732 - val_loss: 1.7585 - val_accuracy: 0.9046\n\nEpoch 843/1000                                                                       \n252/252 - 1s - loss: 1.5757 - accuracy: 0.9736 - val_loss: 1.7580 - val_accuracy: 0.9051\n\nEpoch 844/1000                                                                       \n252/252 - 1s - loss: 1.5751 - accuracy: 0.9728 - val_loss: 1.7567 - val_accuracy: 0.9051\n\nEpoch 845/1000                                                                       \n252/252 - 1s - loss: 1.5737 - accuracy: 0.9717 - val_loss: 1.7562 - val_accuracy: 0.9051\n\nEpoch 846/1000                                                                       \n252/252 - 1s - loss: 1.5721 - accuracy: 0.9733 - val_loss: 1.7552 - val_accuracy: 0.9051\n\nEpoch 847/1000                                                                       \n252/252 - 1s - loss: 1.5728 - accuracy: 0.9702 - val_loss: 1.7542 - val_accuracy: 0.9056\n\nEpoch 848/1000                                                                       \n252/252 - 1s - loss: 1.5728 - accuracy: 0.9731 - val_loss: 1.7537 - val_accuracy: 0.9056\n\nEpoch 849/1000                                                                       \n252/252 - 1s - loss: 1.5718 - accuracy: 0.9707 - val_loss: 1.7525 - val_accuracy: 0.9056\n\nEpoch 850/1000                                                                       \n252/252 - 1s - loss: 1.5713 - accuracy: 0.9736 - val_loss: 1.7518 - val_accuracy: 0.9056\n\nEpoch 851/1000                                                                       \n252/252 - 1s - loss: 1.5698 - accuracy: 0.9744 - val_loss: 1.7507 - val_accuracy: 0.9056\n\nEpoch 852/1000                                                                       \n252/252 - 1s - loss: 1.5685 - accuracy: 0.9712 - val_loss: 1.7500 - val_accuracy: 0.9056\n\nEpoch 853/1000                                                                       \n252/252 - 1s - loss: 1.5687 - accuracy: 0.9735 - val_loss: 1.7490 - val_accuracy: 0.9056\n\nEpoch 854/1000                                                                       \n252/252 - 1s - loss: 1.5665 - accuracy: 0.9728 - val_loss: 1.7489 - val_accuracy: 0.9061\n\nEpoch 855/1000                                                                       \n252/252 - 1s - loss: 1.5662 - accuracy: 0.9732 - val_loss: 1.7477 - val_accuracy: 0.9061\n\nEpoch 856/1000                                                                       \n252/252 - 1s - loss: 1.5654 - accuracy: 0.9743 - val_loss: 1.7469 - val_accuracy: 0.9061\n\nEpoch 857/1000                                                                       \n252/252 - 1s - loss: 1.5653 - accuracy: 0.9738 - val_loss: 1.7458 - val_accuracy: 0.9061\n\nEpoch 858/1000                                                                       \n252/252 - 1s - loss: 1.5639 - accuracy: 0.9757 - val_loss: 1.7449 - val_accuracy: 0.9066\n\nEpoch 859/1000                                                                       \n252/252 - 1s - loss: 1.5625 - accuracy: 0.9739 - val_loss: 1.7444 - val_accuracy: 0.9066\n\nEpoch 860/1000                                                                       \n252/252 - 1s - loss: 1.5625 - accuracy: 0.9730 - val_loss: 1.7431 - val_accuracy: 0.9061\n\nEpoch 861/1000                                                                       \n252/252 - 1s - loss: 1.5619 - accuracy: 0.9736 - val_loss: 1.7418 - val_accuracy: 0.9076\n\nEpoch 862/1000                                                                       \n252/252 - 1s - loss: 1.5617 - accuracy: 0.9727 - val_loss: 1.7418 - val_accuracy: 0.9061\n\nEpoch 863/1000                                                                       \n252/252 - 1s - loss: 1.5582 - accuracy: 0.9741 - val_loss: 1.7408 - val_accuracy: 0.9066\n\nEpoch 864/1000                                                                       \n252/252 - 1s - loss: 1.5592 - accuracy: 0.9741 - val_loss: 1.7400 - val_accuracy: 0.9066\n\nEpoch 865/1000                                                                       \n252/252 - 1s - loss: 1.5591 - accuracy: 0.9720 - val_loss: 1.7391 - val_accuracy: 0.9066\n\nEpoch 866/1000                                                                       \n252/252 - 1s - loss: 1.5567 - accuracy: 0.9761 - val_loss: 1.7383 - val_accuracy: 0.9076\n\nEpoch 867/1000                                                                       \n252/252 - 1s - loss: 1.5574 - accuracy: 0.9730 - val_loss: 1.7371 - val_accuracy: 0.9076\n\nEpoch 868/1000                                                                       \n252/252 - 1s - loss: 1.5573 - accuracy: 0.9744 - val_loss: 1.7365 - val_accuracy: 0.9066\n\nEpoch 869/1000                                                                       \n252/252 - 1s - loss: 1.5551 - accuracy: 0.9748 - val_loss: 1.7357 - val_accuracy: 0.9071\n\nEpoch 870/1000                                                                       \n252/252 - 1s - loss: 1.5555 - accuracy: 0.9733 - val_loss: 1.7344 - val_accuracy: 0.9076\n\nEpoch 871/1000                                                                       \n252/252 - 1s - loss: 1.5538 - accuracy: 0.9737 - val_loss: 1.7335 - val_accuracy: 0.9076\n\nEpoch 872/1000                                                                       \n252/252 - 1s - loss: 1.5508 - accuracy: 0.9758 - val_loss: 1.7326 - val_accuracy: 0.9076\n\nEpoch 873/1000                                                                       \n252/252 - 1s - loss: 1.5519 - accuracy: 0.9738 - val_loss: 1.7316 - val_accuracy: 0.9076\n\nEpoch 874/1000                                                                       \n252/252 - 1s - loss: 1.5505 - accuracy: 0.9727 - val_loss: 1.7314 - val_accuracy: 0.9076\n\nEpoch 875/1000                                                                       \n252/252 - 1s - loss: 1.5498 - accuracy: 0.9744 - val_loss: 1.7305 - val_accuracy: 0.9076\n\nEpoch 876/1000                                                                       \n252/252 - 1s - loss: 1.5504 - accuracy: 0.9746 - val_loss: 1.7295 - val_accuracy: 0.9076\n\nEpoch 877/1000                                                                       \n252/252 - 1s - loss: 1.5504 - accuracy: 0.9744 - val_loss: 1.7290 - val_accuracy: 0.9076\n\nEpoch 878/1000                                                                       \n252/252 - 1s - loss: 1.5482 - accuracy: 0.9748 - val_loss: 1.7276 - val_accuracy: 0.9076\n\nEpoch 879/1000                                                                       \n252/252 - 1s - loss: 1.5490 - accuracy: 0.9738 - val_loss: 1.7277 - val_accuracy: 0.9076\n\nEpoch 880/1000                                                                       \n252/252 - 1s - loss: 1.5474 - accuracy: 0.9743 - val_loss: 1.7271 - val_accuracy: 0.9076\n\nEpoch 881/1000                                                                       \n252/252 - 1s - loss: 1.5458 - accuracy: 0.9737 - val_loss: 1.7263 - val_accuracy: 0.9076\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 882/1000                                                                       \n252/252 - 1s - loss: 1.5455 - accuracy: 0.9759 - val_loss: 1.7256 - val_accuracy: 0.9076\n\nEpoch 883/1000                                                                       \n252/252 - 1s - loss: 1.5437 - accuracy: 0.9767 - val_loss: 1.7246 - val_accuracy: 0.9076\n\nEpoch 884/1000                                                                       \n252/252 - 1s - loss: 1.5436 - accuracy: 0.9756 - val_loss: 1.7230 - val_accuracy: 0.9076\n\nEpoch 885/1000                                                                       \n252/252 - 1s - loss: 1.5428 - accuracy: 0.9747 - val_loss: 1.7227 - val_accuracy: 0.9076\n\nEpoch 886/1000                                                                       \n252/252 - 1s - loss: 1.5432 - accuracy: 0.9747 - val_loss: 1.7222 - val_accuracy: 0.9076\n\nEpoch 887/1000                                                                       \n252/252 - 1s - loss: 1.5415 - accuracy: 0.9743 - val_loss: 1.7212 - val_accuracy: 0.9076\n\nEpoch 888/1000                                                                       \n252/252 - 1s - loss: 1.5413 - accuracy: 0.9747 - val_loss: 1.7197 - val_accuracy: 0.9076\n\nEpoch 889/1000                                                                       \n252/252 - 1s - loss: 1.5398 - accuracy: 0.9746 - val_loss: 1.7183 - val_accuracy: 0.9076\n\nEpoch 890/1000                                                                       \n252/252 - 1s - loss: 1.5387 - accuracy: 0.9737 - val_loss: 1.7180 - val_accuracy: 0.9076\n\nEpoch 891/1000                                                                       \n252/252 - 1s - loss: 1.5383 - accuracy: 0.9753 - val_loss: 1.7176 - val_accuracy: 0.9076\n\nEpoch 892/1000                                                                       \n252/252 - 1s - loss: 1.5369 - accuracy: 0.9738 - val_loss: 1.7161 - val_accuracy: 0.9076\n\nEpoch 893/1000                                                                       \n252/252 - 1s - loss: 1.5359 - accuracy: 0.9752 - val_loss: 1.7157 - val_accuracy: 0.9076\n\nEpoch 894/1000                                                                       \n252/252 - 1s - loss: 1.5363 - accuracy: 0.9754 - val_loss: 1.7144 - val_accuracy: 0.9076\n\nEpoch 895/1000                                                                       \n252/252 - 1s - loss: 1.5356 - accuracy: 0.9751 - val_loss: 1.7138 - val_accuracy: 0.9076\n\nEpoch 896/1000                                                                       \n252/252 - 1s - loss: 1.5347 - accuracy: 0.9726 - val_loss: 1.7135 - val_accuracy: 0.9076\n\nEpoch 897/1000                                                                       \n252/252 - 1s - loss: 1.5345 - accuracy: 0.9759 - val_loss: 1.7118 - val_accuracy: 0.9076\n\nEpoch 898/1000                                                                       \n252/252 - 1s - loss: 1.5344 - accuracy: 0.9732 - val_loss: 1.7109 - val_accuracy: 0.9081\n\nEpoch 899/1000                                                                       \n252/252 - 1s - loss: 1.5335 - accuracy: 0.9748 - val_loss: 1.7103 - val_accuracy: 0.9076\n\nEpoch 900/1000                                                                       \n252/252 - 1s - loss: 1.5326 - accuracy: 0.9752 - val_loss: 1.7099 - val_accuracy: 0.9076\n\nEpoch 901/1000                                                                       \n252/252 - 1s - loss: 1.5316 - accuracy: 0.9738 - val_loss: 1.7096 - val_accuracy: 0.9076\n\nEpoch 902/1000                                                                       \n252/252 - 1s - loss: 1.5302 - accuracy: 0.9749 - val_loss: 1.7089 - val_accuracy: 0.9076\n\nEpoch 903/1000                                                                       \n252/252 - 1s - loss: 1.5293 - accuracy: 0.9747 - val_loss: 1.7077 - val_accuracy: 0.9076\n\nEpoch 904/1000                                                                       \n252/252 - 1s - loss: 1.5276 - accuracy: 0.9741 - val_loss: 1.7069 - val_accuracy: 0.9076\n\nEpoch 905/1000                                                                       \n252/252 - 1s - loss: 1.5280 - accuracy: 0.9756 - val_loss: 1.7066 - val_accuracy: 0.9076\n\nEpoch 906/1000                                                                       \n252/252 - 1s - loss: 1.5283 - accuracy: 0.9747 - val_loss: 1.7056 - val_accuracy: 0.9076\n\nEpoch 907/1000                                                                       \n252/252 - 1s - loss: 1.5270 - accuracy: 0.9764 - val_loss: 1.7046 - val_accuracy: 0.9076\n\nEpoch 908/1000                                                                       \n252/252 - 1s - loss: 1.5265 - accuracy: 0.9757 - val_loss: 1.7045 - val_accuracy: 0.9076\n\nEpoch 909/1000                                                                       \n252/252 - 1s - loss: 1.5255 - accuracy: 0.9753 - val_loss: 1.7025 - val_accuracy: 0.9081\n\nEpoch 910/1000                                                                       \n252/252 - 1s - loss: 1.5243 - accuracy: 0.9748 - val_loss: 1.7022 - val_accuracy: 0.9076\n\nEpoch 911/1000                                                                       \n252/252 - 1s - loss: 1.5239 - accuracy: 0.9746 - val_loss: 1.7020 - val_accuracy: 0.9076\n\nEpoch 912/1000                                                                       \n252/252 - 1s - loss: 1.5235 - accuracy: 0.9754 - val_loss: 1.7009 - val_accuracy: 0.9076\n\nEpoch 913/1000                                                                       \n252/252 - 1s - loss: 1.5227 - accuracy: 0.9749 - val_loss: 1.7000 - val_accuracy: 0.9076\n\nEpoch 914/1000                                                                       \n252/252 - 1s - loss: 1.5210 - accuracy: 0.9748 - val_loss: 1.6992 - val_accuracy: 0.9081\n\nEpoch 915/1000                                                                       \n252/252 - 1s - loss: 1.5203 - accuracy: 0.9748 - val_loss: 1.6986 - val_accuracy: 0.9076\n\nEpoch 916/1000                                                                       \n252/252 - 1s - loss: 1.5204 - accuracy: 0.9768 - val_loss: 1.6970 - val_accuracy: 0.9081\n\nEpoch 917/1000                                                                       \n252/252 - 1s - loss: 1.5193 - accuracy: 0.9743 - val_loss: 1.6963 - val_accuracy: 0.9081\n\nEpoch 918/1000                                                                       \n252/252 - 1s - loss: 1.5197 - accuracy: 0.9748 - val_loss: 1.6959 - val_accuracy: 0.9081\n\nEpoch 919/1000                                                                       \n252/252 - 1s - loss: 1.5170 - accuracy: 0.9757 - val_loss: 1.6952 - val_accuracy: 0.9081\n\nEpoch 920/1000                                                                       \n252/252 - 1s - loss: 1.5175 - accuracy: 0.9756 - val_loss: 1.6951 - val_accuracy: 0.9081\n\nEpoch 921/1000                                                                       \n252/252 - 1s - loss: 1.5170 - accuracy: 0.9743 - val_loss: 1.6935 - val_accuracy: 0.9081\n\nEpoch 922/1000                                                                       \n252/252 - 1s - loss: 1.5173 - accuracy: 0.9742 - val_loss: 1.6927 - val_accuracy: 0.9081\n\nEpoch 923/1000                                                                       \n252/252 - 1s - loss: 1.5154 - accuracy: 0.9749 - val_loss: 1.6917 - val_accuracy: 0.9081\n\nEpoch 924/1000                                                                       \n252/252 - 1s - loss: 1.5142 - accuracy: 0.9752 - val_loss: 1.6905 - val_accuracy: 0.9085\n\nEpoch 925/1000                                                                       \n252/252 - 1s - loss: 1.5135 - accuracy: 0.9762 - val_loss: 1.6910 - val_accuracy: 0.9081\n\nEpoch 926/1000                                                                       \n252/252 - 1s - loss: 1.5144 - accuracy: 0.9742 - val_loss: 1.6894 - val_accuracy: 0.9085\n\nEpoch 927/1000                                                                       \n252/252 - 1s - loss: 1.5128 - accuracy: 0.9735 - val_loss: 1.6883 - val_accuracy: 0.9085\n\nEpoch 928/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 1.5109 - accuracy: 0.9766 - val_loss: 1.6874 - val_accuracy: 0.9085\n\nEpoch 929/1000                                                                       \n252/252 - 1s - loss: 1.5123 - accuracy: 0.9748 - val_loss: 1.6864 - val_accuracy: 0.9090\n\nEpoch 930/1000                                                                       \n252/252 - 1s - loss: 1.5105 - accuracy: 0.9759 - val_loss: 1.6861 - val_accuracy: 0.9085\n\nEpoch 931/1000                                                                       \n252/252 - 1s - loss: 1.5097 - accuracy: 0.9769 - val_loss: 1.6853 - val_accuracy: 0.9090\n\nEpoch 932/1000                                                                       \n252/252 - 1s - loss: 1.5099 - accuracy: 0.9749 - val_loss: 1.6849 - val_accuracy: 0.9085\n\nEpoch 933/1000                                                                       \n252/252 - 1s - loss: 1.5087 - accuracy: 0.9763 - val_loss: 1.6841 - val_accuracy: 0.9085\n\nEpoch 934/1000                                                                       \n252/252 - 1s - loss: 1.5085 - accuracy: 0.9753 - val_loss: 1.6829 - val_accuracy: 0.9090\n\nEpoch 935/1000                                                                       \n252/252 - 1s - loss: 1.5065 - accuracy: 0.9762 - val_loss: 1.6827 - val_accuracy: 0.9085\n\nEpoch 936/1000                                                                       \n252/252 - 1s - loss: 1.5065 - accuracy: 0.9752 - val_loss: 1.6817 - val_accuracy: 0.9085\n\nEpoch 937/1000                                                                       \n252/252 - 1s - loss: 1.5054 - accuracy: 0.9762 - val_loss: 1.6809 - val_accuracy: 0.9085\n\nEpoch 938/1000                                                                       \n252/252 - 1s - loss: 1.5044 - accuracy: 0.9757 - val_loss: 1.6806 - val_accuracy: 0.9090\n\nEpoch 939/1000                                                                       \n252/252 - 1s - loss: 1.5035 - accuracy: 0.9769 - val_loss: 1.6796 - val_accuracy: 0.9090\n\nEpoch 940/1000                                                                       \n252/252 - 1s - loss: 1.5045 - accuracy: 0.9751 - val_loss: 1.6784 - val_accuracy: 0.9090\n\nEpoch 941/1000                                                                       \n252/252 - 1s - loss: 1.5021 - accuracy: 0.9768 - val_loss: 1.6775 - val_accuracy: 0.9100\n\nEpoch 942/1000                                                                       \n252/252 - 1s - loss: 1.5014 - accuracy: 0.9771 - val_loss: 1.6774 - val_accuracy: 0.9100\n\nEpoch 943/1000                                                                       \n252/252 - 1s - loss: 1.5015 - accuracy: 0.9762 - val_loss: 1.6759 - val_accuracy: 0.9090\n\nEpoch 944/1000                                                                       \n252/252 - 1s - loss: 1.5001 - accuracy: 0.9769 - val_loss: 1.6761 - val_accuracy: 0.9085\n\nEpoch 945/1000                                                                       \n252/252 - 1s - loss: 1.4996 - accuracy: 0.9759 - val_loss: 1.6750 - val_accuracy: 0.9085\n\nEpoch 946/1000                                                                       \n252/252 - 1s - loss: 1.4995 - accuracy: 0.9754 - val_loss: 1.6734 - val_accuracy: 0.9095\n\nEpoch 947/1000                                                                       \n252/252 - 1s - loss: 1.4978 - accuracy: 0.9762 - val_loss: 1.6733 - val_accuracy: 0.9095\n\nEpoch 948/1000                                                                       \n252/252 - 1s - loss: 1.4980 - accuracy: 0.9761 - val_loss: 1.6725 - val_accuracy: 0.9095\n\nEpoch 949/1000                                                                       \n252/252 - 1s - loss: 1.4965 - accuracy: 0.9758 - val_loss: 1.6714 - val_accuracy: 0.9095\n\nEpoch 950/1000                                                                       \n252/252 - 1s - loss: 1.4965 - accuracy: 0.9748 - val_loss: 1.6705 - val_accuracy: 0.9095\n\nEpoch 951/1000                                                                       \n252/252 - 1s - loss: 1.4959 - accuracy: 0.9766 - val_loss: 1.6698 - val_accuracy: 0.9100\n\nEpoch 952/1000                                                                       \n252/252 - 1s - loss: 1.4948 - accuracy: 0.9764 - val_loss: 1.6699 - val_accuracy: 0.9100\n\nEpoch 953/1000                                                                       \n252/252 - 1s - loss: 1.4946 - accuracy: 0.9756 - val_loss: 1.6691 - val_accuracy: 0.9095\n\nEpoch 954/1000                                                                       \n252/252 - 1s - loss: 1.4943 - accuracy: 0.9761 - val_loss: 1.6685 - val_accuracy: 0.9095\n\nEpoch 955/1000                                                                       \n252/252 - 1s - loss: 1.4932 - accuracy: 0.9768 - val_loss: 1.6669 - val_accuracy: 0.9095\n\nEpoch 956/1000                                                                       \n252/252 - 1s - loss: 1.4915 - accuracy: 0.9773 - val_loss: 1.6664 - val_accuracy: 0.9105\n\nEpoch 957/1000                                                                       \n252/252 - 1s - loss: 1.4920 - accuracy: 0.9758 - val_loss: 1.6661 - val_accuracy: 0.9110\n\nEpoch 958/1000                                                                       \n252/252 - 1s - loss: 1.4906 - accuracy: 0.9771 - val_loss: 1.6655 - val_accuracy: 0.9110\n\nEpoch 959/1000                                                                       \n252/252 - 1s - loss: 1.4899 - accuracy: 0.9763 - val_loss: 1.6644 - val_accuracy: 0.9110\n\nEpoch 960/1000                                                                       \n252/252 - 1s - loss: 1.4901 - accuracy: 0.9747 - val_loss: 1.6635 - val_accuracy: 0.9110\n\nEpoch 961/1000                                                                       \n252/252 - 1s - loss: 1.4892 - accuracy: 0.9775 - val_loss: 1.6618 - val_accuracy: 0.9115\n\nEpoch 962/1000                                                                       \n252/252 - 1s - loss: 1.4884 - accuracy: 0.9761 - val_loss: 1.6612 - val_accuracy: 0.9110\n\nEpoch 963/1000                                                                       \n252/252 - 1s - loss: 1.4863 - accuracy: 0.9780 - val_loss: 1.6610 - val_accuracy: 0.9110\n\nEpoch 964/1000                                                                       \n252/252 - 1s - loss: 1.4873 - accuracy: 0.9769 - val_loss: 1.6603 - val_accuracy: 0.9115\n\nEpoch 965/1000                                                                       \n252/252 - 1s - loss: 1.4869 - accuracy: 0.9761 - val_loss: 1.6595 - val_accuracy: 0.9120\n\nEpoch 966/1000                                                                       \n252/252 - 1s - loss: 1.4846 - accuracy: 0.9771 - val_loss: 1.6592 - val_accuracy: 0.9115\n\nEpoch 967/1000                                                                       \n252/252 - 1s - loss: 1.4852 - accuracy: 0.9768 - val_loss: 1.6587 - val_accuracy: 0.9105\n\nEpoch 968/1000                                                                       \n252/252 - 1s - loss: 1.4846 - accuracy: 0.9768 - val_loss: 1.6584 - val_accuracy: 0.9110\n\nEpoch 969/1000                                                                       \n252/252 - 1s - loss: 1.4850 - accuracy: 0.9754 - val_loss: 1.6569 - val_accuracy: 0.9125\n\nEpoch 970/1000                                                                       \n252/252 - 1s - loss: 1.4833 - accuracy: 0.9771 - val_loss: 1.6568 - val_accuracy: 0.9115\n\nEpoch 971/1000                                                                       \n252/252 - 1s - loss: 1.4824 - accuracy: 0.9766 - val_loss: 1.6564 - val_accuracy: 0.9115\n\nEpoch 972/1000                                                                       \n252/252 - 1s - loss: 1.4814 - accuracy: 0.9761 - val_loss: 1.6552 - val_accuracy: 0.9115\n\nEpoch 973/1000                                                                       \n252/252 - 1s - loss: 1.4814 - accuracy: 0.9762 - val_loss: 1.6541 - val_accuracy: 0.9120\n\nEpoch 974/1000                                                                       \n252/252 - 1s - loss: 1.4800 - accuracy: 0.9763 - val_loss: 1.6538 - val_accuracy: 0.9120\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 975/1000                                                                       \n252/252 - 1s - loss: 1.4814 - accuracy: 0.9751 - val_loss: 1.6530 - val_accuracy: 0.9120\n\nEpoch 976/1000                                                                       \n252/252 - 1s - loss: 1.4789 - accuracy: 0.9778 - val_loss: 1.6524 - val_accuracy: 0.9120\n\nEpoch 977/1000                                                                       \n252/252 - 1s - loss: 1.4777 - accuracy: 0.9771 - val_loss: 1.6515 - val_accuracy: 0.9120\n\nEpoch 978/1000                                                                       \n252/252 - 1s - loss: 1.4785 - accuracy: 0.9762 - val_loss: 1.6504 - val_accuracy: 0.9125\n\nEpoch 979/1000                                                                       \n252/252 - 1s - loss: 1.4778 - accuracy: 0.9767 - val_loss: 1.6502 - val_accuracy: 0.9125\n\nEpoch 980/1000                                                                       \n252/252 - 1s - loss: 1.4769 - accuracy: 0.9768 - val_loss: 1.6486 - val_accuracy: 0.9130\n\nEpoch 981/1000                                                                       \n252/252 - 1s - loss: 1.4755 - accuracy: 0.9774 - val_loss: 1.6485 - val_accuracy: 0.9130\n\nEpoch 982/1000                                                                       \n252/252 - 1s - loss: 1.4761 - accuracy: 0.9768 - val_loss: 1.6481 - val_accuracy: 0.9125\n\nEpoch 983/1000                                                                       \n252/252 - 1s - loss: 1.4745 - accuracy: 0.9771 - val_loss: 1.6471 - val_accuracy: 0.9130\n\nEpoch 984/1000                                                                       \n252/252 - 1s - loss: 1.4734 - accuracy: 0.9778 - val_loss: 1.6466 - val_accuracy: 0.9125\n\nEpoch 985/1000                                                                       \n252/252 - 1s - loss: 1.4735 - accuracy: 0.9773 - val_loss: 1.6461 - val_accuracy: 0.9125\n\nEpoch 986/1000                                                                       \n252/252 - 1s - loss: 1.4725 - accuracy: 0.9774 - val_loss: 1.6451 - val_accuracy: 0.9130\n\nEpoch 987/1000                                                                       \n252/252 - 1s - loss: 1.4728 - accuracy: 0.9753 - val_loss: 1.6438 - val_accuracy: 0.9130\n\nEpoch 988/1000                                                                       \n252/252 - 1s - loss: 1.4712 - accuracy: 0.9772 - val_loss: 1.6427 - val_accuracy: 0.9145\n\nEpoch 989/1000                                                                       \n252/252 - 1s - loss: 1.4702 - accuracy: 0.9779 - val_loss: 1.6421 - val_accuracy: 0.9155\n\nEpoch 990/1000                                                                       \n252/252 - 1s - loss: 1.4694 - accuracy: 0.9800 - val_loss: 1.6419 - val_accuracy: 0.9135\n\nEpoch 991/1000                                                                       \n252/252 - 1s - loss: 1.4707 - accuracy: 0.9780 - val_loss: 1.6404 - val_accuracy: 0.9145\n\nEpoch 992/1000                                                                       \n252/252 - 1s - loss: 1.4686 - accuracy: 0.9773 - val_loss: 1.6405 - val_accuracy: 0.9140\n\nEpoch 993/1000                                                                       \n252/252 - 1s - loss: 1.4687 - accuracy: 0.9767 - val_loss: 1.6396 - val_accuracy: 0.9140\n\nEpoch 994/1000                                                                       \n252/252 - 1s - loss: 1.4682 - accuracy: 0.9775 - val_loss: 1.6393 - val_accuracy: 0.9145\n\nEpoch 995/1000                                                                       \n252/252 - 1s - loss: 1.4668 - accuracy: 0.9790 - val_loss: 1.6384 - val_accuracy: 0.9145\n\nEpoch 996/1000                                                                       \n252/252 - 1s - loss: 1.4660 - accuracy: 0.9785 - val_loss: 1.6378 - val_accuracy: 0.9140\n\nEpoch 997/1000                                                                       \n252/252 - 1s - loss: 1.4663 - accuracy: 0.9768 - val_loss: 1.6373 - val_accuracy: 0.9140\n\nEpoch 998/1000                                                                       \n252/252 - 1s - loss: 1.4655 - accuracy: 0.9764 - val_loss: 1.6358 - val_accuracy: 0.9150\n\nEpoch 999/1000                                                                       \n252/252 - 1s - loss: 1.4642 - accuracy: 0.9773 - val_loss: 1.6354 - val_accuracy: 0.9150\n\nEpoch 1000/1000                                                                      \n252/252 - 1s - loss: 1.4637 - accuracy: 0.9771 - val_loss: 1.6345 - val_accuracy: 0.9155\n\n 1/63 [..............................]                                               \n - ETA: 0s - loss: 1.4689 - accuracy: 0.9688                                         \n                                                                                    \n18/63 [=======>......................]                                               \n - ETA: 0s - loss: 1.5146 - accuracy: 0.9688                                         \n                                                                                    \n34/63 [===============>..............]                                               \n - ETA: 0s - loss: 1.5518 - accuracy: 0.9375                                         \n                                                                                    \n49/63 [======================>.......]                                               \n - ETA: 0s - loss: 1.6374 - accuracy: 0.9190                                         \n                                                                                    \n63/63 [==============================]                                               \n - 0s 3ms/step - loss: 1.6345 - accuracy: 0.9155                                     \n\nValidation Accuracy: 91.5507%                                                        \nValidation Loss: 1.6345497369766235                                                  \nTest Accuracy: 0.9712837837837838                                                    \n*_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_**_*\nParameters testing:                                                                  \n{'act_reg': 0.001, 'activation_function': 'selu', 'beta_1': 0.5, 'beta_2': 0.7, 'bias_reg': 1e-07, 'hidden1': 256, 'ker_reg': 0.1, 'lr': 0.0001}\nEpoch 1/1000                                                                         \n252/252 - 1s - loss: 28.5521 - accuracy: 0.6430 - val_loss: 15.4988 - val_accuracy: 0.8146\n\nEpoch 2/1000                                                                         \n252/252 - 1s - loss: 8.8899 - accuracy: 0.8944 - val_loss: 4.7494 - val_accuracy: 0.8101\n\nEpoch 3/1000                                                                         \n252/252 - 1s - loss: 2.8580 - accuracy: 0.9294 - val_loss: 1.9342 - val_accuracy: 0.8559\n\nEpoch 4/1000                                                                         \n252/252 - 1s - loss: 1.3350 - accuracy: 0.9450 - val_loss: 1.1864 - val_accuracy: 0.8434\n\nEpoch 5/1000                                                                         \n252/252 - 1s - loss: 0.8476 - accuracy: 0.9551 - val_loss: 0.8758 - val_accuracy: 0.9170\n\nEpoch 6/1000                                                                         \n252/252 - 1s - loss: 0.6478 - accuracy: 0.9562 - val_loss: 0.7104 - val_accuracy: 0.9389\n\nEpoch 7/1000                                                                         \n252/252 - 1s - loss: 0.5495 - accuracy: 0.9619 - val_loss: 0.6298 - val_accuracy: 0.9394\n\nEpoch 8/1000                                                                         \n252/252 - 1s - loss: 0.4889 - accuracy: 0.9622 - val_loss: 0.5910 - val_accuracy: 0.9250\n\nEpoch 9/1000                                                                         \n252/252 - 1s - loss: 0.4511 - accuracy: 0.9627 - val_loss: 0.5631 - val_accuracy: 0.9309\n\nEpoch 10/1000                                                                        \n252/252 - 1s - loss: 0.4270 - accuracy: 0.9620 - val_loss: 0.5285 - val_accuracy: 0.9369\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 11/1000                                                                        \n252/252 - 1s - loss: 0.4012 - accuracy: 0.9668 - val_loss: 0.5433 - val_accuracy: 0.8976\n\nEpoch 12/1000                                                                        \n252/252 - 1s - loss: 0.3898 - accuracy: 0.9646 - val_loss: 0.5270 - val_accuracy: 0.9269\n\nEpoch 13/1000                                                                        \n252/252 - 1s - loss: 0.3732 - accuracy: 0.9672 - val_loss: 0.4775 - val_accuracy: 0.9483\n\nEpoch 14/1000                                                                        \n252/252 - 1s - loss: 0.3623 - accuracy: 0.9672 - val_loss: 0.4856 - val_accuracy: 0.9344\n\nEpoch 15/1000                                                                        \n252/252 - 1s - loss: 0.3519 - accuracy: 0.9689 - val_loss: 0.5902 - val_accuracy: 0.8464\n\nEpoch 16/1000                                                                        \n252/252 - 1s - loss: 0.3399 - accuracy: 0.9689 - val_loss: 0.4987 - val_accuracy: 0.8961\n\nEpoch 17/1000                                                                        \n252/252 - 1s - loss: 0.3326 - accuracy: 0.9708 - val_loss: 0.5093 - val_accuracy: 0.9041\n\nEpoch 18/1000                                                                        \n252/252 - 1s - loss: 0.3284 - accuracy: 0.9689 - val_loss: 0.4351 - val_accuracy: 0.9205\n\nEpoch 19/1000                                                                        \n252/252 - 1s - loss: 0.3231 - accuracy: 0.9658 - val_loss: 0.3882 - val_accuracy: 0.9657\n\nEpoch 20/1000                                                                        \n252/252 - 1s - loss: 0.3208 - accuracy: 0.9661 - val_loss: 0.4908 - val_accuracy: 0.8956\n\nEpoch 21/1000                                                                        \n252/252 - 1s - loss: 0.3087 - accuracy: 0.9692 - val_loss: 0.4552 - val_accuracy: 0.9324\n\nEpoch 22/1000                                                                        \n252/252 - 1s - loss: 0.3090 - accuracy: 0.9664 - val_loss: 0.4095 - val_accuracy: 0.9428\n\nEpoch 23/1000                                                                        \n252/252 - 1s - loss: 0.3018 - accuracy: 0.9697 - val_loss: 0.3601 - val_accuracy: 0.9587\n\nEpoch 24/1000                                                                        \n252/252 - 1s - loss: 0.2965 - accuracy: 0.9694 - val_loss: 0.4118 - val_accuracy: 0.9254\n\nEpoch 25/1000                                                                        \n252/252 - 1s - loss: 0.2888 - accuracy: 0.9718 - val_loss: 0.4278 - val_accuracy: 0.9250\n\nEpoch 26/1000                                                                        \n252/252 - 1s - loss: 0.2873 - accuracy: 0.9692 - val_loss: 0.3360 - val_accuracy: 0.9677\n\nEpoch 27/1000                                                                        \n252/252 - 1s - loss: 0.2875 - accuracy: 0.9676 - val_loss: 0.4869 - val_accuracy: 0.8723\n\nEpoch 28/1000                                                                        \n252/252 - 1s - loss: 0.2807 - accuracy: 0.9682 - val_loss: 0.3253 - val_accuracy: 0.9747\n\nEpoch 29/1000                                                                        \n252/252 - 1s - loss: 0.2807 - accuracy: 0.9680 - val_loss: 0.3229 - val_accuracy: 0.9712\n\nEpoch 30/1000                                                                        \n252/252 - 1s - loss: 0.2741 - accuracy: 0.9727 - val_loss: 0.3432 - val_accuracy: 0.9702\n\nEpoch 31/1000                                                                        \n252/252 - 1s - loss: 0.2725 - accuracy: 0.9711 - val_loss: 0.5100 - val_accuracy: 0.8569\n\nEpoch 32/1000                                                                        \n252/252 - 1s - loss: 0.2674 - accuracy: 0.9696 - val_loss: 0.3607 - val_accuracy: 0.9468\n\nEpoch 33/1000                                                                        \n252/252 - 1s - loss: 0.2721 - accuracy: 0.9672 - val_loss: 0.4519 - val_accuracy: 0.9006\n\nEpoch 34/1000                                                                        \n252/252 - 1s - loss: 0.2648 - accuracy: 0.9711 - val_loss: 0.3801 - val_accuracy: 0.9344\n\nEpoch 35/1000                                                                        \n252/252 - 1s - loss: 0.2653 - accuracy: 0.9697 - val_loss: 0.4829 - val_accuracy: 0.8807\n\nEpoch 36/1000                                                                        \n252/252 - 1s - loss: 0.2622 - accuracy: 0.9696 - val_loss: 0.3202 - val_accuracy: 0.9558\n\nEpoch 37/1000                                                                        \n252/252 - 1s - loss: 0.2595 - accuracy: 0.9706 - val_loss: 0.3722 - val_accuracy: 0.9359\n\nEpoch 38/1000                                                                        \n252/252 - 1s - loss: 0.2597 - accuracy: 0.9700 - val_loss: 0.3415 - val_accuracy: 0.9344\n\nEpoch 39/1000                                                                        \n252/252 - 1s - loss: 0.2614 - accuracy: 0.9674 - val_loss: 0.3078 - val_accuracy: 0.9677\n\nEpoch 40/1000                                                                        \n252/252 - 1s - loss: 0.2572 - accuracy: 0.9689 - val_loss: 0.3268 - val_accuracy: 0.9508\n\nEpoch 41/1000                                                                        \n252/252 - 1s - loss: 0.2559 - accuracy: 0.9681 - val_loss: 0.3892 - val_accuracy: 0.9245\n\nEpoch 42/1000                                                                        \n252/252 - 1s - loss: 0.2517 - accuracy: 0.9695 - val_loss: 0.3400 - val_accuracy: 0.9533\n\nEpoch 43/1000                                                                        \n252/252 - 1s - loss: 0.2532 - accuracy: 0.9669 - val_loss: 0.4422 - val_accuracy: 0.8837\n\nEpoch 44/1000                                                                        \n252/252 - 1s - loss: 0.2503 - accuracy: 0.9682 - val_loss: 0.2880 - val_accuracy: 0.9642\n\nEpoch 45/1000                                                                        \n252/252 - 1s - loss: 0.2442 - accuracy: 0.9727 - val_loss: 0.3678 - val_accuracy: 0.9155\n\nEpoch 46/1000                                                                        \n252/252 - 1s - loss: 0.2459 - accuracy: 0.9731 - val_loss: 0.3464 - val_accuracy: 0.9309\n\nEpoch 47/1000                                                                        \n252/252 - 1s - loss: 0.2536 - accuracy: 0.9669 - val_loss: 0.3490 - val_accuracy: 0.9289\n\nEpoch 48/1000                                                                        \n252/252 - 1s - loss: 0.2474 - accuracy: 0.9680 - val_loss: 0.3175 - val_accuracy: 0.9563\n\nEpoch 49/1000                                                                        \n252/252 - 1s - loss: 0.2493 - accuracy: 0.9661 - val_loss: 0.4073 - val_accuracy: 0.8961\n\nEpoch 50/1000                                                                        \n252/252 - 1s - loss: 0.2433 - accuracy: 0.9716 - val_loss: 0.3782 - val_accuracy: 0.9115\n\nEpoch 51/1000                                                                        \n252/252 - 1s - loss: 0.2459 - accuracy: 0.9691 - val_loss: 0.3310 - val_accuracy: 0.9463\n\nEpoch 52/1000                                                                        \n252/252 - 1s - loss: 0.2455 - accuracy: 0.9694 - val_loss: 0.3314 - val_accuracy: 0.9578\n\nEpoch 53/1000                                                                        \n252/252 - 1s - loss: 0.2426 - accuracy: 0.9695 - val_loss: 0.3962 - val_accuracy: 0.9081\n\nEpoch 54/1000                                                                        \n252/252 - 1s - loss: 0.2341 - accuracy: 0.9727 - val_loss: 0.5345 - val_accuracy: 0.8131\n\nEpoch 55/1000                                                                        \n252/252 - 1s - loss: 0.2405 - accuracy: 0.9701 - val_loss: 0.2857 - val_accuracy: 0.9682\n\nEpoch 56/1000                                                                        \n252/252 - 1s - loss: 0.2364 - accuracy: 0.9694 - val_loss: 0.2918 - val_accuracy: 0.9592\n\nEpoch 57/1000                                                                        \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.2374 - accuracy: 0.9692 - val_loss: 0.6825 - val_accuracy: 0.7460\n\nEpoch 58/1000                                                                        \n252/252 - 1s - loss: 0.2400 - accuracy: 0.9670 - val_loss: 0.3555 - val_accuracy: 0.9379\n\nEpoch 59/1000                                                                        \n252/252 - 1s - loss: 0.2350 - accuracy: 0.9684 - val_loss: 0.3460 - val_accuracy: 0.9245\n\nEpoch 60/1000                                                                        \n252/252 - 1s - loss: 0.2328 - accuracy: 0.9715 - val_loss: 0.3776 - val_accuracy: 0.8996\n\nEpoch 61/1000                                                                        \n252/252 - 1s - loss: 0.2350 - accuracy: 0.9687 - val_loss: 0.2802 - val_accuracy: 0.9587\n\nEpoch 62/1000                                                                        \n252/252 - 1s - loss: 0.2414 - accuracy: 0.9670 - val_loss: 0.2854 - val_accuracy: 0.9578\n\nEpoch 63/1000                                                                        \n252/252 - 1s - loss: 0.2372 - accuracy: 0.9663 - val_loss: 0.2845 - val_accuracy: 0.9622\n\nEpoch 64/1000                                                                        \n252/252 - 1s - loss: 0.2338 - accuracy: 0.9677 - val_loss: 0.3830 - val_accuracy: 0.9076\n\nEpoch 65/1000                                                                        \n252/252 - 1s - loss: 0.2290 - accuracy: 0.9704 - val_loss: 0.2829 - val_accuracy: 0.9607\n\nEpoch 66/1000                                                                        \n252/252 - 1s - loss: 0.2355 - accuracy: 0.9670 - val_loss: 0.3137 - val_accuracy: 0.9409\n\nEpoch 67/1000                                                                        \n252/252 - 1s - loss: 0.2338 - accuracy: 0.9685 - val_loss: 0.3876 - val_accuracy: 0.9041\n\nEpoch 68/1000                                                                        \n252/252 - 1s - loss: 0.2297 - accuracy: 0.9700 - val_loss: 0.3601 - val_accuracy: 0.9225\n\nEpoch 69/1000                                                                        \n252/252 - 1s - loss: 0.2264 - accuracy: 0.9679 - val_loss: 0.4540 - val_accuracy: 0.8564\n\nEpoch 70/1000                                                                        \n252/252 - 1s - loss: 0.2273 - accuracy: 0.9689 - val_loss: 0.3117 - val_accuracy: 0.9394\n\nEpoch 71/1000                                                                        \n252/252 - 1s - loss: 0.2281 - accuracy: 0.9690 - val_loss: 0.3715 - val_accuracy: 0.9090\n\nEpoch 72/1000                                                                        \n252/252 - 1s - loss: 0.2256 - accuracy: 0.9687 - val_loss: 0.2730 - val_accuracy: 0.9657\n\nEpoch 73/1000                                                                        \n252/252 - 1s - loss: 0.2332 - accuracy: 0.9646 - val_loss: 0.2857 - val_accuracy: 0.9533\n\nEpoch 74/1000                                                                        \n252/252 - 1s - loss: 0.2278 - accuracy: 0.9686 - val_loss: 0.2816 - val_accuracy: 0.9548\n\nEpoch 75/1000                                                                        \n252/252 - 1s - loss: 0.2233 - accuracy: 0.9681 - val_loss: 0.6254 - val_accuracy: 0.7813\n\nEpoch 76/1000                                                                        \n252/252 - 1s - loss: 0.2327 - accuracy: 0.9632 - val_loss: 0.3115 - val_accuracy: 0.9448\n\nEpoch 77/1000                                                                        \n252/252 - 1s - loss: 0.2234 - accuracy: 0.9685 - val_loss: 0.3799 - val_accuracy: 0.9011\n\nEpoch 78/1000                                                                        \n252/252 - 1s - loss: 0.2267 - accuracy: 0.9672 - val_loss: 0.3495 - val_accuracy: 0.9081\n\nEpoch 79/1000                                                                        \n252/252 - 1s - loss: 0.2287 - accuracy: 0.9664 - val_loss: 0.2686 - val_accuracy: 0.9637\n\nEpoch 80/1000                                                                        \n252/252 - 1s - loss: 0.2252 - accuracy: 0.9672 - val_loss: 0.3218 - val_accuracy: 0.9344\n\nEpoch 81/1000                                                                        \n252/252 - 1s - loss: 0.2301 - accuracy: 0.9655 - val_loss: 0.2787 - val_accuracy: 0.9642\n\nEpoch 82/1000                                                                        \n252/252 - 1s - loss: 0.2269 - accuracy: 0.9654 - val_loss: 0.3877 - val_accuracy: 0.9036\n\nEpoch 83/1000                                                                        \n252/252 - 1s - loss: 0.2238 - accuracy: 0.9677 - val_loss: 0.3066 - val_accuracy: 0.9538\n\nEpoch 84/1000                                                                        \n252/252 - 1s - loss: 0.2199 - accuracy: 0.9690 - val_loss: 0.7363 - val_accuracy: 0.7490\n\nEpoch 85/1000                                                                        \n252/252 - 1s - loss: 0.2314 - accuracy: 0.9643 - val_loss: 0.3187 - val_accuracy: 0.9414\n\nEpoch 86/1000                                                                        \n252/252 - 1s - loss: 0.2222 - accuracy: 0.9677 - val_loss: 0.3051 - val_accuracy: 0.9414\n\nEpoch 87/1000                                                                        \n252/252 - 1s - loss: 0.2232 - accuracy: 0.9687 - val_loss: 0.2519 - val_accuracy: 0.9617\n\nEpoch 88/1000                                                                        \n252/252 - 1s - loss: 0.2224 - accuracy: 0.9649 - val_loss: 0.2347 - val_accuracy: 0.9761\n\nEpoch 89/1000                                                                        \n252/252 - 1s - loss: 0.2232 - accuracy: 0.9672 - val_loss: 0.3810 - val_accuracy: 0.9011\n\nEpoch 90/1000                                                                        \n252/252 - 1s - loss: 0.2202 - accuracy: 0.9675 - val_loss: 0.3989 - val_accuracy: 0.8872\n\nEpoch 91/1000                                                                        \n252/252 - 1s - loss: 0.2185 - accuracy: 0.9695 - val_loss: 0.2460 - val_accuracy: 0.9667\n\nEpoch 92/1000                                                                        \n252/252 - 1s - loss: 0.2191 - accuracy: 0.9687 - val_loss: 0.3237 - val_accuracy: 0.9225\n\nEpoch 93/1000                                                                        \n252/252 - 1s - loss: 0.2178 - accuracy: 0.9670 - val_loss: 0.2930 - val_accuracy: 0.9438\n\nEpoch 94/1000                                                                        \n252/252 - 1s - loss: 0.2180 - accuracy: 0.9682 - val_loss: 0.2207 - val_accuracy: 0.9751\n\nEpoch 95/1000                                                                        \n252/252 - 1s - loss: 0.2307 - accuracy: 0.9645 - val_loss: 0.2629 - val_accuracy: 0.9592\n\nEpoch 96/1000                                                                        \n252/252 - 1s - loss: 0.2207 - accuracy: 0.9660 - val_loss: 0.2669 - val_accuracy: 0.9702\n\nEpoch 97/1000                                                                        \n252/252 - 1s - loss: 0.2279 - accuracy: 0.9632 - val_loss: 0.3437 - val_accuracy: 0.9240\n\nEpoch 98/1000                                                                        \n252/252 - 1s - loss: 0.2207 - accuracy: 0.9668 - val_loss: 0.2986 - val_accuracy: 0.9483\n\nEpoch 99/1000                                                                        \n252/252 - 1s - loss: 0.2235 - accuracy: 0.9663 - val_loss: 0.2350 - val_accuracy: 0.9776\n\nEpoch 100/1000                                                                       \n252/252 - 1s - loss: 0.2225 - accuracy: 0.9658 - val_loss: 0.2450 - val_accuracy: 0.9637\n\nEpoch 101/1000                                                                       \n252/252 - 1s - loss: 0.2153 - accuracy: 0.9694 - val_loss: 0.4205 - val_accuracy: 0.8822\n\nEpoch 102/1000                                                                       \n252/252 - 1s - loss: 0.2159 - accuracy: 0.9680 - val_loss: 0.2873 - val_accuracy: 0.9428\n\nEpoch 103/1000                                                                       \n252/252 - 1s - loss: 0.2174 - accuracy: 0.9666 - val_loss: 0.5104 - val_accuracy: 0.8549\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 104/1000                                                                       \n252/252 - 1s - loss: 0.2274 - accuracy: 0.9634 - val_loss: 0.5188 - val_accuracy: 0.8290\n\nEpoch 105/1000                                                                       \n252/252 - 1s - loss: 0.2161 - accuracy: 0.9684 - val_loss: 0.2277 - val_accuracy: 0.9717\n\nEpoch 106/1000                                                                       \n252/252 - 1s - loss: 0.2227 - accuracy: 0.9635 - val_loss: 0.2271 - val_accuracy: 0.9737\n\nEpoch 107/1000                                                                       \n252/252 - 1s - loss: 0.2276 - accuracy: 0.9630 - val_loss: 0.3710 - val_accuracy: 0.9061\n\nEpoch 108/1000                                                                       \n252/252 - 1s - loss: 0.2150 - accuracy: 0.9670 - val_loss: 0.5713 - val_accuracy: 0.8057\n\nEpoch 109/1000                                                                       \n252/252 - 1s - loss: 0.2239 - accuracy: 0.9625 - val_loss: 0.3597 - val_accuracy: 0.9090\n\nEpoch 110/1000                                                                       \n252/252 - 1s - loss: 0.2142 - accuracy: 0.9684 - val_loss: 0.2572 - val_accuracy: 0.9578\n\nEpoch 111/1000                                                                       \n252/252 - 1s - loss: 0.2162 - accuracy: 0.9675 - val_loss: 0.2882 - val_accuracy: 0.9468\n\nEpoch 112/1000                                                                       \n252/252 - 1s - loss: 0.2262 - accuracy: 0.9608 - val_loss: 0.4225 - val_accuracy: 0.8752\n\nEpoch 113/1000                                                                       \n252/252 - 1s - loss: 0.2148 - accuracy: 0.9646 - val_loss: 0.6597 - val_accuracy: 0.7694\n\nEpoch 114/1000                                                                       \n252/252 - 1s - loss: 0.2184 - accuracy: 0.9680 - val_loss: 0.5593 - val_accuracy: 0.8017\n\nEpoch 115/1000                                                                       \n252/252 - 1s - loss: 0.2203 - accuracy: 0.9646 - val_loss: 0.2935 - val_accuracy: 0.9359\n\nEpoch 116/1000                                                                       \n252/252 - 1s - loss: 0.2191 - accuracy: 0.9670 - val_loss: 0.6709 - val_accuracy: 0.7858\n\nEpoch 117/1000                                                                       \n252/252 - 1s - loss: 0.2184 - accuracy: 0.9658 - val_loss: 0.3027 - val_accuracy: 0.9364\n\nEpoch 118/1000                                                                       \n252/252 - 1s - loss: 0.2156 - accuracy: 0.9656 - val_loss: 0.3860 - val_accuracy: 0.9031\n\nEpoch 119/1000                                                                       \n252/252 - 1s - loss: 0.2193 - accuracy: 0.9653 - val_loss: 0.2225 - val_accuracy: 0.9732\n\nEpoch 120/1000                                                                       \n252/252 - 1s - loss: 0.2098 - accuracy: 0.9689 - val_loss: 0.2644 - val_accuracy: 0.9632\n\nEpoch 121/1000                                                                       \n252/252 - 1s - loss: 0.2232 - accuracy: 0.9628 - val_loss: 0.5862 - val_accuracy: 0.7903\n\nEpoch 122/1000                                                                       \n252/252 - 1s - loss: 0.2148 - accuracy: 0.9674 - val_loss: 0.2679 - val_accuracy: 0.9657\n\nEpoch 123/1000                                                                       \n252/252 - 1s - loss: 0.2160 - accuracy: 0.9672 - val_loss: 0.3742 - val_accuracy: 0.9071\n\nEpoch 124/1000                                                                       \n252/252 - 1s - loss: 0.2183 - accuracy: 0.9645 - val_loss: 0.2389 - val_accuracy: 0.9652\n\nEpoch 125/1000                                                                       \n252/252 - 1s - loss: 0.2165 - accuracy: 0.9641 - val_loss: 0.2509 - val_accuracy: 0.9558\n\nEpoch 126/1000                                                                       \n252/252 - 1s - loss: 0.2154 - accuracy: 0.9639 - val_loss: 0.3229 - val_accuracy: 0.9329\n\nEpoch 127/1000                                                                       \n252/252 - 1s - loss: 0.2079 - accuracy: 0.9696 - val_loss: 0.3150 - val_accuracy: 0.9364\n\nEpoch 128/1000                                                                       \n252/252 - 1s - loss: 0.2098 - accuracy: 0.9675 - val_loss: 0.6728 - val_accuracy: 0.7828\n\nEpoch 129/1000                                                                       \n252/252 - 1s - loss: 0.2202 - accuracy: 0.9640 - val_loss: 0.2233 - val_accuracy: 0.9761\n\nEpoch 130/1000                                                                       \n252/252 - 1s - loss: 0.2136 - accuracy: 0.9670 - val_loss: 0.2773 - val_accuracy: 0.9503\n\nEpoch 131/1000                                                                       \n252/252 - 1s - loss: 0.2127 - accuracy: 0.9666 - val_loss: 0.2500 - val_accuracy: 0.9583\n\nEpoch 132/1000                                                                       \n252/252 - 1s - loss: 0.2151 - accuracy: 0.9668 - val_loss: 0.3768 - val_accuracy: 0.9006\n\nEpoch 133/1000                                                                       \n252/252 - 1s - loss: 0.2160 - accuracy: 0.9663 - val_loss: 0.4485 - val_accuracy: 0.8633\n\nEpoch 134/1000                                                                       \n252/252 - 1s - loss: 0.2185 - accuracy: 0.9637 - val_loss: 0.2385 - val_accuracy: 0.9742\n\nEpoch 135/1000                                                                       \n252/252 - 1s - loss: 0.2122 - accuracy: 0.9651 - val_loss: 0.2370 - val_accuracy: 0.9652\n\nEpoch 136/1000                                                                       \n252/252 - 1s - loss: 0.2238 - accuracy: 0.9644 - val_loss: 0.2511 - val_accuracy: 0.9697\n\nEpoch 137/1000                                                                       \n252/252 - 1s - loss: 0.2178 - accuracy: 0.9643 - val_loss: 0.3208 - val_accuracy: 0.9205\n\nEpoch 138/1000                                                                       \n252/252 - 1s - loss: 0.2145 - accuracy: 0.9649 - val_loss: 0.6907 - val_accuracy: 0.7530\n\nEpoch 139/1000                                                                       \n252/252 - 1s - loss: 0.2157 - accuracy: 0.9653 - val_loss: 0.2284 - val_accuracy: 0.9687\n\nEpoch 140/1000                                                                       \n252/252 - 1s - loss: 0.2151 - accuracy: 0.9655 - val_loss: 0.2155 - val_accuracy: 0.9692\n\nEpoch 141/1000                                                                       \n252/252 - 1s - loss: 0.2110 - accuracy: 0.9672 - val_loss: 0.2427 - val_accuracy: 0.9707\n\nEpoch 142/1000                                                                       \n252/252 - 1s - loss: 0.2162 - accuracy: 0.9632 - val_loss: 0.8907 - val_accuracy: 0.6660\n\nEpoch 143/1000                                                                       \n252/252 - 1s - loss: 0.2146 - accuracy: 0.9668 - val_loss: 0.4207 - val_accuracy: 0.8827\n\nEpoch 144/1000                                                                       \n252/252 - 1s - loss: 0.2191 - accuracy: 0.9630 - val_loss: 0.5337 - val_accuracy: 0.8335\n\nEpoch 145/1000                                                                       \n252/252 - 1s - loss: 0.2212 - accuracy: 0.9617 - val_loss: 0.2817 - val_accuracy: 0.9404\n\nEpoch 146/1000                                                                       \n252/252 - 1s - loss: 0.2091 - accuracy: 0.9701 - val_loss: 0.3801 - val_accuracy: 0.9056\n\nEpoch 147/1000                                                                       \n252/252 - 1s - loss: 0.2137 - accuracy: 0.9644 - val_loss: 0.3911 - val_accuracy: 0.8931\n\nEpoch 148/1000                                                                       \n252/252 - 1s - loss: 0.2083 - accuracy: 0.9665 - val_loss: 0.3524 - val_accuracy: 0.9046\n\nEpoch 149/1000                                                                       \n252/252 - 1s - loss: 0.2100 - accuracy: 0.9668 - val_loss: 0.2531 - val_accuracy: 0.9568\n\nEpoch 150/1000                                                                       \n","name":"stdout"},{"output_type":"stream","text":"252/252 - 1s - loss: 0.2179 - accuracy: 0.9627 - val_loss: 0.5681 - val_accuracy: 0.8121\n\nEpoch 151/1000                                                                       \n252/252 - 1s - loss: 0.2142 - accuracy: 0.9676 - val_loss: 0.2293 - val_accuracy: 0.9687\n\nEpoch 152/1000                                                                       \n252/252 - 1s - loss: 0.2183 - accuracy: 0.9628 - val_loss: 0.2350 - val_accuracy: 0.9697\n\nEpoch 153/1000                                                                       \n252/252 - 1s - loss: 0.2105 - accuracy: 0.9707 - val_loss: 0.6252 - val_accuracy: 0.7828\n\nEpoch 154/1000                                                                       \n252/252 - 1s - loss: 0.2164 - accuracy: 0.9634 - val_loss: 0.3126 - val_accuracy: 0.9289\n\nEpoch 155/1000                                                                       \n252/252 - 1s - loss: 0.2107 - accuracy: 0.9664 - val_loss: 0.2194 - val_accuracy: 0.9722\n\nEpoch 156/1000                                                                       \n252/252 - 1s - loss: 0.2152 - accuracy: 0.9658 - val_loss: 0.3932 - val_accuracy: 0.8976\n\nEpoch 157/1000                                                                       \n252/252 - 1s - loss: 0.2151 - accuracy: 0.9644 - val_loss: 0.3149 - val_accuracy: 0.9225\n\nEpoch 158/1000                                                                       \n252/252 - 1s - loss: 0.2192 - accuracy: 0.9630 - val_loss: 0.4076 - val_accuracy: 0.8797\n\nEpoch 159/1000                                                                       \n252/252 - 1s - loss: 0.2121 - accuracy: 0.9653 - val_loss: 0.2983 - val_accuracy: 0.9299\n\nEpoch 160/1000                                                                       \n252/252 - 1s - loss: 0.2158 - accuracy: 0.9623 - val_loss: 0.2273 - val_accuracy: 0.9712\n\nEpoch 161/1000                                                                       \n252/252 - 1s - loss: 0.2178 - accuracy: 0.9635 - val_loss: 0.2613 - val_accuracy: 0.9518\n\nEpoch 162/1000                                                                       \n252/252 - 1s - loss: 0.2132 - accuracy: 0.9666 - val_loss: 0.3848 - val_accuracy: 0.8976\n\nEpoch 163/1000                                                                       \n252/252 - 1s - loss: 0.2140 - accuracy: 0.9661 - val_loss: 0.4297 - val_accuracy: 0.8787\n\nEpoch 164/1000                                                                       \n252/252 - 1s - loss: 0.2087 - accuracy: 0.9690 - val_loss: 0.2497 - val_accuracy: 0.9543\n\nEpoch 165/1000                                                                       \n252/252 - 1s - loss: 0.2124 - accuracy: 0.9638 - val_loss: 0.3692 - val_accuracy: 0.9051\n\nEpoch 166/1000                                                                       \n252/252 - 1s - loss: 0.2191 - accuracy: 0.9623 - val_loss: 0.2073 - val_accuracy: 0.9722\n\nEpoch 167/1000                                                                       \n252/252 - 1s - loss: 0.2150 - accuracy: 0.9632 - val_loss: 0.3224 - val_accuracy: 0.9279\n\nEpoch 168/1000                                                                       \n252/252 - 1s - loss: 0.2144 - accuracy: 0.9641 - val_loss: 0.3531 - val_accuracy: 0.9120\n\nEpoch 169/1000                                                                       \n252/252 - 1s - loss: 0.2012 - accuracy: 0.9705 - val_loss: 0.3354 - val_accuracy: 0.9200\n\nEpoch 170/1000                                                                       \n252/252 - 1s - loss: 0.2142 - accuracy: 0.9634 - val_loss: 0.2717 - val_accuracy: 0.9587\n\nEpoch 171/1000                                                                       \n252/252 - 1s - loss: 0.2105 - accuracy: 0.9670 - val_loss: 0.2376 - val_accuracy: 0.9607\n\nEpoch 172/1000                                                                       \n252/252 - 1s - loss: 0.2087 - accuracy: 0.9671 - val_loss: 0.4716 - val_accuracy: 0.8444\n\nEpoch 173/1000                                                                       \n252/252 - 1s - loss: 0.2109 - accuracy: 0.9635 - val_loss: 0.3976 - val_accuracy: 0.8743\n\nEpoch 174/1000                                                                       \n252/252 - 1s - loss: 0.2081 - accuracy: 0.9651 - val_loss: 0.2911 - val_accuracy: 0.9428\n\nEpoch 175/1000                                                                       \n252/252 - 1s - loss: 0.2115 - accuracy: 0.9656 - val_loss: 0.4094 - val_accuracy: 0.8921\n\nEpoch 176/1000                                                                       \n252/252 - 1s - loss: 0.2078 - accuracy: 0.9675 - val_loss: 0.2699 - val_accuracy: 0.9553\n\nEpoch 177/1000                                                                       \n252/252 - 1s - loss: 0.2130 - accuracy: 0.9640 - val_loss: 0.1981 - val_accuracy: 0.9806\n\nEpoch 178/1000                                                                       \n252/252 - 1s - loss: 0.2030 - accuracy: 0.9685 - val_loss: 0.4336 - val_accuracy: 0.8693\n\nEpoch 179/1000                                                                       \n252/252 - 1s - loss: 0.2075 - accuracy: 0.9672 - val_loss: 1.0868 - val_accuracy: 0.7008\n\nEpoch 180/1000                                                                       \n252/252 - 1s - loss: 0.2111 - accuracy: 0.9651 - val_loss: 0.2124 - val_accuracy: 0.9737\n\nEpoch 181/1000                                                                       \n252/252 - 1s - loss: 0.2144 - accuracy: 0.9614 - val_loss: 0.2314 - val_accuracy: 0.9677\n\nEpoch 182/1000                                                                       \n252/252 - 1s - loss: 0.2091 - accuracy: 0.9655 - val_loss: 0.2268 - val_accuracy: 0.9602\n\nEpoch 183/1000                                                                       \n252/252 - 1s - loss: 0.2113 - accuracy: 0.9628 - val_loss: 0.2411 - val_accuracy: 0.9637\n\nEpoch 184/1000                                                                       \n252/252 - 1s - loss: 0.2124 - accuracy: 0.9622 - val_loss: 0.6070 - val_accuracy: 0.8002\n\nEpoch 185/1000                                                                       \n252/252 - 1s - loss: 0.2081 - accuracy: 0.9640 - val_loss: 0.2972 - val_accuracy: 0.9284\n\nEpoch 186/1000                                                                       \n252/252 - 1s - loss: 0.2018 - accuracy: 0.9675 - val_loss: 0.2042 - val_accuracy: 0.9756\n\nEpoch 187/1000                                                                       \n252/252 - 1s - loss: 0.2079 - accuracy: 0.9656 - val_loss: 0.2931 - val_accuracy: 0.9284\n\nEpoch 188/1000                                                                       \n252/252 - 1s - loss: 0.2158 - accuracy: 0.9635 - val_loss: 0.4244 - val_accuracy: 0.8708\n\nEpoch 189/1000                                                                       \n252/252 - 1s - loss: 0.2045 - accuracy: 0.9676 - val_loss: 0.8520 - val_accuracy: 0.7217\n\nEpoch 190/1000                                                                       \n252/252 - 1s - loss: 0.2042 - accuracy: 0.9686 - val_loss: 0.2162 - val_accuracy: 0.9761\n\nEpoch 191/1000                                                                       \n252/252 - 1s - loss: 0.2143 - accuracy: 0.9638 - val_loss: 0.1969 - val_accuracy: 0.9781\n\nEpoch 192/1000                                                                       \n252/252 - 1s - loss: 0.2155 - accuracy: 0.9618 - val_loss: 0.3077 - val_accuracy: 0.9364\n\nEpoch 193/1000                                                                       \n252/252 - 1s - loss: 0.2103 - accuracy: 0.9640 - val_loss: 0.4297 - val_accuracy: 0.8638\n\nEpoch 194/1000                                                                       \n252/252 - 1s - loss: 0.2158 - accuracy: 0.9641 - val_loss: 0.2071 - val_accuracy: 0.9742\n\nEpoch 195/1000                                                                       \n252/252 - 1s - loss: 0.2118 - accuracy: 0.9649 - val_loss: 0.2894 - val_accuracy: 0.9344\n\nEpoch 196/1000                                                                       \n252/252 - 1s - loss: 0.2125 - accuracy: 0.9634 - val_loss: 0.1988 - val_accuracy: 0.9727\n\n","name":"stdout"},{"output_type":"stream","text":"Epoch 197/1000                                                                       \n252/252 - 1s - loss: 0.2099 - accuracy: 0.9641 - val_loss: 0.3498 - val_accuracy: 0.9021\n\nEpoch 198/1000                                                                       \n252/252 - 1s - loss: 0.2056 - accuracy: 0.9675 - val_loss: 0.5345 - val_accuracy: 0.8255\n\nEpoch 199/1000                                                                       \n252/252 - 1s - loss: 0.2118 - accuracy: 0.9656 - val_loss: 0.4684 - val_accuracy: 0.8743\n\nEpoch 200/1000                                                                       \n252/252 - 1s - loss: 0.2090 - accuracy: 0.9659 - val_loss: 0.2898 - val_accuracy: 0.9453\n\nEpoch 201/1000                                                                       \n252/252 - 1s - loss: 0.2154 - accuracy: 0.9646 - val_loss: 0.3938 - val_accuracy: 0.8822\n\nEpoch 202/1000                                                                       \n252/252 - 1s - loss: 0.2008 - accuracy: 0.9689 - val_loss: 0.4636 - val_accuracy: 0.8554\n\nEpoch 203/1000                                                                       \n252/252 - 1s - loss: 0.2073 - accuracy: 0.9656 - val_loss: 0.2374 - val_accuracy: 0.9652\n\nEpoch 204/1000                                                                       \n252/252 - 1s - loss: 0.2138 - accuracy: 0.9670 - val_loss: 0.4811 - val_accuracy: 0.8673\n\nEpoch 205/1000                                                                       \n252/252 - 1s - loss: 0.2115 - accuracy: 0.9635 - val_loss: 0.3539 - val_accuracy: 0.9130\n\nEpoch 206/1000                                                                       \n252/252 - 1s - loss: 0.2077 - accuracy: 0.9666 - val_loss: 0.2012 - val_accuracy: 0.9806\n\nEpoch 207/1000                                                                       \n252/252 - 1s - loss: 0.2100 - accuracy: 0.9637 - val_loss: 0.2641 - val_accuracy: 0.9518\n\nEpoch 208/1000                                                                       \n252/252 - 1s - loss: 0.2017 - accuracy: 0.9699 - val_loss: 0.7585 - val_accuracy: 0.7490\n\nEpoch 209/1000                                                                       \n252/252 - 1s - loss: 0.2097 - accuracy: 0.9650 - val_loss: 0.3820 - val_accuracy: 0.8976\n\nEpoch 210/1000                                                                       \n252/252 - 1s - loss: 0.2145 - accuracy: 0.9629 - val_loss: 0.3434 - val_accuracy: 0.9081\n\nEpoch 211/1000                                                                       \n252/252 - 1s - loss: 0.2099 - accuracy: 0.9649 - val_loss: 0.3474 - val_accuracy: 0.9021\n\nEpoch 212/1000                                                                       \n252/252 - 1s - loss: 0.2063 - accuracy: 0.9643 - val_loss: 0.3001 - val_accuracy: 0.9309\n\nEpoch 213/1000                                                                       \n252/252 - 1s - loss: 0.2107 - accuracy: 0.9645 - val_loss: 0.2040 - val_accuracy: 0.9801\n\nEpoch 214/1000                                                                       \n252/252 - 1s - loss: 0.2137 - accuracy: 0.9629 - val_loss: 0.2349 - val_accuracy: 0.9677\n\nEpoch 215/1000                                                                       \n252/252 - 1s - loss: 0.2097 - accuracy: 0.9638 - val_loss: 0.2444 - val_accuracy: 0.9578\n\nEpoch 216/1000                                                                       \n252/252 - 1s - loss: 0.2017 - accuracy: 0.9669 - val_loss: 0.2093 - val_accuracy: 0.9751\n\nEpoch 217/1000                                                                       \n252/252 - 1s - loss: 0.2008 - accuracy: 0.9681 - val_loss: 0.2382 - val_accuracy: 0.9578\n\nEpoch 218/1000                                                                       \n252/252 - 1s - loss: 0.2087 - accuracy: 0.9643 - val_loss: 0.2392 - val_accuracy: 0.9573\n\nEpoch 219/1000                                                                       \n252/252 - 1s - loss: 0.2050 - accuracy: 0.9640 - val_loss: 0.4327 - val_accuracy: 0.8961\n\nEpoch 220/1000                                                                       \n252/252 - 1s - loss: 0.2116 - accuracy: 0.9653 - val_loss: 0.5580 - val_accuracy: 0.8161\n\nEpoch 221/1000                                                                       \n252/252 - 1s - loss: 0.2143 - accuracy: 0.9619 - val_loss: 0.2228 - val_accuracy: 0.9697\n\nEpoch 222/1000                                                                       \n252/252 - 1s - loss: 0.2128 - accuracy: 0.9654 - val_loss: 0.2114 - val_accuracy: 0.9727\n\nEpoch 223/1000                                                                       \n252/252 - 1s - loss: 0.2158 - accuracy: 0.9644 - val_loss: 0.3074 - val_accuracy: 0.9329\n\nEpoch 224/1000                                                                       \n252/252 - 1s - loss: 0.2060 - accuracy: 0.9669 - val_loss: 0.4144 - val_accuracy: 0.8733\n\nEpoch 225/1000                                                                       \n252/252 - 1s - loss: 0.2071 - accuracy: 0.9669 - val_loss: 0.2679 - val_accuracy: 0.9438\n\nEpoch 226/1000                                                                       \n252/252 - 1s - loss: 0.2089 - accuracy: 0.9656 - val_loss: 0.2547 - val_accuracy: 0.9518\n\nEpoch 227/1000                                                                       \n252/252 - 1s - loss: 0.2064 - accuracy: 0.9627 - val_loss: 0.2596 - val_accuracy: 0.9433\n\nEpoch 228/1000                                                                       \n252/252 - 1s - loss: 0.2060 - accuracy: 0.9664 - val_loss: 0.2419 - val_accuracy: 0.9622\n\nEpoch 229/1000                                                                       \n252/252 - 1s - loss: 0.2118 - accuracy: 0.9628 - val_loss: 0.3404 - val_accuracy: 0.9130\n\nEpoch 230/1000                                                                       \n252/252 - 1s - loss: 0.2168 - accuracy: 0.9635 - val_loss: 0.2212 - val_accuracy: 0.9617\n\nEpoch 231/1000                                                                       \n252/252 - 1s - loss: 0.1928 - accuracy: 0.9684 - val_loss: 0.2254 - val_accuracy: 0.9687\n\nEpoch 232/1000                                                                       \n252/252 - 1s - loss: 0.2047 - accuracy: 0.9670 - val_loss: 0.2861 - val_accuracy: 0.9433\n\nEpoch 233/1000                                                                       \n  1%|          | 2/300 [33:32<83:18:37, 1006.44s/trial, best loss: 0.3004775643348694]\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-ca2799564092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         )\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         )\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-40c69e187a4b>\u001b[0m in \u001b[0;36mf_nn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                         callbacks=get_callbacks(params))\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Done!!!!!!!!!!!!!!!!!!!!!!!!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}