{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 8221475156914908975, name: \"/device:XLA_CPU:0\"\n device_type: \"XLA_CPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 6779919852133347022\n physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n device_type: \"XLA_GPU\"\n memory_limit: 17179869184\n locality {\n }\n incarnation: 10910358016444415445\n physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 15876469556\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 7763539752194769798\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, partial, rand, space_eval\n# from sklearn.metrics import log_loss\nimport sys\n# import time\nimport math\nimport os\nimport pandas as pd\n# from keras.callbacks import EarlyStopping\nimport numpy as np\n# from glob import glob\n# import cv2\n# import skimage\n# from skimage.transform import resize\nfrom keras.utils.np_utils import to_categorical\n# import keras\n# from keras import layers\n# from keras import models\n# from keras import optimizers\n# from keras.models import load_model\n# import keras.callbacks as kcall\nfrom keras.optimizers import Adam, RMSprop,SGD\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Input,concatenate\n# from keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\n# from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n# from keras.applications.vgg19 import VGG19\nfrom keras.regularizers import l2, l1\n\nimport matplotlib.pyplot as plt\n# from keras.layers import Input, concatenate\n# from keras import optimizers, metrics, models\n# from keras.layers import Input, Flatten, Dense\n\n%matplotlib inline","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\n\nprint(\"Keras Version\", keras.__version__)\nprint(\"tensorflow Version\", tf.__version__)\n# print(\"dim_ordering:\", K.image_dim_ordering())","execution_count":4,"outputs":[{"output_type":"stream","text":"Keras Version 2.3.1\ntensorflow Version 2.0.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nimg_height, img_width = 256, 256\ninput_shape = (img_height, img_width, 3)\nepochs = 400","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/\"))","execution_count":6,"outputs":[{"output_type":"stream","text":"['payam-thesis', 'full-keras-pretrained-no-top']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(os.listdir(\"../input/full-keras-pretrained-no-top\"))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/payam-thesis/train/'\ntest_dir = '../input/payam-thesis/test'","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(x):\n    # 'RGB'->'BGR'\n    x = x[:, :, ::-1]\n    # Zero-center by imagenet mean pixel\n    x[:, :, 0] -= 103.939\n    x[:, :, 1] -= 116.779\n    x[:, :, 2] -= 123.68\n    return x","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = np.random.seed(1142)\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n#     featurewise_center=True,\n#     featurewise_std_normalization=True,\n    preprocessing_function = preprocess_input,\n    rotation_range=90,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.3,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='reflect',\n    validation_split= 0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'training',\n    class_mode='categorical')\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    subset = 'validation',\n    class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1. / 255, preprocessing_function = preprocess_input)\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    seed = random_seed,\n    shuffle = False,\n    class_mode='categorical')","execution_count":10,"outputs":[{"output_type":"stream","text":"Found 4611 images belonging to 3 classes.\nFound 1152 images belonging to 3 classes.\nFound 765 images belonging to 3 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(train_generator.filenames)\nnb_validation_samples = len(validation_generator.filenames)\nnb_test_samples = len(test_generator.filenames)\n\npredict_size_train = int(math.ceil(nb_train_samples / batch_size))\npredict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\npredict_size_test = int(math.ceil(nb_test_samples / batch_size))\n\nnum_classes = len(train_generator.class_indices)\n\nprint(\"nb_train_samples:\", nb_train_samples)\nprint(\"nb_validation_samples:\", nb_validation_samples)\nprint(\"nb_test_samples:\", nb_test_samples)\n\nprint(\"\\npredict_size_train:\", predict_size_train)\nprint(\"predict_size_validation:\", predict_size_validation)\nprint(\"predict_size_test:\", predict_size_test)\n\nprint(\"\\n num_classes:\", num_classes)","execution_count":11,"outputs":[{"output_type":"stream","text":"nb_train_samples: 4611\nnb_validation_samples: 1152\nnb_test_samples: 765\n\npredict_size_train: 73\npredict_size_validation: 18\npredict_size_test: 12\n\n num_classes: 3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg19_weights =\"../input/full-keras-pretrained-no-top/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_weights =\"../input/full-keras-pretrained-no-top//inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nvgg16_weights =\"../input/full-keras-pretrained-no-top/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet201_weights =\"../input/full-keras-pretrained-no-top/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ndenseNet121_weights =\"../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nresenet50_weights =\"../input/full-keras-pretrained-no-top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\ninception_resnet_v2_weights =\"../input/full-keras-pretrained-no-top/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\nnasnet_weights =\"../input/full-keras-pretrained-no-top/nasnet_large_no_top.h5\"\nnasnet_mobile_weights =\"../input/full-keras-pretrained-no-top/nasnet_mobile_no_top.h5\"\nmobilenet_weights =\"../input/full-keras-pretrained-no-top/mobilenet_1_0_224_tf_no_top.h5\"","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg19 import VGG19\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.applications import DenseNet201\nfrom keras.applications import DenseNet121\nfrom keras.applications import ResNet50\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications import NASNetLarge, NASNetMobile\nfrom keras.applications import MobileNet","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_tensor = Input(shape = input_shape)  \n\nbase_model1=InceptionV3(input_shape= input_shape,weights=inception_weights, include_top=False, input_tensor=input_tensor)\nbase_model2=DenseNet201(input_shape= input_shape,weights=denseNet201_weights, include_top=False, input_tensor=input_tensor)\n\nx1 = base_model1.output\nx1 = GlobalAveragePooling2D()(x1)\n\nx2 = base_model2.output\nx2 = GlobalAveragePooling2D()(x2)\n\nmerge = concatenate([x1, x2])\npredictions = Dense(num_classes, activation='softmax')(merge)\n\nmodel = Model(inputs=input_tensor,outputs=predictions)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, layer in enumerate(model.layers):\n    print(i, layer.name)","execution_count":15,"outputs":[{"output_type":"stream","text":"0 input_1\n1 zero_padding2d_1\n2 conv1/conv\n3 conv1/bn\n4 conv1/relu\n5 zero_padding2d_2\n6 pool1\n7 conv2_block1_0_bn\n8 conv2_block1_0_relu\n9 conv2_block1_1_conv\n10 conv2_block1_1_bn\n11 conv2_block1_1_relu\n12 conv2_block1_2_conv\n13 conv2_block1_concat\n14 conv2_block2_0_bn\n15 conv2_block2_0_relu\n16 conv2_block2_1_conv\n17 conv2_block2_1_bn\n18 conv2_block2_1_relu\n19 conv2_block2_2_conv\n20 conv2_block2_concat\n21 conv2_block3_0_bn\n22 conv2_block3_0_relu\n23 conv2_block3_1_conv\n24 conv2_block3_1_bn\n25 conv2_block3_1_relu\n26 conv2_block3_2_conv\n27 conv2_block3_concat\n28 conv2_block4_0_bn\n29 conv2_block4_0_relu\n30 conv2_block4_1_conv\n31 conv2_block4_1_bn\n32 conv2_block4_1_relu\n33 conv2_block4_2_conv\n34 conv2_block4_concat\n35 conv2_block5_0_bn\n36 conv2_block5_0_relu\n37 conv2_block5_1_conv\n38 conv2_block5_1_bn\n39 conv2_block5_1_relu\n40 conv2_block5_2_conv\n41 conv2_block5_concat\n42 conv2_block6_0_bn\n43 conv2_block6_0_relu\n44 conv2_block6_1_conv\n45 conv2_block6_1_bn\n46 conv2_block6_1_relu\n47 conv2_block6_2_conv\n48 conv2_block6_concat\n49 pool2_bn\n50 pool2_relu\n51 pool2_conv\n52 pool2_pool\n53 conv3_block1_0_bn\n54 conv3_block1_0_relu\n55 conv3_block1_1_conv\n56 conv3_block1_1_bn\n57 conv3_block1_1_relu\n58 conv3_block1_2_conv\n59 conv3_block1_concat\n60 conv3_block2_0_bn\n61 conv3_block2_0_relu\n62 conv3_block2_1_conv\n63 conv3_block2_1_bn\n64 conv3_block2_1_relu\n65 conv3_block2_2_conv\n66 conv3_block2_concat\n67 conv3_block3_0_bn\n68 conv3_block3_0_relu\n69 conv3_block3_1_conv\n70 conv3_block3_1_bn\n71 conv3_block3_1_relu\n72 conv3_block3_2_conv\n73 conv3_block3_concat\n74 conv3_block4_0_bn\n75 conv3_block4_0_relu\n76 conv3_block4_1_conv\n77 conv3_block4_1_bn\n78 conv3_block4_1_relu\n79 conv3_block4_2_conv\n80 conv3_block4_concat\n81 conv3_block5_0_bn\n82 conv3_block5_0_relu\n83 conv3_block5_1_conv\n84 conv3_block5_1_bn\n85 conv3_block5_1_relu\n86 conv3_block5_2_conv\n87 conv3_block5_concat\n88 conv3_block6_0_bn\n89 conv3_block6_0_relu\n90 conv3_block6_1_conv\n91 conv3_block6_1_bn\n92 conv3_block6_1_relu\n93 conv3_block6_2_conv\n94 conv3_block6_concat\n95 conv3_block7_0_bn\n96 conv3_block7_0_relu\n97 conv3_block7_1_conv\n98 conv3_block7_1_bn\n99 conv3_block7_1_relu\n100 conv3_block7_2_conv\n101 conv3_block7_concat\n102 conv3_block8_0_bn\n103 conv3_block8_0_relu\n104 conv3_block8_1_conv\n105 conv3_block8_1_bn\n106 conv3_block8_1_relu\n107 conv3_block8_2_conv\n108 conv3_block8_concat\n109 conv3_block9_0_bn\n110 conv3_block9_0_relu\n111 conv3_block9_1_conv\n112 conv3_block9_1_bn\n113 conv3_block9_1_relu\n114 conv3_block9_2_conv\n115 conv3_block9_concat\n116 conv3_block10_0_bn\n117 conv3_block10_0_relu\n118 conv3_block10_1_conv\n119 conv3_block10_1_bn\n120 conv3_block10_1_relu\n121 conv3_block10_2_conv\n122 conv3_block10_concat\n123 conv3_block11_0_bn\n124 conv3_block11_0_relu\n125 conv3_block11_1_conv\n126 conv3_block11_1_bn\n127 conv3_block11_1_relu\n128 conv3_block11_2_conv\n129 conv3_block11_concat\n130 conv3_block12_0_bn\n131 conv3_block12_0_relu\n132 conv3_block12_1_conv\n133 conv3_block12_1_bn\n134 conv3_block12_1_relu\n135 conv3_block12_2_conv\n136 conv3_block12_concat\n137 pool3_bn\n138 pool3_relu\n139 pool3_conv\n140 pool3_pool\n141 conv4_block1_0_bn\n142 conv4_block1_0_relu\n143 conv4_block1_1_conv\n144 conv4_block1_1_bn\n145 conv4_block1_1_relu\n146 conv4_block1_2_conv\n147 conv4_block1_concat\n148 conv4_block2_0_bn\n149 conv4_block2_0_relu\n150 conv4_block2_1_conv\n151 conv4_block2_1_bn\n152 conv4_block2_1_relu\n153 conv4_block2_2_conv\n154 conv4_block2_concat\n155 conv4_block3_0_bn\n156 conv4_block3_0_relu\n157 conv4_block3_1_conv\n158 conv4_block3_1_bn\n159 conv4_block3_1_relu\n160 conv4_block3_2_conv\n161 conv4_block3_concat\n162 conv4_block4_0_bn\n163 conv4_block4_0_relu\n164 conv4_block4_1_conv\n165 conv4_block4_1_bn\n166 conv4_block4_1_relu\n167 conv4_block4_2_conv\n168 conv4_block4_concat\n169 conv4_block5_0_bn\n170 conv4_block5_0_relu\n171 conv4_block5_1_conv\n172 conv4_block5_1_bn\n173 conv4_block5_1_relu\n174 conv4_block5_2_conv\n175 conv4_block5_concat\n176 conv4_block6_0_bn\n177 conv4_block6_0_relu\n178 conv4_block6_1_conv\n179 conv4_block6_1_bn\n180 conv4_block6_1_relu\n181 conv4_block6_2_conv\n182 conv4_block6_concat\n183 conv4_block7_0_bn\n184 conv4_block7_0_relu\n185 conv4_block7_1_conv\n186 conv4_block7_1_bn\n187 conv4_block7_1_relu\n188 conv4_block7_2_conv\n189 conv4_block7_concat\n190 conv4_block8_0_bn\n191 conv4_block8_0_relu\n192 conv4_block8_1_conv\n193 conv4_block8_1_bn\n194 conv4_block8_1_relu\n195 conv4_block8_2_conv\n196 conv4_block8_concat\n197 conv4_block9_0_bn\n198 conv4_block9_0_relu\n199 conv4_block9_1_conv\n200 conv4_block9_1_bn\n201 conv4_block9_1_relu\n202 conv4_block9_2_conv\n203 conv4_block9_concat\n204 conv4_block10_0_bn\n205 conv4_block10_0_relu\n206 conv4_block10_1_conv\n207 conv4_block10_1_bn\n208 conv4_block10_1_relu\n209 conv4_block10_2_conv\n210 conv4_block10_concat\n211 conv4_block11_0_bn\n212 conv4_block11_0_relu\n213 conv4_block11_1_conv\n214 conv4_block11_1_bn\n215 conv4_block11_1_relu\n216 conv4_block11_2_conv\n217 conv4_block11_concat\n218 conv4_block12_0_bn\n219 conv4_block12_0_relu\n220 conv4_block12_1_conv\n221 conv4_block12_1_bn\n222 conv4_block12_1_relu\n223 conv4_block12_2_conv\n224 conv4_block12_concat\n225 conv4_block13_0_bn\n226 conv4_block13_0_relu\n227 conv4_block13_1_conv\n228 conv4_block13_1_bn\n229 conv4_block13_1_relu\n230 conv4_block13_2_conv\n231 conv4_block13_concat\n232 conv4_block14_0_bn\n233 conv4_block14_0_relu\n234 conv4_block14_1_conv\n235 conv4_block14_1_bn\n236 conv4_block14_1_relu\n237 conv4_block14_2_conv\n238 conv4_block14_concat\n239 conv4_block15_0_bn\n240 conv4_block15_0_relu\n241 conv4_block15_1_conv\n242 conv4_block15_1_bn\n243 conv4_block15_1_relu\n244 conv4_block15_2_conv\n245 conv4_block15_concat\n246 conv4_block16_0_bn\n247 conv4_block16_0_relu\n248 conv4_block16_1_conv\n249 conv4_block16_1_bn\n250 conv4_block16_1_relu\n251 conv4_block16_2_conv\n252 conv4_block16_concat\n253 conv4_block17_0_bn\n254 conv4_block17_0_relu\n255 conv4_block17_1_conv\n256 conv4_block17_1_bn\n257 conv4_block17_1_relu\n258 conv4_block17_2_conv\n259 conv4_block17_concat\n260 conv4_block18_0_bn\n261 conv4_block18_0_relu\n262 conv4_block18_1_conv\n263 conv4_block18_1_bn\n264 conv4_block18_1_relu\n265 conv4_block18_2_conv\n266 conv4_block18_concat\n267 conv4_block19_0_bn\n268 conv4_block19_0_relu\n269 conv4_block19_1_conv\n270 conv4_block19_1_bn\n271 conv4_block19_1_relu\n272 conv4_block19_2_conv\n273 conv4_block19_concat\n274 conv4_block20_0_bn\n275 conv4_block20_0_relu\n276 conv4_block20_1_conv\n277 conv4_block20_1_bn\n278 conv4_block20_1_relu\n279 conv4_block20_2_conv\n280 conv4_block20_concat\n281 conv4_block21_0_bn\n282 conv4_block21_0_relu\n283 conv4_block21_1_conv\n284 conv4_block21_1_bn\n285 conv4_block21_1_relu\n286 conv4_block21_2_conv\n287 conv4_block21_concat\n288 conv4_block22_0_bn\n289 conv4_block22_0_relu\n290 conv4_block22_1_conv\n291 conv4_block22_1_bn\n292 conv4_block22_1_relu\n293 conv4_block22_2_conv\n294 conv4_block22_concat\n295 conv4_block23_0_bn\n296 conv4_block23_0_relu\n297 conv4_block23_1_conv\n298 conv4_block23_1_bn\n299 conv4_block23_1_relu\n300 conv4_block23_2_conv\n301 conv4_block23_concat\n302 conv4_block24_0_bn\n303 conv4_block24_0_relu\n304 conv4_block24_1_conv\n305 conv4_block24_1_bn\n306 conv4_block24_1_relu\n307 conv4_block24_2_conv\n308 conv4_block24_concat\n309 conv4_block25_0_bn\n310 conv4_block25_0_relu\n311 conv4_block25_1_conv\n312 conv4_block25_1_bn\n313 conv4_block25_1_relu\n314 conv4_block25_2_conv\n315 conv4_block25_concat\n316 conv4_block26_0_bn\n317 conv4_block26_0_relu\n318 conv4_block26_1_conv\n319 conv4_block26_1_bn\n320 conv4_block26_1_relu\n321 conv4_block26_2_conv\n322 conv4_block26_concat\n323 conv4_block27_0_bn\n324 conv4_block27_0_relu\n325 conv4_block27_1_conv\n326 conv4_block27_1_bn\n327 conv4_block27_1_relu\n328 conv4_block27_2_conv\n329 conv4_block27_concat\n330 conv4_block28_0_bn\n331 conv4_block28_0_relu\n332 conv4_block28_1_conv\n333 conv4_block28_1_bn\n334 conv4_block28_1_relu\n335 conv4_block28_2_conv\n336 conv4_block28_concat\n337 conv4_block29_0_bn\n338 conv4_block29_0_relu\n339 conv4_block29_1_conv\n340 conv4_block29_1_bn\n341 conv4_block29_1_relu\n342 conv4_block29_2_conv\n343 conv4_block29_concat\n344 conv4_block30_0_bn\n345 conv4_block30_0_relu\n346 conv4_block30_1_conv\n347 conv4_block30_1_bn\n348 conv4_block30_1_relu\n349 conv4_block30_2_conv\n350 conv4_block30_concat\n351 conv4_block31_0_bn\n352 conv4_block31_0_relu\n353 conv4_block31_1_conv\n354 conv4_block31_1_bn\n355 conv4_block31_1_relu\n356 conv4_block31_2_conv\n357 conv4_block31_concat\n358 conv4_block32_0_bn\n359 conv4_block32_0_relu\n360 conv4_block32_1_conv\n361 conv4_block32_1_bn\n362 conv4_block32_1_relu\n363 conv4_block32_2_conv\n364 conv4_block32_concat\n365 conv4_block33_0_bn\n366 conv4_block33_0_relu\n367 conv4_block33_1_conv\n368 conv4_block33_1_bn\n369 conv4_block33_1_relu\n370 conv4_block33_2_conv\n371 conv4_block33_concat\n372 conv4_block34_0_bn\n373 conv4_block34_0_relu\n374 conv4_block34_1_conv\n375 conv4_block34_1_bn\n376 conv4_block34_1_relu\n377 conv4_block34_2_conv\n378 conv4_block34_concat\n379 conv4_block35_0_bn\n380 conv4_block35_0_relu\n381 conv4_block35_1_conv\n382 conv4_block35_1_bn\n383 conv4_block35_1_relu\n384 conv4_block35_2_conv\n385 conv4_block35_concat\n386 conv4_block36_0_bn\n387 conv4_block36_0_relu\n388 conv4_block36_1_conv\n389 conv4_block36_1_bn\n390 conv4_block36_1_relu\n391 conv4_block36_2_conv\n392 conv4_block36_concat\n393 conv4_block37_0_bn\n394 conv4_block37_0_relu\n395 conv4_block37_1_conv\n396 conv4_block37_1_bn\n397 conv4_block37_1_relu\n398 conv4_block37_2_conv\n399 conv4_block37_concat\n400 conv4_block38_0_bn\n401 conv4_block38_0_relu\n402 conv4_block38_1_conv\n403 conv4_block38_1_bn\n404 conv4_block38_1_relu\n405 conv4_block38_2_conv\n406 conv4_block38_concat\n407 conv4_block39_0_bn\n408 conv4_block39_0_relu\n409 conv4_block39_1_conv\n410 conv4_block39_1_bn\n411 conv4_block39_1_relu\n412 conv4_block39_2_conv\n413 conv4_block39_concat\n414 conv4_block40_0_bn\n415 conv4_block40_0_relu\n416 conv4_block40_1_conv\n417 conv4_block40_1_bn\n418 conv4_block40_1_relu\n419 conv4_block40_2_conv\n420 conv4_block40_concat\n421 conv4_block41_0_bn\n422 conv4_block41_0_relu\n423 conv4_block41_1_conv\n424 conv4_block41_1_bn\n425 conv4_block41_1_relu\n426 conv4_block41_2_conv\n427 conv4_block41_concat\n428 conv4_block42_0_bn\n429 conv4_block42_0_relu\n430 conv4_block42_1_conv\n431 conv4_block42_1_bn\n432 conv4_block42_1_relu\n433 conv4_block42_2_conv\n434 conv4_block42_concat\n435 conv4_block43_0_bn\n436 conv4_block43_0_relu\n437 conv4_block43_1_conv\n438 conv4_block43_1_bn\n439 conv4_block43_1_relu\n440 conv4_block43_2_conv\n441 conv4_block43_concat\n442 conv4_block44_0_bn\n443 conv4_block44_0_relu\n444 conv4_block44_1_conv\n445 conv4_block44_1_bn\n446 conv4_block44_1_relu\n447 conv4_block44_2_conv\n448 conv4_block44_concat\n449 conv4_block45_0_bn\n450 conv4_block45_0_relu\n451 conv4_block45_1_conv\n452 conv4_block45_1_bn\n453 conv4_block45_1_relu\n454 conv4_block45_2_conv\n455 conv4_block45_concat\n456 conv4_block46_0_bn\n457 conv4_block46_0_relu\n458 conv4_block46_1_conv\n459 conv4_block46_1_bn\n460 conv4_block46_1_relu\n461 conv4_block46_2_conv\n462 conv4_block46_concat\n463 conv4_block47_0_bn\n464 conv4_block47_0_relu\n465 conv4_block47_1_conv\n466 conv4_block47_1_bn\n467 conv4_block47_1_relu\n468 conv4_block47_2_conv\n469 conv4_block47_concat\n470 conv4_block48_0_bn\n471 conv4_block48_0_relu\n472 conv4_block48_1_conv\n473 conv4_block48_1_bn\n474 conv4_block48_1_relu\n475 conv4_block48_2_conv\n476 conv4_block48_concat\n477 pool4_bn\n478 pool4_relu\n479 pool4_conv\n480 pool4_pool\n481 conv5_block1_0_bn\n482 conv5_block1_0_relu\n483 conv5_block1_1_conv\n484 conv5_block1_1_bn\n485 conv5_block1_1_relu\n486 conv5_block1_2_conv\n487 conv5_block1_concat\n488 conv5_block2_0_bn\n489 conv5_block2_0_relu\n490 conv5_block2_1_conv\n491 conv5_block2_1_bn\n492 conv5_block2_1_relu\n493 conv5_block2_2_conv\n494 conv5_block2_concat\n495 conv5_block3_0_bn\n496 conv5_block3_0_relu\n497 conv5_block3_1_conv\n498 conv5_block3_1_bn\n499 conv5_block3_1_relu\n500 conv5_block3_2_conv\n501 conv5_block3_concat\n502 conv5_block4_0_bn\n503 conv5_block4_0_relu\n504 conv5_block4_1_conv\n505 conv5_block4_1_bn\n506 conv5_block4_1_relu\n507 conv5_block4_2_conv\n508 conv5_block4_concat\n509 conv5_block5_0_bn\n510 conv5_block5_0_relu\n511 conv5_block5_1_conv\n512 conv5_block5_1_bn\n513 conv5_block5_1_relu\n514 conv5_block5_2_conv\n515 conv5_block5_concat\n516 conv5_block6_0_bn\n517 conv5_block6_0_relu\n518 conv5_block6_1_conv\n519 conv5_block6_1_bn\n520 conv5_block6_1_relu\n521 conv5_block6_2_conv\n522 conv5_block6_concat\n523 conv5_block7_0_bn\n524 conv5_block7_0_relu\n525 conv5_block7_1_conv\n526 conv5_block7_1_bn\n527 conv5_block7_1_relu\n528 conv5_block7_2_conv\n529 conv5_block7_concat\n530 conv5_block8_0_bn\n531 conv5_block8_0_relu\n532 conv5_block8_1_conv\n533 conv5_block8_1_bn\n534 conv5_block8_1_relu\n535 conv5_block8_2_conv\n536 conv5_block8_concat\n537 conv5_block9_0_bn\n538 conv5_block9_0_relu\n539 conv5_block9_1_conv\n540 conv5_block9_1_bn\n541 conv5_block9_1_relu\n542 conv5_block9_2_conv\n543 conv5_block9_concat\n544 conv5_block10_0_bn\n545 conv5_block10_0_relu\n546 conv5_block10_1_conv\n547 conv5_block10_1_bn\n548 conv5_block10_1_relu\n549 conv5_block10_2_conv\n550 conv5_block10_concat\n551 conv2d_1\n552 conv5_block11_0_bn\n553 batch_normalization_1\n554 conv5_block11_0_relu\n555 activation_1\n556 conv5_block11_1_conv\n557 conv2d_2\n558 conv5_block11_1_bn\n559 batch_normalization_2\n560 conv5_block11_1_relu\n561 activation_2\n562 conv5_block11_2_conv\n563 conv2d_3\n564 conv5_block11_concat\n565 batch_normalization_3\n566 conv5_block12_0_bn\n567 activation_3\n568 conv5_block12_0_relu\n569 max_pooling2d_1\n570 conv5_block12_1_conv\n571 conv2d_4\n572 conv5_block12_1_bn\n573 batch_normalization_4\n574 conv5_block12_1_relu\n575 activation_4\n576 conv5_block12_2_conv\n577 conv2d_5\n578 conv5_block12_concat\n579 batch_normalization_5\n580 conv5_block13_0_bn\n581 activation_5\n582 conv5_block13_0_relu\n583 max_pooling2d_2\n584 conv5_block13_1_conv\n585 conv2d_9\n586 conv5_block13_1_bn\n587 batch_normalization_9\n588 conv5_block13_1_relu\n589 activation_9\n590 conv5_block13_2_conv\n591 conv2d_7\n592 conv2d_10\n593 conv5_block13_concat\n594 batch_normalization_7\n595 batch_normalization_10\n596 conv5_block14_0_bn\n597 activation_7\n598 activation_10\n599 average_pooling2d_1\n600 conv5_block14_0_relu\n601 conv2d_6\n602 conv2d_8\n603 conv2d_11\n604 conv2d_12\n605 conv5_block14_1_conv\n606 batch_normalization_6\n607 batch_normalization_8\n608 batch_normalization_11\n609 batch_normalization_12\n610 conv5_block14_1_bn\n611 activation_6\n612 activation_8\n613 activation_11\n614 activation_12\n615 conv5_block14_1_relu\n616 mixed0\n617 conv5_block14_2_conv\n618 conv2d_16\n619 conv5_block14_concat\n620 batch_normalization_16\n621 conv5_block15_0_bn\n622 activation_16\n623 conv5_block15_0_relu\n624 conv2d_14\n625 conv2d_17\n626 conv5_block15_1_conv\n627 batch_normalization_14\n628 batch_normalization_17\n629 conv5_block15_1_bn\n630 activation_14\n631 activation_17\n632 average_pooling2d_2\n633 conv5_block15_1_relu\n634 conv2d_13\n635 conv2d_15\n636 conv2d_18\n637 conv2d_19\n638 conv5_block15_2_conv\n639 batch_normalization_13\n640 batch_normalization_15\n641 batch_normalization_18\n642 batch_normalization_19\n643 conv5_block15_concat\n644 activation_13\n645 activation_15\n646 activation_18\n647 activation_19\n648 conv5_block16_0_bn\n649 mixed1\n650 conv5_block16_0_relu\n651 conv2d_23\n652 conv5_block16_1_conv\n653 batch_normalization_23\n654 conv5_block16_1_bn\n655 activation_23\n656 conv5_block16_1_relu\n657 conv2d_21\n658 conv2d_24\n659 conv5_block16_2_conv\n660 batch_normalization_21\n661 batch_normalization_24\n662 conv5_block16_concat\n663 activation_21\n664 activation_24\n665 average_pooling2d_3\n666 conv5_block17_0_bn\n667 conv2d_20\n668 conv2d_22\n669 conv2d_25\n670 conv2d_26\n671 conv5_block17_0_relu\n672 batch_normalization_20\n673 batch_normalization_22\n674 batch_normalization_25\n675 batch_normalization_26\n676 conv5_block17_1_conv\n677 activation_20\n678 activation_22\n679 activation_25\n680 activation_26\n681 conv5_block17_1_bn\n682 mixed2\n683 conv5_block17_1_relu\n684 conv2d_28\n685 conv5_block17_2_conv\n686 batch_normalization_28\n687 conv5_block17_concat\n688 activation_28\n689 conv5_block18_0_bn\n690 conv2d_29\n691 conv5_block18_0_relu\n692 batch_normalization_29\n693 conv5_block18_1_conv\n694 activation_29\n695 conv5_block18_1_bn\n696 conv2d_27\n697 conv2d_30\n698 conv5_block18_1_relu\n699 batch_normalization_27\n700 batch_normalization_30\n701 conv5_block18_2_conv\n702 activation_27\n703 activation_30\n704 max_pooling2d_3\n705 conv5_block18_concat\n706 mixed3\n707 conv5_block19_0_bn\n708 conv2d_35\n709 conv5_block19_0_relu\n710 batch_normalization_35\n711 conv5_block19_1_conv\n712 activation_35\n713 conv5_block19_1_bn\n714 conv2d_36\n715 conv5_block19_1_relu\n716 batch_normalization_36\n717 conv5_block19_2_conv\n718 activation_36\n719 conv5_block19_concat\n720 conv2d_32\n721 conv2d_37\n722 conv5_block20_0_bn\n723 batch_normalization_32\n724 batch_normalization_37\n725 conv5_block20_0_relu\n726 activation_32\n727 activation_37\n728 conv5_block20_1_conv\n729 conv2d_33\n730 conv2d_38\n731 conv5_block20_1_bn\n732 batch_normalization_33\n733 batch_normalization_38\n734 conv5_block20_1_relu\n735 activation_33\n736 activation_38\n737 average_pooling2d_4\n738 conv5_block20_2_conv\n739 conv2d_31\n740 conv2d_34\n741 conv2d_39\n742 conv2d_40\n743 conv5_block20_concat\n744 batch_normalization_31\n745 batch_normalization_34\n746 batch_normalization_39\n747 batch_normalization_40\n748 conv5_block21_0_bn\n749 activation_31\n750 activation_34\n751 activation_39\n752 activation_40\n753 conv5_block21_0_relu\n754 mixed4\n755 conv5_block21_1_conv\n756 conv2d_45\n757 conv5_block21_1_bn\n758 batch_normalization_45\n759 conv5_block21_1_relu\n760 activation_45\n761 conv5_block21_2_conv\n762 conv2d_46\n763 conv5_block21_concat\n764 batch_normalization_46\n765 conv5_block22_0_bn\n766 activation_46\n767 conv5_block22_0_relu\n768 conv2d_42\n769 conv2d_47\n770 conv5_block22_1_conv\n771 batch_normalization_42\n772 batch_normalization_47\n773 conv5_block22_1_bn\n774 activation_42\n775 activation_47\n776 conv5_block22_1_relu\n777 conv2d_43\n778 conv2d_48\n779 conv5_block22_2_conv\n780 batch_normalization_43\n781 batch_normalization_48\n782 conv5_block22_concat\n783 activation_43\n784 activation_48\n785 average_pooling2d_5\n786 conv5_block23_0_bn\n787 conv2d_41\n788 conv2d_44\n789 conv2d_49\n790 conv2d_50\n791 conv5_block23_0_relu\n792 batch_normalization_41\n793 batch_normalization_44\n794 batch_normalization_49\n795 batch_normalization_50\n796 conv5_block23_1_conv\n797 activation_41\n798 activation_44\n799 activation_49\n800 activation_50\n801 conv5_block23_1_bn\n802 mixed5\n803 conv5_block23_1_relu\n804 conv2d_55\n805 conv5_block23_2_conv\n806 batch_normalization_55\n807 conv5_block23_concat\n808 activation_55\n809 conv5_block24_0_bn\n810 conv2d_56\n811 conv5_block24_0_relu\n812 batch_normalization_56\n813 conv5_block24_1_conv\n814 activation_56\n815 conv5_block24_1_bn\n816 conv2d_52\n817 conv2d_57\n818 conv5_block24_1_relu\n819 batch_normalization_52\n820 batch_normalization_57\n821 conv5_block24_2_conv\n822 activation_52\n823 activation_57\n824 conv5_block24_concat\n825 conv2d_53\n826 conv2d_58\n827 conv5_block25_0_bn\n828 batch_normalization_53\n829 batch_normalization_58\n830 conv5_block25_0_relu\n831 activation_53\n832 activation_58\n833 average_pooling2d_6\n834 conv5_block25_1_conv\n835 conv2d_51\n836 conv2d_54\n837 conv2d_59\n838 conv2d_60\n839 conv5_block25_1_bn\n840 batch_normalization_51\n841 batch_normalization_54\n842 batch_normalization_59\n843 batch_normalization_60\n844 conv5_block25_1_relu\n845 activation_51\n846 activation_54\n847 activation_59\n848 activation_60\n849 conv5_block25_2_conv\n850 mixed6\n851 conv5_block25_concat\n852 conv2d_65\n853 conv5_block26_0_bn\n854 batch_normalization_65\n855 conv5_block26_0_relu\n856 activation_65\n857 conv5_block26_1_conv\n858 conv2d_66\n859 conv5_block26_1_bn\n860 batch_normalization_66\n861 conv5_block26_1_relu\n862 activation_66\n863 conv5_block26_2_conv\n864 conv2d_62\n865 conv2d_67\n866 conv5_block26_concat\n867 batch_normalization_62\n868 batch_normalization_67\n869 conv5_block27_0_bn\n870 activation_62\n871 activation_67\n872 conv5_block27_0_relu\n873 conv2d_63\n874 conv2d_68\n875 conv5_block27_1_conv\n876 batch_normalization_63\n877 batch_normalization_68\n878 conv5_block27_1_bn\n879 activation_63\n880 activation_68\n881 average_pooling2d_7\n882 conv5_block27_1_relu\n883 conv2d_61\n884 conv2d_64\n885 conv2d_69\n886 conv2d_70\n887 conv5_block27_2_conv\n888 batch_normalization_61\n889 batch_normalization_64\n890 batch_normalization_69\n891 batch_normalization_70\n892 conv5_block27_concat\n893 activation_61\n894 activation_64\n895 activation_69\n896 activation_70\n897 conv5_block28_0_bn\n898 mixed7\n899 conv5_block28_0_relu\n900 conv2d_73\n901 conv5_block28_1_conv\n902 batch_normalization_73\n903 conv5_block28_1_bn\n904 activation_73\n905 conv5_block28_1_relu\n906 conv2d_74\n907 conv5_block28_2_conv\n908 batch_normalization_74\n909 conv5_block28_concat\n910 activation_74\n911 conv5_block29_0_bn\n912 conv2d_71\n913 conv2d_75\n914 conv5_block29_0_relu\n915 batch_normalization_71\n916 batch_normalization_75\n917 conv5_block29_1_conv\n918 activation_71\n919 activation_75\n920 conv5_block29_1_bn\n921 conv2d_72\n922 conv2d_76\n923 conv5_block29_1_relu\n924 batch_normalization_72\n925 batch_normalization_76\n926 conv5_block29_2_conv\n927 activation_72\n928 activation_76\n929 max_pooling2d_4\n930 conv5_block29_concat\n931 mixed8\n932 conv5_block30_0_bn\n933 conv2d_81\n934 conv5_block30_0_relu\n935 batch_normalization_81\n936 conv5_block30_1_conv\n937 activation_81\n938 conv5_block30_1_bn\n939 conv2d_78\n","name":"stdout"},{"output_type":"stream","text":"940 conv2d_82\n941 conv5_block30_1_relu\n942 batch_normalization_78\n943 batch_normalization_82\n944 conv5_block30_2_conv\n945 activation_78\n946 activation_82\n947 conv5_block30_concat\n948 conv2d_79\n949 conv2d_80\n950 conv2d_83\n951 conv2d_84\n952 average_pooling2d_8\n953 conv5_block31_0_bn\n954 conv2d_77\n955 batch_normalization_79\n956 batch_normalization_80\n957 batch_normalization_83\n958 batch_normalization_84\n959 conv2d_85\n960 conv5_block31_0_relu\n961 batch_normalization_77\n962 activation_79\n963 activation_80\n964 activation_83\n965 activation_84\n966 batch_normalization_85\n967 conv5_block31_1_conv\n968 activation_77\n969 mixed9_0\n970 concatenate_1\n971 activation_85\n972 conv5_block31_1_bn\n973 mixed9\n974 conv5_block31_1_relu\n975 conv2d_90\n976 conv5_block31_2_conv\n977 batch_normalization_90\n978 conv5_block31_concat\n979 activation_90\n980 conv5_block32_0_bn\n981 conv2d_87\n982 conv2d_91\n983 conv5_block32_0_relu\n984 batch_normalization_87\n985 batch_normalization_91\n986 conv5_block32_1_conv\n987 activation_87\n988 activation_91\n989 conv5_block32_1_bn\n990 conv2d_88\n991 conv2d_89\n992 conv2d_92\n993 conv2d_93\n994 average_pooling2d_9\n995 conv5_block32_1_relu\n996 conv2d_86\n997 batch_normalization_88\n998 batch_normalization_89\n999 batch_normalization_92\n1000 batch_normalization_93\n1001 conv2d_94\n1002 conv5_block32_2_conv\n1003 batch_normalization_86\n1004 activation_88\n1005 activation_89\n1006 activation_92\n1007 activation_93\n1008 batch_normalization_94\n1009 conv5_block32_concat\n1010 activation_86\n1011 mixed9_1\n1012 concatenate_2\n1013 activation_94\n1014 bn\n1015 mixed10\n1016 relu\n1017 global_average_pooling2d_1\n1018 global_average_pooling2d_2\n1019 concatenate_3\n1020 dense_1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1 = model.layers[585].output \nc1 = GlobalAveragePooling2D()(c1)       \n\nc2 = model.layers[601].output\nc2 = GlobalAveragePooling2D()(c2)       \n\nc3 = model.layers[634].output\nc3 = GlobalAveragePooling2D()(c3)       \n\nc4 = model.layers[667].output\nc4 = GlobalAveragePooling2D()(c4) \n\nc5 = model.layers[708].output\nc5 = GlobalAveragePooling2D()(c5) \n\nc6 = model.layers[739].output\nc6 = GlobalAveragePooling2D()(c6) \n\nc7 = model.layers[787].output\nc7 = GlobalAveragePooling2D()(c7) \n\nc8 = model.layers[835].output\nc8 = GlobalAveragePooling2D()(c8) \n\nc9 = model.layers[883].output\nc9 = GlobalAveragePooling2D()(c9) \n\nc10 = model.layers[933].output\nc10 = GlobalAveragePooling2D()(c10) \n\nc11 = model.layers[954].output\nc11 = GlobalAveragePooling2D()(c11) \n\nc12 = model.layers[996].output\nc12 = GlobalAveragePooling2D()(c12) \n\n# c13 = model.layers[376].output\n# c13 = GlobalAveragePooling2D()(c13) \n\n# c14 = model.layers[291].output\n# c14 = GlobalAveragePooling2D()(c14) \n\n# c15 = model.layers[305].output\n# c15 = GlobalAveragePooling2D()(c15) \n\n# c16 = model.layers[311].output\n# c16 = GlobalAveragePooling2D()(c16) \n\ncon = concatenate([c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12])\n\nbottleneck_final_model = Model(inputs=model.input, outputs=con)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir(\"extracted_features\")\nextracted_features_dir = \"extracted_features/\"\nmodel_name = \"densenet201_InceptionResNetV2_descriptors\"","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_train = bottleneck_final_model.predict_generator(train_generator, predict_size_train, max_q_size=1, pickle_safe=False)\nnp.save(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_validation = bottleneck_final_model.predict_generator(validation_generator, predict_size_validation)\nnp.save(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bottleneck_features_test = bottleneck_final_model.predict_generator(test_generator, predict_size_test)\nnp.save(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# from keras.backend.tensorflow_backend import get_session\n# from keras.backend.tensorflow_backend import clear_session\n# from keras.backend.tensorflow_backend import set_session\n\n# def reset_keras_tf_session():\n#     \"\"\"\n#     this function clears the gpu memory and set the \n#     tf session to not use the whole gpu\n#     \"\"\"\n#     sess = get_session()\n#     clear_session()\n#     sess.close()\n#     sess = get_session()\n\n#     config = tf.ConfigProto()\n#     config.gpu_options.allow_growth = True\n#     set_session(tf.Session(config=config))\n\n\n# reset_keras_tf_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = np.load(extracted_features_dir+'bottleneck_features_train_'+model_name+'.npy')\nvalidation_data = np.load(extracted_features_dir+'bottleneck_features_validation_'+model_name+'.npy')\ntest_data = np.load(extracted_features_dir+'bottleneck_features_test_'+model_name+'.npy')\n\ntrain_labels = train_generator.classes\ntrain_labels = to_categorical(train_labels, num_classes=num_classes)\n\nvalidation_labels = validation_generator.classes\nvalidation_labels = to_categorical(validation_labels, num_classes=num_classes)\n\ntest_labels = test_generator.classes\ntest_labels = to_categorical(test_labels, num_classes=num_classes)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Machine Learning Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score, classification_report\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n\nk_fold = KFold(n_splits=10, shuffle=True, random_state=5)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.asarray(test_labels)\ny_test = np.argmax(y_test, axis=1)\n\ny_train = np.asarray(train_labels)\ny_train = np.argmax(y_train, axis=1)","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bagging Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('AdaBoost Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":24,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.9934640522875817\nAdaBoost Classifier test accuracies 0.9935\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99       329\n           1       1.00      0.98      0.99       211\n           2       1.00      1.00      1.00       225\n\n    accuracy                           0.99       765\n   macro avg       1.00      0.99      0.99       765\nweighted avg       0.99      0.99      0.99       765\n\n0.9934640522875817\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BaggingClassifier(RandomForestClassifier(), n_estimators = 300 )\nscoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"BaggingClassifier - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":25,"outputs":[{"output_type":"stream","text":"Scores Mean: 96.7413 and (STDEV 0.0261)\nBest result for fold 9\nBest accuracy is 1.0\nScores of all folds: [0.90909091 0.96103896 0.98701299 0.93506494 0.97402597 0.97368421\n 0.96052632 0.98684211 0.98684211 1.        ]\nBaggingClassifier - Test Accuracy on all folds: 0.97 (+/- 0.05)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# AdaBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators = 300 )\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('AdaBoost Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":26,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.9019607843137255\nAdaBoost Classifier test accuracies 0.9020\n              precision    recall  f1-score   support\n\n           0       0.92      0.92      0.92       329\n           1       0.81      0.89      0.84       211\n           2       0.98      0.88      0.93       225\n\n    accuracy                           0.90       765\n   macro avg       0.90      0.90      0.90       765\nweighted avg       0.91      0.90      0.90       765\n\n0.9019607843137255\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(AdaBoost Classifier) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":27,"outputs":[{"output_type":"stream","text":"Scores Mean: 88.6295 and (STDEV 0.0274)\nBest result for fold 7\nBest accuracy is 0.9342105263157895\nScores of all folds: [0.84415584 0.8961039  0.92207792 0.88311688 0.87012987 0.85526316\n 0.86842105 0.93421053 0.90789474 0.88157895]\n(AdaBoost Classifier) Test Accuracy on all folds: 0.89 (+/- 0.05)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('XGB Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":28,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9995662546085448\nTest accuracy 0.996078431372549\nXGB Classifier test accuracies 0.9961\n              precision    recall  f1-score   support\n\n           0       0.99      1.00      1.00       329\n           1       1.00      0.99      0.99       211\n           2       1.00      1.00      1.00       225\n\n    accuracy                           1.00       765\n   macro avg       1.00      1.00      1.00       765\nweighted avg       1.00      1.00      1.00       765\n\n0.996078431372549\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"XGB - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":29,"outputs":[{"output_type":"stream","text":"Scores Mean: 97.6521 and (STDEV 0.0163)\nBest result for fold 7\nBest accuracy is 1.0\nScores of all folds: [0.96103896 0.98701299 0.97402597 0.94805195 0.97402597 0.98684211\n 0.96052632 1.         0.97368421 1.        ]\nXGB - Test Accuracy on all folds: 0.98 (+/- 0.03)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('DecisionTree Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":30,"outputs":[{"output_type":"stream","text":"Train accuracy 1.0\nTest accuracy 0.8823529411764706\nDecisionTree Classifier test accuracies 0.8824\n              precision    recall  f1-score   support\n\n           0       0.88      0.91      0.90       329\n           1       0.80      0.89      0.84       211\n           2       0.98      0.84      0.90       225\n\n    accuracy                           0.88       765\n   macro avg       0.89      0.88      0.88       765\nweighted avg       0.89      0.88      0.88       765\n\n0.8823529411764706\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"DecisionTree - Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":31,"outputs":[{"output_type":"stream","text":"Scores Mean: 87.5854 and (STDEV 0.0279)\nBest result for fold 7\nBest accuracy is 0.9078947368421053\nScores of all folds: [0.80519481 0.87012987 0.8961039  0.8961039  0.88311688 0.85526316\n 0.86842105 0.90789474 0.89473684 0.88157895]\nDecisionTree - Test Accuracy on all folds: 0.88 (+/- 0.06)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=5)\nclf.fit(train_data, y_train)\nprint(\"Train accuracy\", clf.score(train_data, y_train))\nprint(\"Test accuracy\", clf.score(test_data, y_test))\n\ny_test_pred = clf.predict(test_data)\nclf_test = accuracy_score(y_test, y_test_pred)\nprint('RandomForest Classifier test accuracies %.4f' % (clf_test))\n\nprint(classification_report(y_test, y_test_pred))\n\n# print(confusion_matrix(test_labels, y_test_pred))\nprint(accuracy_score(y_test, y_test_pred))","execution_count":32,"outputs":[{"output_type":"stream","text":"Train accuracy 0.9967469095640858\nTest accuracy 0.9568627450980393\nRandomForest Classifier test accuracies 0.9569\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96       329\n           1       0.96      0.89      0.93       211\n           2       1.00      0.97      0.98       225\n\n    accuracy                           0.96       765\n   macro avg       0.96      0.95      0.96       765\nweighted avg       0.96      0.96      0.96       765\n\n0.9568627450980393\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring = 'accuracy'\nscores = cross_val_score(clf, test_data, test_labels, cv=k_fold, n_jobs=1, scoring=scoring)\n\nprint (\"Scores Mean: %.4f and (STDEV %.4f)\" % (np.mean(scores)*100, np.std(scores)))\nprint (\"Best result for fold %s\" % np.argmax(scores))\nprint (\"Best accuracy is\", (scores[np.argmax(scores)]))\nprint (\"Scores of all folds:\", scores)\nprint(\"(RandomForest) Test Accuracy on all folds: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":33,"outputs":[{"output_type":"stream","text":"Scores Mean: 89.0277 and (STDEV 0.0407)\nBest result for fold 3\nBest accuracy is 0.935064935064935\nScores of all folds: [0.85714286 0.81818182 0.85714286 0.93506494 0.92207792 0.93421053\n 0.84210526 0.89473684 0.92105263 0.92105263]\n(RandomForest) Test Accuracy on all folds: 0.89 (+/- 0.08)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Done!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}